from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup
import urllib as url
import urllib.request
import re
import sys
import requests
import os
from joblib import Parallel, delayed
import time
from multiprocessing import Pool, cpu_count
import random

url3 = 'https://download.cnet.com/s/software/windows/?ftag=DSM-03-10aaa0i&sort=latest'
url2 = 'https://filehippo.com/popular'


def prapare_browser():
    downloadpath = 'D:\\exewebdownloadfolder'
    fp = webdriver.FirefoxProfile()

    fp.set_preference("browser.download.folderList", 2)
    fp.set_preference("browser.download.dir", downloadpath)
    fp.set_preference("browser.download.manager.alertOnEXEOpen", False)
    fp.set_preference("browser.helperApps.neverAsk.saveToDisk",
                      "application/msword, application/csv, application/ris, text/csv, image/png, application/pdf, text/html, text/plain, application/zip, application/x-zip, application/x-zip-compressed, application/download, application/octet-stream")
    fp.set_preference("browser.download.manager.showWhenStarting", False)
    fp.set_preference("browser.download.manager.focusWhenStarting", False)
    fp.set_preference("browser.download.useDownloadDir", True)
    fp.set_preference("browser.helperApps.alwaysAsk.force", False)
    fp.set_preference("browser.download.manager.alertOnEXEOpen", False)
    fp.set_preference("browser.download.manager.closeWhenDone", True)
    fp.set_preference("browser.download.manager.showAlertOnComplete", False)
    fp.set_preference("browser.download.manager.useWindow", False)
    fp.set_preference("services.sync.prefs.sync.browser.download.manager.showWhenStarting", False)
    fp.set_preference("pdfjs.disabled", True)
    return fp


class filehippo_crawler:
    def __init__(self, webdriver):
        self.main_url = 'https://filehippo.com/'
        self.link_counter = 0
        self.downloaded_program_names = []
        self.webdriver = webdriver

    def traverse_main_site_get_category_pages(self):
        with urllib.request.urlopen(self.main_url) as url:
            s = url.read()

        soup = BeautifulSoup(s, 'lxml')
        l = soup.findAll('ul', id="categories-list")[0].findAll('a')
        downlinks = []
        for link in l:
            downlinks.append(link.get('href'))

        all_main_urls = [self.main_url + l for l in downlinks]
        return all_main_urls

    def get_total_links_of_category_url(self, category_main_link):
        with urllib.request.urlopen(category_main_link) as url:
            s = url.read()

        soup = BeautifulSoup(s, 'lxml')
        nextstagelinks = []
        for link in soup.findAll('a', class_="pager-page-link"):
            nextstagelinks.append(link.get('href'))
        return nextstagelinks

    def extract_links_from_category_page_piece(self, central_url):

        with urllib.request.urlopen(central_url) as url:
            s = url.read()

        soup = BeautifulSoup(s, 'lxml')
        for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
            print(link.get('href'))
        links = []
        for link in soup.findAll('a', class_="green program-entry-download-link button-link"):
            links.append(link.get('href'))
        return links

    def get_download_link_from_source_url_filehippo(self, target_link):

        with urllib.request.urlopen(target_link) as url:
            s = url.read()

        soup = BeautifulSoup(s, 'lxml')

        downlink = []
        for link in soup.findAll('a', class_="program-header-download-link button-link active short download-button"):
            downlink = link.get('href')

        if len(downlink) == 0:
            for link in soup.findAll('a', class_="internal-link bold"):
                downlink = link.get('href')

            with urllib.request.urlopen('https://filehippo.com' + downlink) as url:
                s = url.read()

            soup = BeautifulSoup(s, 'lxml')

            downlink = []
            for link in soup.findAll('a',
                                     class_="program-header-download-link button-link active short download-button"):
                downlink = link.get('href')

        print(downlink)
        if downlink is not 'https://filehippo.com':
            self.downloaded_program_names.append(downlink)
        self.link_counter = self.link_counter + 1
        return downlink

    def main_operation(self):
        all_category_urls = self.traverse_main_site_get_category_pages()

        list_ofallurl = [tuple(self.get_total_links_of_category_url(url)) for url in all_category_urls]
        all_url = [(all_category_urls[ind],) + url_tuple for ind, url_tuple in enumerate(list_ofallurl)]

        all_url_list = []
        for i in all_url:
            all_url_list.extend(list(i))

        all_central_links = []
        for main_url in all_url_list:
            all_central_links.extend(self.extract_links_from_category_page_piece(main_url))

        for main_url in all_central_links:
            self.get_download_link_from_source_url_filehippo(main_url)


def run_downloader_for_set_links(links):
    fp = prapare_browser()
    browser = webdriver.Firefox(executable_path=r'C:\\Users\\Ben\\Desktop\\bigproject_repository\\geckodriver.exe'
                                , firefox_profile=fp)
    for inds, link in enumerate(links):
        try:
            print(str(inds) + '   ' + link)
            browser.get(link)
            time.sleep(20)
        except:
            print(link)


def run_parallel_selenium_processes(datalist, driver, selenium_func):
    pool = Pool()

    # max number of parallel process
    ITERATION_COUNT = cpu_count() - 1

    count_per_iteration = len(datalist) / float(ITERATION_COUNT)

    for i in range(0, ITERATION_COUNT):
        list_start = int(count_per_iteration * i)
        list_end = int(count_per_iteration * (i + 1))
        pool.apply_async(selenium_func, args=(driver, datalist[list_start:list_end]))


if __name__ == "__main__":
    # fp =  prapare_browser()
    # browser = webdriver.Firefox(executable_path=r'C:\\Users\\Ben\\Desktop\\bigproject_repository\\geckodriver.exe'
    #                                     , firefox_profile=fp)
    # # craw = filehippo_crawler(browser)
    # craw.main_operation()
    # with open('c:\\users\\ben\\desktop\\bigproject_repository\\filedownloadnames.txt','w') as f:
    #     for line in craw.downloaded_program_names:
    #         f.write("%s\n" % line)

    with open('C:\\Users\\Ben\\Desktop\\bigproject_repository\\filedownloadnames.txt', 'r') as f:
        proglist = [i[:-1] for i in f]

    random.shuffle(proglist)
    divided_prog_list = []
    # max number of parallel process
    ITERATION_COUNT = cpu_count() - 1

    count_per_iteration = len(proglist) / float(ITERATION_COUNT)

    for i in range(0, ITERATION_COUNT):
        list_start = int(count_per_iteration * i)
        list_end = int(count_per_iteration * (i + 1))
        divided_prog_list.append(proglist[list_start:list_end])
    #

    scores = Parallel(n_jobs=7, verbose=100)(delayed(run_downloader_for_set_links)(x) for x in divided_prog_list)

    # run_parallel_selenium_processes(proglist, browser,  run_downloader_for_set_links)
