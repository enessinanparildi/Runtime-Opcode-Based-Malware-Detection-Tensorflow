from bs4 import BeautifulSoup
import urllib as url
import urllib.request
import re
import sys
import requests

url1 = 'https://en.softonic.com/windows/trending'

url2 = 'https://filehippo.com/popular'

with urllib.request.urlopen(url2) as url:
    s = url.read()

soup = BeautifulSoup(s)
tag = soup.find_all('li')
for link in soup.findAll('li', attrs={'href': re.compile("^https://")}):
    print(link.get('href'))
for link in soup.findAll('a', class_="app-list-item app-list-item--interactive"):
    print(link.get('href'))
soup.find("a").findAll('div', class_="app-list-item__media", recursive=False)

links = []
for link in soup.findAll('a', class_=None, title=None):
    links.append(link.get('href'))

target = links[4]
with urllib.request.urlopen(target) as url:
    s = url.read()

soup = BeautifulSoup(s)

for link in soup.findAll('a', class_="program-header-download-link button-link active short download-button"):
    rawlink = link.get('href')

with urllib.request.urlopen(rawlink) as url:
    s = url.read()
soup = BeautifulSoup(s)
tag = soup.find_all('a')
for link in soup.findAll('a', class_="text"):
    link2 = link.get('href')

likfinal = rawlink[:-1] + link2

with urllib.request.urlopen(likfinal) as url:
    s = url.read()
soup = BeautifulSoup(s)
tag = soup.find_all('script')

for i in soup.find_all('a'):
    print(re.search('http://.*\.exe', i.get('href')).group(0))

tag2 = soup.find_all('script')

re.search("^https://", str(tag2))
for link in soup.findAll('a', attrs={'href': re.compile("^https://")}):
    print(link.get('href'))


def get_filename_from_cd(cd):
    """
    Get filename from content-disposition
    """
    if not cd:
        return None
    fname = re.findall('filename=(.+)', cd)
    if len(fname) == 0:
        return None
    return fname[0]


url = likfinal
r = requests.get(url, allow_redirects=True)
filename = get_filename_from_cd(r.headers.get('content-disposition'))


#
# targettags = []
# for t in tag:
#     if len(t.find_all('article')) > 0 and len(t.find_all(attrs={"class": "app-list-item app-list-item--interactive"}))> 0:
#         targettags.append(t)
# links = []
# for tags in targettags:
#     links.append(tags.get('href'))

def get_next_navigation_link(soup):
    navigation_link = soup.find_all("a", class_="pagination-links__next")[0].get('href')
    return navigation_link


def get_in_file_link(link):
    with urllib.request.urlopen(link) as url:
        s = url.read()
    soup = BeautifulSoup(s)
    download_link = soup.find_all("a", attrs={"data-auto": "download-button"})[0].get('href')

    with urllib.request.urlopen(download_link) as url:
        s = url.read()
    soup = BeautifulSoup(s)

    for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
        print(link.get('href'))

    with urllib.request.urlopen(target) as url:
        s = url.read()
    soup = BeautifulSoup(s)

    set = []
    names = []
    for i, link in enumerate(soup.findAll('a')):
        links = link.get('href')
        if links.endswith('.exe'):
            set.append(links)
            names.append(soup.select('a')[i].attrs['href'])

    links = [i.get('data-download') for i in soup.find_all("a")]
    links = [l for l in links if l is not None]
    text = links[0]
    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
    target = urls[0]

    with urllib.request.urlopen(target) as url:
        s = url.read()
    soup = BeautifulSoup(s)

    return target


def get_single_link_from_source_url_softonic(target_link):
    with urllib.request.urlopen(target_link) as url:
        s = url.read()

    soup = BeautifulSoup(s)

    for link in soup.findAll('a', class_="program-header-download-link button-link active short download-button"):
        downlink = link.get('href')
    print(downlink)
    return downlink
