import numpy as np
import tensorflow as tf
import logging

import helper_functions
import cnnmodeldefinitions

from sklearn.metrics import confusion_matrix

from graph_training_functions import GraphHelperFunctions
import os


class CnnModelClass(GraphHelperFunctions):
    logging.basicConfig(filename='cnn_model.log', format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S',
                        level=logging.DEBUG)

    def __init__(self):
        super().__init__()
        self.use_class_weights = False
        # self.resample_dataset and self.use_feature_selection gives suboptimal results
        self.resample_dataset = False
        self.use_feature_selection = False
        # hyparameter optim on ml algortims trained by cnn features , uses 5-fold cv , takes very long
        self.use_cross_validation = False
        self.do_k_mean_cluster = True
        # Set it true to obtain cnn feature visualization
        self.get_visualizations = True

        self.save_results = True
        # To fine tune model set this false , to create new model set this true
        self.new_model = False
        self.change_graph = True

        # 1- Droput rate , higher means weaker dropout
        self.keep_prob_fc = 1.0
        self.keep_prob_conv = 1.0
        
        # As a more advanced regularization strategy add gaussian noise to data after embedding projection
        # this parameter sets standard deviation of noise , setting this variable zero turns off the noise
        self.noise_level = 0.000000

        self.modify_jnz_jne = True

        self.batch_size = 16
        self.test_batch_size = 64
        self.learning_rate = 0.00001
        self.total_seq_length = 1000
        self.epochnum = 10
        self.new_malware_test_ratio = 0.9

        # If self.new_model = True , the new model is created with this name , and if the name belongs to already
        # existant model , that pre-trained model will get erased. If self.new_model = False , self.model_name must
        # be some model name which is already existent in pre-trained model directory self.root_dir
        self.model_name = 'cnn_model_v19_fit_entire_set'
        # To overwrite leave this same as self.model_name , if different from self.model_name program creates a copy
        # and fine - tune that copy.
        self.save_name = 'cnn_model_v19_fit_entire_set'
        # Model save root dir , change this to save model according to our directory names
        self.root_dir = 'E:\\All_code\\ML_DeepLearning_Codebase\\saveddata\\Networks\\convolutional_networks'

    def get_cnn_graph(self):

        tf.reset_default_graph()
        cnn_graph = tf.Graph()

        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.config = config
        with cnn_graph.as_default():

            iterator_train, iterator_test = self.get_input_data_pipeline(cnn_graph)

            is_training_tensor = tf.placeholder(tf.bool, shape=())

            data_piece, label_piece = tf.cond(is_training_tensor,
                                              lambda: self.fetch_data_from_pipeline(cnn_graph, True, iterator_train,
                                                                                    iterator_test),
                                              lambda: self.fetch_data_from_pipeline(cnn_graph, False, iterator_train,
                                                                                    iterator_test))

            embedded_mat_piece = tf.nn.embedding_lookup(tf.convert_to_tensor(self.embedding_mat), data_piece)
            embedded_mat_piece = tf.expand_dims(embedded_mat_piece, 3)
            embedded_mat_piece_norm_old = self.tensorflow_normalize(embedded_mat_piece)

            embedded_mat_piece_norm = tf.cond(is_training_tensor,
                                              lambda: self.gaussian_noise_layer(input_layer=embedded_mat_piece_norm_old,
                                                                                stddev=self.noise_level),
                                              lambda: embedded_mat_piece_norm_old)

            batch_norm_switch = tf.placeholder(tf.bool, shape=())
            keep_prob_tensor = tf.placeholder_with_default(1.0, shape=())

            model_definition = cnnmodeldefinitions.cnnmodelgraphs(cnn_graph, keep_prob_tensor, keep_prob_tensor,
                                                                  batch_norm_switch)

            unnormalized_scores, features, name = model_definition.advanced_cnn_graph_with_batchnorm_gelu(
                embedded_mat_piece_norm)

            self.model_type_name = name

            if not self.use_class_weights:
                loss = tf.reduce_mean(
                    tf.nn.softmax_cross_entropy_with_logits_v2(logits=unnormalized_scores, labels=label_piece))
            else:
                weight = tf.constant([self.class_weight])
                # weight = 1
                loss = tf.reduce_mean(
                    tf.nn.weighted_cross_entropy_with_logits(logits=unnormalized_scores, targets=label_piece,
                                                             pos_weight=1 / (weight)))

            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
            #
            # decayed_lr = tf.train.exponential_decay(learning_rate,
            #                                         global_step, 10000,
            #                                         0.95, staircase=True)
            #
            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)

            with tf.control_dependencies(update_ops):
                optimizer = optimizer.minimize(loss, global_step=tf.train.get_global_step())

            scores = tf.nn.softmax(unnormalized_scores)
            self.add_metrics(cnn_graph, unnormalized_scores, label_piece)

            saver = tf.train.Saver()

            cnn_graph.add_to_collection('train_iter', iterator_train)
            cnn_graph.add_to_collection('test_iter', iterator_test)
            cnn_graph.add_to_collection('final_data', embedded_mat_piece_norm)

            cnn_graph.add_to_collection('batch_norm_switch', batch_norm_switch)
            cnn_graph.add_to_collection('scores', scores)
            cnn_graph.add_to_collection('features', features)
            cnn_graph.add_to_collection('loss', loss)
            cnn_graph.add_to_collection('config', config)
            cnn_graph.add_to_collection('saver', saver)
            cnn_graph.add_to_collection('optim_op', optimizer)
            cnn_graph.add_to_collection('dropout', keep_prob_tensor)
            cnn_graph.add_to_collection('is_training_tensor', is_training_tensor)
            cnn_graph.add_to_collection('label', label_piece)
            # all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
            return cnn_graph

    def get_cnn_graph_for_adversarial(self):

        tf.reset_default_graph()
        cnn_graph = tf.Graph()

        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True

        with cnn_graph.as_default():

            iterator_train = self.get_only_train_data_pipeline(cnn_graph)

            is_training_tensor = tf.placeholder(tf.bool, shape=())

            data_piece, label_piece = iterator_train.get_next()

            embedded_mat_piece = tf.expand_dims(data_piece, 3)
            embedded_mat_piece_norm_old = self.tensorflow_normalize(embedded_mat_piece)

            batch_norm_switch = tf.placeholder(tf.bool, shape=())
            keep_prob_tensor = tf.placeholder_with_default(1.0, shape=())

            model_definition = cnnmodeldefinitions.cnnmodelgraphs(cnn_graph, keep_prob_tensor, keep_prob_tensor,
                                                                  batch_norm_switch)

            unnormalized_scores, features, name = model_definition.advanced_cnn_graph_with_batchnorm_gelu(
                embedded_mat_piece_norm_old)

            self.model_type_name = name

            if not self.use_class_weights:
                loss = tf.reduce_mean(
                    tf.nn.softmax_cross_entropy_with_logits_v2(logits=unnormalized_scores, labels=label_piece))
            else:
                weight = tf.constant([self.class_weight])
                # weight = 1
                loss = tf.reduce_mean(
                    tf.nn.weighted_cross_entropy_with_logits(logits=tf.nn.softmax(unnormalized_scores),
                                                             targets=label_piece,
                                                             pos_weight=1 / weight))

            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
            #
            # decayed_lr = tf.train.exponential_decay(learning_rate,
            #                                         global_step, 10000,
            #                                         0.95, staircase=True)
            #
            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)

            with tf.control_dependencies(update_ops):
                optimizer = optimizer.minimize(loss, global_step=tf.train.get_global_step())

            scores = tf.nn.softmax(unnormalized_scores)
            self.add_metrics(cnn_graph, unnormalized_scores, label_piece)

            saver = tf.train.Saver()

            cnn_graph.add_to_collection('train_iter', iterator_train)
            # cnn_graph.add_to_collection('test_iter', iterator_test)

            cnn_graph.add_to_collection('batch_norm_switch', batch_norm_switch)
            cnn_graph.add_to_collection('scores', scores)
            cnn_graph.add_to_collection('features', features)
            cnn_graph.add_to_collection('loss', loss)
            cnn_graph.add_to_collection('config', config)
            cnn_graph.add_to_collection('saver', saver)
            cnn_graph.add_to_collection('optim_op', optimizer)
            cnn_graph.add_to_collection('dropout', keep_prob_tensor)
            cnn_graph.add_to_collection('is_training_tensor', is_training_tensor)
            cnn_graph.add_to_collection('label', label_piece)
            # all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
            return cnn_graph

    def fgsm_adversarial_samples(self, graph, noise_level):
        eps_tf = noise_level
        main_graph = None
        restore_op = None

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()

        if self.change_graph:
            restore_op = graph.get_collection_ref('saver')[0]
            main_graph = graph

        with main_graph.as_default():
            grad_tf = tf.gradients(main_graph.get_collection_ref('loss')[0],
                                   [main_graph.get_collection_ref('final_data')[0]])
            grad_sign_tf = tf.sign(grad_tf)
            grad_sign_eps_tf = tf.scalar_mul(eps_tf, grad_sign_tf)
            new_image_tf = tf.add(grad_sign_eps_tf, main_graph.get_collection_ref('final_data')[0])

            with tf.Session() as session:

                if not self.new_model:
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                session.run(main_graph.get_operation_by_name('train_init'))
                all_adversarial_set = []
                while True:
                    try:
                        data_feed = {main_graph.get_collection_ref('batch_norm_switch')[0]: False,
                                     main_graph.get_collection_ref('is_training_tensor')[0]: True,
                                     main_graph.get_collection_ref('dropout')[0]: 1.0}

                        batch_adversarial = session.run(new_image_tf, data_feed)
                        all_adversarial_set.append(np.squeeze(batch_adversarial))
                    except tf.errors.OutOfRangeError:
                        print('epoch_adversarial_end')
                        break
                adversarial_set = np.concatenate(all_adversarial_set, axis=0)
        print(adversarial_set.shape)
        np.save(self.root_dir + "cnn_advarsarial_examples_noise_level= " + str(noise_level), adversarial_set)
        return adversarial_set

    def train_graph(self, graph):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph().finalize()
        else:
            restore_op = None
            main_graph = graph

        if self.change_graph:
            restore_op = graph.get_collection_ref('saver')[0]
            main_graph = graph

        with main_graph.as_default():
            query_list = [main_graph.get_collection_ref(str)[0] for str in
                          ['optim_op', 'loss', 'acc', 'preds', 'scores', 'label']]
            with tf.Session() as session:

                if not self.new_model:
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                for epoch_step in range(self.epochnum):

                    final_predictions = []
                    epoch_loss = 0
                    epoch_accuracy = 0
                    step = 0
                    session.run(main_graph.get_operation_by_name('train_init'))

                    while True:
                        step = step + 1
                        try:
                            data_feed = {main_graph.get_collection_ref('batch_norm_switch')[0]: True,
                                         main_graph.get_collection_ref('is_training_tensor')[0]: True,
                                         main_graph.get_collection_ref('dropout')[0]: self.keep_prob_fc}

                            _, batch_loss, batch_acc, batch_preds, batch_scores, batch_labels = session.run(query_list,
                                                                                                            data_feed)

                            epoch_loss = epoch_loss + batch_loss
                            epoch_accuracy = epoch_accuracy + batch_acc
                            final_predictions.append(batch_preds)
                        except tf.errors.OutOfRangeError:
                            print('epoch_train_end')
                            epoch_loss = epoch_loss / (step - 1)
                            epoch_accuracy = epoch_accuracy / (step - 1)
                            break
                    print("Epoch: %.4d cost=  %.9f ", (epoch_step + 1), epoch_loss)
                    logging.info("Epoch: %.4d cost=  %.9f", (epoch_step + 1), epoch_loss)
                    print("Epoch:", '%.4d' % (epoch_step + 1), "acc=", "%9f" % epoch_accuracy)
                    logging.info("Epoch: %.4d acc =  %.9f ", (epoch_step + 1), epoch_accuracy)

                    predict_set = np.concatenate(final_predictions)
                    print(confusion_matrix(self.train_label, predict_set))

                if self.save_results:
                    self.save_graph(main_graph, session, restore_op)

    def train_test_graph_run(self, graph):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()
        else:
            restore_op = None
            main_graph = graph

        if self.change_graph:
            restore_op = graph.get_collection_ref('saver')[0]
            main_graph = graph

        with main_graph.as_default():
            query_list_train = [main_graph.get_collection_ref(str)[0] for str in
                                ['optim_op', 'loss', 'acc', 'preds', 'scores', 'features']]
            query_list_test = [main_graph.get_collection_ref(str)[0] for str in
                               ['loss', 'acc', 'preds', 'features']]

            print(main_graph.get_all_collection_keys())
            print(main_graph.get_collection_ref('iterators'))
            with tf.Session() as session:

                if not self.new_model:
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                for epoch_step in range(self.epochnum):

                    final_predictions = []
                    train_features = []  # type: list
                    epoch_loss = 0
                    epoch_accuracy = 0
                    step = 0
                    # print(main_graph.get_operation_by_name('train_init'))
                    session.run(main_graph.get_operation_by_name('train_init'))
                    # session.run(main_graph.get_collection_ref('test_iter')[0].initializer)

                    while True:
                        step = step + 1
                        try:
                            data_feed = {main_graph.get_collection_ref('batch_norm_switch')[0]: True,
                                         main_graph.get_collection_ref('is_training_tensor')[0]: True,
                                         main_graph.get_collection_ref('dropout')[0]: self.keep_prob_fc}

                            _, batch_loss, batch_acc, batch_preds, batch_scores, batch_features = session.run(
                                query_list_train,
                                data_feed)

                            epoch_loss = epoch_loss + batch_loss
                            epoch_accuracy = epoch_accuracy + batch_acc
                            final_predictions.append(batch_preds)
                            train_features.append(batch_features)

                        except tf.errors.OutOfRangeError:
                            print('epoch_train_end')
                            epoch_loss = epoch_loss / (step - 1)
                            epoch_accuracy = epoch_accuracy / (step - 1)
                            break

                    print("Epoch: %.4d cost=  %.9f ", (epoch_step + 1), epoch_loss)
                    logging.info("Epoch: %.4d cost=  %.9f", (epoch_step + 1), epoch_loss)
                    print("Epoch:", '%.4d' % (epoch_step + 1), "acc=", "%9f" % epoch_accuracy)
                    logging.info("Epoch: %.4d acc =  %.9f ", (epoch_step + 1), epoch_accuracy)

                    predict_set = np.concatenate(final_predictions)
                    train_set = np.concatenate(train_features)
                    print(confusion_matrix(self.train_label, predict_set))

                    session.run(main_graph.get_operation_by_name('test_init'))

                    step = 0
                    loss = 0
                    accuracy = 0
                    final_predictions = []
                    test_features = []
                    while True:
                        step = step + 1
                        try:
                            data_feed = {main_graph.get_collection_ref('batch_norm_switch')[0]: False,
                                         main_graph.get_collection_ref('is_training_tensor')[0]: False}

                            batch_loss, batch_accuracy, batch_predictions, batch_features = session.run(
                                query_list_test, data_feed)

                            final_predictions.append(batch_predictions)
                            test_features.append(batch_features)
                            loss = loss + batch_loss
                            accuracy = accuracy + batch_accuracy

                        except tf.errors.OutOfRangeError:
                            print('epoch_test_end')
                            loss = loss / (step - 1)
                            accuracy = accuracy / (step - 1)
                            break

                    print("Epoch: %.4d cost=  %.9f ", (0 + 1), loss)
                    logging.info("Epoch: %.4d cost=  %.9f", (0 + 1), loss)
                    print("Epoch:", '%.4d' % (0 + 1), "acc=", "%9f" % accuracy)
                    logging.info("Epoch: %.4d acc =  %.9f ", (0 + 1), accuracy)

                    predict_set = np.concatenate(final_predictions)
                    test_set = np.concatenate(test_features)
                    print(confusion_matrix(self.test_label, predict_set))
                    # self.run_ml_classification(train_set, test_set, None, None)

                if self.save_results:
                    self.save_graph(main_graph, session, restore_op)

    def run_train_forward_pass(self, graph):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()
        else:
            restore_op = None
            main_graph = None

        if self.change_graph:
            restore_op = graph.get_collection_ref('saver')[0]
            main_graph = graph

        with main_graph.as_default():
            query_list = [main_graph.get_collection_ref(str)[0] for str in ['preds', 'scores', 'features', 'acc']]

            with tf.Session(config=self.config) as session:

                if not self.new_model:
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                session.run(main_graph.get_operation_by_name('train_init'))
                step = 0
                final_predictions = []
                train_features = []  # type: list
                train_scores = []
                while True:
                    step = step + 1

                    try:
                        data_feed = {main_graph.get_collection_ref('batch_norm_switch')[0]: False,
                                     main_graph.get_collection_ref('dropout')[0]: 1.0,
                                     main_graph.get_collection_ref('is_training_tensor')[0]: True}

                        batch_predictions, batch_scores, batch_features, batch_acc = session.run(query_list, data_feed)

                        final_predictions.append(batch_predictions)
                        train_features.append(batch_features)
                        train_scores.append(batch_scores)
                        # print(train_scores)

                    except tf.errors.OutOfRangeError:
                        print('epoch_train_end')
                        break
                predict_set = np.concatenate(final_predictions)
                feature_set = np.concatenate(train_features)
                score_set = np.concatenate(train_scores)
                print(confusion_matrix(self.train_label, predict_set))
                # helper_functions.train_data_visualize(feature_set, self.train_label, predict_set)

            opt_thres = helper_functions.choose_optimal_threshold(score_set, self.train_label)
            print('optimal_threshold =' + str(opt_thres))

        return predict_set, feature_set, score_set, opt_thres

    def run_test_inference(self, graph):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()
        else:
            restore_op = None
            main_graph = None

        if self.change_graph:
            restore_op = graph.get_collection_ref('saver')[0]
            main_graph = graph

        with main_graph.as_default():

            query_list = [main_graph.get_collection_ref(elem)[0] for elem in
                          ['loss', 'acc', 'preds', 'scores', 'features']]

            with tf.Session(config=self.config) as session:
                if not self.new_model:
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                session.run(main_graph.get_operation_by_name('test_init'))

                step = 0
                loss = 0
                accuracy = 0

                final_predictions = []
                train_features = []  # type: list
                train_scores = []
                while True:
                    step = step + 1
                    try:
                        data_feed = {main_graph.get_collection_ref('batch_norm_switch')[0]: False,
                                     main_graph.get_collection_ref('dropout')[0]: 1.0,
                                     main_graph.get_collection_ref('is_training_tensor')[0]: False}

                        batch_loss, batch_accuracy, batch_predictions, batch_scores, batch_features = session.run(
                            query_list, data_feed)

                        final_predictions.append(batch_predictions)
                        train_features.append(batch_features)
                        train_scores.append(batch_scores)
                        print(batch_scores)

                        loss = loss + batch_loss
                        accuracy = accuracy + batch_accuracy

                    except tf.errors.OutOfRangeError:
                        print('epoch_test_end')
                        loss = loss / (step - 1)
                        accuracy = accuracy / (step - 1)
                        break

                print("Epoch: %.4d cost=  %.9f ", (0 + 1), loss)
                logging.info("Epoch: %.4d cost=  %.9f", (0 + 1), loss)
                print("Epoch:", '%.4d' % (0 + 1), "acc=", "%9f" % accuracy)
                logging.info("Epoch: %.4d acc =  %.9f ", (0 + 1), accuracy)

                predict_set = np.concatenate(final_predictions)
                feature_set = np.concatenate(train_features)
                score_set = np.concatenate(train_scores)
                print(predict_set.shape)
                print(self.test_label.shape)
                print(confusion_matrix(self.test_label, predict_set))
        return predict_set, feature_set, score_set


