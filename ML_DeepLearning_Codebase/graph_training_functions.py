import numpy as np
import tensorflow as tf
import ml_algorithms
import helper_functions
import data_load_functions
from sklearn.model_selection import train_test_split
from tensorflow.python.data.experimental.ops import prefetching_ops


class GraphHelperFunctions:
    def __init__(self):

        self.use_feature_selection = False
        self.use_attention = True
        self.class_weight = False
        self.resample_dataset = False
        self.modify_jnz_jne = True

        self.use_cross_validation = False
        self.do_k_mean_cluster = True
        self.get_visualizations = False

        self.model_name = None
        self.save_name = None
        self.root_dir = None
        self.model_type_name = None

        self.total_seq_length = 1000

        self.batch_size = 8
        self.test_batch_size = 256

        self.new_malware_test_ratio = None

        self.new_model = None
        self.converted_train_data = None
        self.train_label = None
        self.converted_test_data_balanced = None
        self.test_label_balanced = None
        self.converted_test_data = None
        self.test_label = None
        self.embedding_mat = None
        self.feature_size = None
        self.total_data_length = None
        self.total_test_length = None
        self.data_load_func_name = None
        self.class_weight = None
        self.config = None

    def label_smoothing(self, inputs, epsilon=0.1):
        K = inputs.get_shape().as_list()[-1]  # number of channels
        return ((1 - epsilon) * inputs) + (epsilon / K)

    # call this function in yield dataset to set up experiment for new malware prediction
    def new_malware_predict_dataset(self):

        self.converted_train_data = np.concatenate((self.converted_train_data, self.converted_test_data))
        self.train_label = np.concatenate((self.train_label, self.test_label))

        benign_indices = np.where(self.train_label == 0)[0][:400]
        benign_part_of_data = self.converted_train_data[benign_indices, :]
        benign_part_of_labels = self.train_label[benign_indices]
        self.converted_train_data = np.delete(self.converted_train_data, benign_indices, axis=0)
        self.train_label = np.delete(self.train_label, benign_indices, axis=0)
        print("length of new train = " + str(self.converted_train_data.shape))

        converted_test_data1, test_label1 = data_load_functions.load_new_malware()
        converted_test_data2, test_label2 = data_load_functions.load_new_malware_v2()
        converted_test_data3, test_label3 = data_load_functions.load_new_malware_v3()

        self.converted_test_data = np.concatenate(
            (converted_test_data1, converted_test_data2, converted_test_data3))
        self.test_label = np.concatenate((test_label1, test_label2, test_label3))
        print("length of new malware = " + str(len(self.test_label)))

        train_data, test_data, train_label, test_label = train_test_split(self.converted_test_data,
                                                                          self.test_label,
                                                                          test_size=self.new_malware_test_ratio,
                                                                          random_state=31)
        self.converted_train_data = np.concatenate((train_data, self.converted_train_data))
        self.train_label = np.concatenate((train_label, self.train_label))
        self.converted_test_data = test_data
        self.test_label = test_label
        self.converted_test_data = np.concatenate(
            (self.converted_test_data, benign_part_of_data))
        self.test_label = np.concatenate((self.test_label, benign_part_of_labels))

    # call this function fit entire dataset including new malware as training dataset
    def merge_whole_dataset(self):
        self.converted_train_data = np.concatenate((self.converted_train_data, self.converted_test_data))
        self.train_label = np.concatenate((self.train_label, self.test_label))
        converted_test_data1, test_label1 = data_load_functions.load_new_malware()
        converted_test_data2, test_label2 = data_load_functions.load_new_malware_v2()
        converted_test_data3, test_label3 = data_load_functions.load_new_malware_v3()

        self.converted_test_data = np.concatenate(
            (converted_test_data1, converted_test_data2, converted_test_data3))
        self.test_label = np.concatenate((test_label1, test_label2, test_label3))
        self.converted_train_data = np.concatenate((self.converted_train_data, self.converted_test_data))
        self.train_label = np.concatenate((self.train_label, self.test_label))

        print("length of new train data = " + str(len( self.converted_test_data)))

    # Use this function to load particular dataset or just by modifying self.converted_train_data, self.train_label,
    # self.converted_test_data, self.test_label within class instance will achieve the same. You also need to assign
    # embedding matrix and dictionary.
    def yield_dataset(self):

        self.converted_train_data, self.train_label, self.converted_test_data, self.test_label, \
        self.embedding_mat, diction, self.class_weight = \
            data_load_functions.load_data_v3_all_mixed_final_version(load_larger_test=False, modify_opcode=self.modify_jnz_jne)
        self.data_load_func_name = "load_data_v3_all_mixed_final_version(load_larger_test=False, modify_opcode=self.modify_jnz_jne)"
        self.merge_whole_dataset()

        print(len(np.where(self.train_label == 1)[0]))
        print(len(np.where(self.train_label == 0)[0]))

        print(len(np.where(self.test_label == 1)[0]))
        print(len(np.where(self.test_label == 0)[0]))

        self.feature_size = self.embedding_mat.shape[1]
        self.total_data_length = self.converted_train_data.shape[0]
        self.total_test_length = self.converted_test_data.shape[0]

    def tensorflow_normalize(self, tensor, epsilon=1e-8):
        mean, variance = tf.nn.moments(tensor, axes=[0])
        meantensor = tensor - tf.expand_dims(mean, 0)
        variancetensor = meantensor / (tf.sqrt(tf.expand_dims(variance, 0) + epsilon))
        return variancetensor

    def add_metrics(self, graph, unnormalized_scores, label_piece):
        with graph.as_default():
            binary_predictions = tf.argmax(unnormalized_scores, 1)
            correct = tf.equal(binary_predictions, tf.argmax(label_piece, 1))
            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
            graph.add_to_collection('acc', accuracy)
            graph.add_to_collection('preds', binary_predictions)

    def gaussian_noise_layer(self, input_layer, stddev):
        noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=stddev, dtype=tf.float32)
        return tf.add(input_layer, noise)

    def get_input_data_pipeline(self, graph):
        n_classes = 2
        with graph.as_default():
            one_hot_batch_train_label = self.label_smoothing(tf.one_hot(self.train_label, n_classes))

            dataset = tf.data.Dataset.from_tensor_slices(
                (self.converted_train_data, one_hot_batch_train_label))

            # dataset = dataset.shuffle(len(self.converted_train_data), reshuffle_each_iteration=True)
            # dataset = dataset.repeat(self.epochnum)
            batch_op_train = dataset.batch(self.batch_size)
            # batch_op_train = batch_op_train.prefetch(1)
            # batch_op_train = batch_op_train.apply(tf.data.experimental.prefetch_to_device('/device:GPU:%d' % (0)))
            # batch_op_train = batch_op_train.apply(prefetching_ops.copy_to_device("/gpu:0")).prefetch(1)

            iterator_train = tf.data.Iterator.from_structure(batch_op_train.output_types,
                                                             batch_op_train.output_shapes)

            dataset_init_op_train = iterator_train.make_initializer(batch_op_train, name='train_init')

            one_hot_batch_test_label = tf.one_hot(self.test_label, n_classes)
            dataset = tf.data.Dataset.from_tensor_slices(
                (self.converted_test_data, one_hot_batch_test_label))

            batch_op_test = dataset.batch(self.test_batch_size)
            batch_op_test = batch_op_test.prefetch(1)

            iterator_test = tf.data.Iterator.from_structure(batch_op_test.output_types,
                                                            batch_op_test.output_shapes)
            dataset_init_op_test = iterator_test.make_initializer(batch_op_test, name='test_init')

        return iterator_train, iterator_test

    def get_only_train_data_pipeline(self, graph):
        n_classes = 2
        with graph.as_default():
            one_hot_batch_train_label = self.label_smoothing(tf.one_hot(self.train_label, n_classes))

            dataset = tf.data.Dataset.from_tensor_slices(
                (self.converted_train_data, one_hot_batch_train_label))

            # dataset = dataset.shuffle(len(self.converted_train_data), reshuffle_each_iteration=True)
            # dataset = dataset.repeat(self.epochnum)
            batch_op_train = dataset.batch(self.batch_size)

            iterator_train = tf.data.Iterator.from_structure(batch_op_train.output_types,
                                                             batch_op_train.output_shapes)

            dataset_init_op_train = iterator_train.make_initializer(batch_op_train, name='train_init')

            return iterator_train

    def fetch_data_from_pipeline(self, graph, is_training, iterator_train, iterator_test):
        with graph.as_default():
            if is_training:
                data_piece, label_piece = iterator_train.get_next()
            else:
                data_piece, label_piece = iterator_test.get_next()
            return data_piece, label_piece

    def save_graph(self, graph, session, new_saver=None):
        with graph.as_default():
            save_dir = helper_functions.cnn_model_save(self.root_dir, 0, self.batch_size, self.model_type_name,
                                                       self.model_name, data_load_func=self.data_load_func_name,
                                                       save_name=self.save_name)

            print("save_dir :" + save_dir)
            if not self.new_model:
                new_saver.save(session, save_dir)
            else:
                graph.get_collection_ref('saver')[0].save(session, save_dir)

    def apply_attention(self, graph, inputs, attention_size, return_alphas=False):
        with graph.as_default():
            hidden_size = inputs.shape[2].value
            # Trainable parameters
            w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))
            b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))
            u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))

            with tf.name_scope('v'):
                v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)

            vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape
            alphas = tf.nn.softmax(vu, name='alphas')  # (B,T) shape

            output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)

            if not return_alphas:
                return output
            else:
                return output, alphas

    def cluster_k_means(self, encoded_vec):
        all_labels = np.concatenate((self.train_label, self.test_label))
        cluster_labels = ml_algorithms.k_means_clustering(encoded_vec)
        matching_rate = len(np.where(cluster_labels == all_labels)[0]) / len(cluster_labels)
        print('matching_rate = ' + str(matching_rate))
        helper_functions.plot_clustering_labels(cluster_labels, encoded_vec, all_labels)

    def run_ml_classification(self, train_features, test_features, test_scores, opt_thres):

        helper_functions.results_new_threshold(test_scores[:, 1], self.test_label, opt_thres, 0, self.model_name,
                                               self.resample_dataset)

        parameters_original = (train_features, self.train_label, test_features, self.test_label, self.model_name)
        file = {"file_write": True, "use_count_data": False}

        print('resample_dataset = ' + str(self.resample_dataset))
        print('feature_selection = ' + str(self.use_feature_selection))

        if self.use_feature_selection:
            train_features, test_features = ml_algorithms.boruta_feature_selection(train_features,
                                                                                   test_features, self.train_label,
                                                                                   dataset_type='sequence')
        if self.resample_dataset:
            resampled_features, resampled_labels = ml_algorithms.resample_dataset(train_features, self.train_label,
                                                                                  mode='smote')

            resampled_test_features, resampled_test_labels = ml_algorithms.resample_dataset(test_features,
                                                                                            self.test_label,
                                                                                            mode='smote')
            print(resampled_features.shape)
            parameters = (resampled_features, resampled_labels, test_features, self.test_label, self.model_name)
        else:
            parameters = parameters_original

        print('starting ml classification')
        if self.use_cross_validation:
            print('starting ml classification with cross validation')
            ml_algorithms.hyperparameter_random_search(*parameters, class_type='knn', **file)
            ml_algorithms.hyperparameter_random_search(*parameters, class_type='svm', **file)
            ml_algorithms.hyperparameter_random_search(*parameters, class_type='rf', **file)
        else:
            ml_algorithms.svm_classification(*parameters, **file)
            ml_algorithms.random_forest(*parameters, **file)
            ml_algorithms.knearest_neighbors(*parameters_original, **file)
            # ml_algorithms.xgb_classifier(*parameters, **file)

        if self.do_k_mean_cluster:
            print('starting clustering')
            cluster_labels = ml_algorithms.k_means_clustering(test_features)
            cluster_labels_flipped = np.logical_xor(cluster_labels, np.ones(len(cluster_labels)))
            matching_rate = len(np.where(cluster_labels_flipped == self.test_label)[0]) / len(cluster_labels)
            model_dir = self.root_dir + '\\' + self.model_name
            with open(model_dir + '_class_results.txt', 'a') as file:
                file.write('k_means results = ' + '\n')
                file.write('matching_rate = ' + str(matching_rate))
            print('matching_rate = ' + str(matching_rate))
            helper_functions.plot_clustering_labels(cluster_labels_flipped, test_features, self.test_label)

        if self.get_visualizations:
            # helper_functions.visualize_features(train_features, self.train_label, mode='train' , method = 'pca',
            # dimension='3d') helper_functions.visualize_features(train_features, self.train_label, mode='train',
            # method = 'tsne',dimension='3d')
            helper_functions.visualize_features(train_features, self.train_label, test_features, self.test_label,
                                                name=self.model_name, mode='train',
                                                method='tsne', dimension='2d')
            helper_functions.visualize_features(train_features, self.train_label, test_features, self.test_label,
                                                name=self.model_name, mode='train',
                                                method='pca', dimension='2d')
            # helper_functions.visualize_features(train_features, self.train_label, name =
            # self.model_name, mode='train',
            # method='tsne',dimension='2d')
            # helper_functions.visualize_features(test_features, self.test_label, test_features, self.test_label,
            #                                     name=self.model_name, mode='test',
            #                                     method='pca', dimension='2d')
            # helper_functions.visualize_features(train_features, self.train_label, mode='test',
            # method = 'pca', dimension='3d')

        # self.new_malware_predict_dataset()

        # self.converted_train_data, self.train_label, self.converted_test_data, self.test_label, self.embedding_mat,
        # class_weight = \ data_load_functions.prapare_data( self.use_imbalanced)
        #

        # malware label 1
        # benign  label 0

        # self.converted_train_data, self.train_label, self.converted_test_data, self.test_label, self.embedding_mat,
        # diction, class_weight = \ data_load_functions.load_data()

        # self.converted_train_data, self.train_label, converted_test_data1, test_label1, self.embedding_mat,
        # diction, class_weight = \ data_load_functions.load_data_v2()

        # converted_test_data2, test_label2, self.embedding_mat, diction = \
        #     helper_functions.load_new_benign_data()
        # self.converted_test_data = np.int32(converted_test_data2)
        # self.test_label = test_label2
        #

        # self.converted_train_data, self.train_label, self.converted_test_data, self.test_label, \
        # self.embedding_mat, diction, class_weight = \
        #     data_load_functions.sample_of_2000_trial()
        #

        # self.converted_train_data = np.concatenate((self.converted_train_data, self.converted_test_data))
        # self.train_label = np.concatenate((self.train_label, self.test_label))

        # selected_test_data = self.converted_test_data[:20, :]
        # selected_test_label = self.test_label[:20]
        # self.converted_train_data = np.concatenate((self.converted_train_data, selected_test_data))
        # self.train_label = np.concatenate((self.train_label, selected_test_label))
        # self.converted_test_data = self.converted_test_data[20:, :]
        # self.test_label = self.test_label[20:]

        # self.step_num = int(len(self.converted_train_data) / self.batch_size)
        #

        # self.converted_train_data, self.train_label, self.converted_test_data, self.test_label,
        # self.embedding_mat, diction, class_weight = \
        #     data_load_functions.load_data_v_predict_only_new_benign(modify_opcode=self.modify_jnz_jne,
        #                                                             load_artifical_included=True)
        # self.step_num = int(len(self.converted_train_data) / self.batch_size)

        #
        # self.converted_train_data, self.train_label, self.converted_test_data, self.test_label, self.embedding_mat,
        # diction, class_weight = \ data_load_functions.load_data_v5()

        #
        # self.converted_test_data = np.concatenate((converted_test_data2, converted_test_data1))
        # self.test_label = np.concatenate((test_label2, test_label1))
