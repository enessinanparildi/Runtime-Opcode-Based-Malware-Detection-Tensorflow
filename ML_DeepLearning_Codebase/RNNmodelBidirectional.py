import numpy as np
import tensorflow as tf


import ml_algorithms
import opcode_embeddings
import helper_functions



# Also equals embedding size


def apply_attention(inputs, attention_size, return_alphas=False):
    # https://github.com/ilivans/tf-rnn-attention/blob/master/attention.py


    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer

    # Trainable parameters
    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))
    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))
    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))

    with tf.name_scope('v'):
        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;
        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size
        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)

    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector
    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape
    alphas = tf.nn.softmax(vu, name='alphas')  # (B,T) shape

    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape
    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)

    if not return_alphas:
        return output
    else:
        return output, alphas



def bidirectional_lstm_graph_run():

    # embedding_mat = normalize(embedding_mat)

    new_model = False
    resample_dataset = False
    use_imbalanced = True
    use_attention = True
    use_peepholes = True
    train_model = True
    save_results = True
    only_forward = True

    model_name = 'model_v11'
    save_name  = 'model_v11'
    root_dir = 'D:\\thesis_code_base\\saveddata\\Networks\\lstm_bidirectional'

    num_layers = 2
    max_epoch = 5
    keep_prob = 0.5

    batch_size = 64
    test_batch_size = 256

    save_step = 1

    lstmfw_size = 64
    lstmbw_size = 64
    attention_size = 70
    rate = 0.001
    n_classes = 2
    split_length = 200

    if only_forward:
        max_epoch = 1

    root_dir_data = 'D:\\thesis_code_base\\saveddata\\Embedding_matrix_networks\\'



    converted_train_data, train_label, converted_test_data, test_label, embedding_mat, class_weight = helper_functions.prapare_data(use_imbalanced)


    total_seq_length = 1000
    converted_train_data = converted_train_data[0:int(0.95 * len(converted_train_data)), :]
    train_label = train_label[0:int(0.95 * len(train_label))]

    feature_size = embedding_mat.shape[1]
    total_data_length = converted_train_data.shape[0]
    total_test_length = converted_test_data.shape[0]

    num_steps = int(total_seq_length / split_length)


    tf.reset_default_graph()
    lstm_graph = tf.Graph()

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True


    with lstm_graph.as_default():
        weight = tf.Variable(tf.truncated_normal([lstmfw_size + lstmbw_size, n_classes]) , name = 'weigth')
        bias = tf.Variable(tf.constant(0.1, shape=[n_classes]),name = 'weigth2')
        variables_names = [v.name for v in tf.trainable_variables()]

        one_hot_batch_train_label = tf.one_hot(train_label, n_classes)
        one_hot_batch_test_label = tf.one_hot(test_label, n_classes)

        training_dataset = tf.data.Dataset.from_tensor_slices((converted_train_data, one_hot_batch_train_label))
        test_dataset = tf.data.Dataset.from_tensor_slices((converted_test_data, one_hot_batch_test_label))

        train_batches = training_dataset.batch(batch_size)
        test_batches = test_dataset.batch(test_batch_size)

        handle = tf.placeholder(tf.string, shape=[])
        iterator = tf.data.Iterator.from_string_handle(
            handle, train_batches.output_types, train_batches.output_shapes)

        data_piece, label_piece = iterator.get_next()
        embedded_mat_piece = tf.nn.embedding_lookup(tf.convert_to_tensor(embedding_mat), data_piece)
        embedded_mat_piece = tf.expand_dims(embedded_mat_piece, 3)

        # input_tensor = tf.placeholder("float", [None, split_length, feature_size, num_steps], name='input_placeholder')
        # label_tensor = tf.placeholder("float", [None, n_classes], name='labels_placeholder')
        batch_size_tensor = tf.placeholder_with_default(batch_size, shape=[], name=None)
        keep_prob_tensor = tf.placeholder_with_default(keep_prob, shape=())
        batch_norm_switch = tf.placeholder(tf.bool,shape= ())


        def create_one_phased_fw_cell():
            lstm_fw_cell = tf.contrib.rnn.PhasedLSTMCell(lstmfw_size, use_peepholes=True)
            if keep_prob < 1.0:
                lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell, output_keep_prob=keep_prob_tensor)
            return lstm_fw_cell

        def create_one_phased_bw_cell():
            lstmbw_cell = tf.contrib.rnn.PhasedLSTMCell(lstmbw_size, use_peepholes=True)
            if keep_prob < 1.0:
                lstmbw_cell = tf.contrib.rnn.DropoutWrapper(lstmbw_cell, output_keep_prob=keep_prob_tensor)
            return lstmbw_cell

        cell_fw_list = [create_one_phased_fw_cell() for _ in range(num_layers)]
        cell_bw_list = [create_one_phased_bw_cell() for _ in range(num_layers)]

        init_state_list_fw = [cell.zero_state(batch_size_tensor, dtype=tf.float32) for cell in cell_fw_list]
        init_state_list_bw = [cell.zero_state(batch_size_tensor, dtype=tf.float32) for cell in cell_bw_list]

        all_time_output = []
        (all_time_output_piece, out_state_list_fw, out_state_list_bw) = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cell_fw_list, cell_bw_list, input_tensor[:, :, :, i], dtype=tf.float32)

        all_time_output.append(all_time_output_piece)

            init_state_list_fw = [out_state_list_fw[j] for j in range(num_layers)]
            init_state_list_bw = [out_state_list_bw[j] for j in range(num_layers)]

        all_time_output = tf.concat(all_time_output, axis=1, name='concat')



        final_state_fw = init_state_list_fw
        final_state_bw = init_state_list_bw

        # for time vector averaging
        if use_attention:
            attention_output = apply_attention(all_time_output, attention_size)
            time_average_output = attention_output
        else:
            time_average_output = tf.reduce_mean(all_time_output, 1)


        # concatoutputvec = tf.transpose(concatoutputvec, [1, 0, 2])
        # Choosing only last output for sequence classification
        # last = tf.gather(concatoutputvec, int(concatoutputvec.get_shape()[0]) - 1, name="last_lstm_output")

        # Softmax regression multi- class logistic regression
        weight = tf.Variable(tf.truncated_normal([lstmfw_size + lstmbw_size, n_classes]))
        bias = tf.Variable(tf.constant(0.1, shape=[n_classes]))

        prediction_score = tf.matmul(time_average_output, weight) + bias
        # All time averaging
        # use only final state
        # all_final_state = tf.concat((final_state_fw[0].h,final_state_bw[0].h) ,axis = 1, name='concat' )
        # prediction_vector = tf.matmul(all_final_state, weight) + bias

        # labels one hot
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction_score, labels=label_tensor))
        optimizer = tf.train.AdamOptimizer(learning_rate= rate).minimize(loss)

        binary_predictions = tf.argmax(prediction_score, 1)
        correct = tf.equal(binary_predictions, tf.argmax(label_tensor, 1))
        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))


        saver = tf.train.Saver()


        config = tf.ConfigProto(
            device_count={'GPU': 0}
        )

    with tf.Session(graph=lstmbidirec_graph ) as sess:

        tf.global_variables_initializer().run()

        # Restoring
        if (not new_model):
            new_saver = tf.train.import_meta_graph(root_dir + '\\' + model_name + '.meta' )
            new_saver.restore(sess, root_dir + '\\' + model_name)

            # chkpdir = bidired_model_dir_pull(root_dir,index = 5)
            # chkp = tf.train.get_checkpoint_state(chkpdir)
            # if chkp and  chkp.model_checkpoint_path:
            #     saver = tf.train.import_meta_graph(chkp.model_checkpoint_path + '.meta')

            #     saver.restore(sess, chkp.model_checkpoint_path)

        query_outputs_train = [optimizer, loss, accuracy, binary_predictions, time_average_output]
        query_outputs_test = [loss, accuracy, binary_predictions, time_average_output, prediction_score]
        query_outputs_feature_extract = [time_average_output]

        placeholders = (input_tensor , label_tensor , batch_size_tensor , keep_prob_tensor,batch_norm_switch )

        if only_forward:
            query_outputs_train = query_outputs_feature_extract

        for epoch_step in range(max_epoch):


            epoch_loss = 0
            epoch_accuracy = 0

            total_batch, remain = divmod(total_data_length, batch_size)
            final_predictions = np.ones(0)
            train_features = np.ones(0)
            train_scores = np.ones(0)

            if train_model:

                for i in range(total_batch):

                    print(i)
                    train_loss, train_accuracy, train_predictions, train_batch_features  = helper_functions.batch_iteration(
                                                                                                         train_data,
                                                                                                         one_hot_batch_train_label,
                                                                                                         embedding_mat,
                                                                                                         dictionary, i,
                                                                                                         batch_size,
                                                                                                         remain,
                                                                                                         query_outputs_train,
                                                                                                         *placeholders,
                                                                                                         only_forward_pass= only_forward,
                                                                                                         use_random_balanced_batch = False,
                                                                                                         mode="train",
                                                                                                         session=sess,
                                                                                                         final=False)
                    print(train_batch_features)

                    final_predictions = np.append(final_predictions, train_predictions)
                    train_batch_scores = 0
                    if not (isinstance(train_batch_features, int)):
                        if i == 0:
                            train_features = train_batch_features[0]
                        else:
                            train_features = np.concatenate((train_features, train_batch_features[0]))

                    # avg_cost = train_loss / int(batch_size)

                    avg_cost = train_loss
                    epoch_loss = epoch_loss + avg_cost

                    epoch_accuracy = epoch_accuracy + train_accuracy * batch_size

                train_loss, train_accuracy, train_predictions, train_batch_features  = helper_functions.batch_iteration(
                                                                                                     train_data,
                                                                                                     one_hot_batch_train_label,
                                                                                                     embedding_mat,
                                                                                                     dictionary,
                                                                                                     total_batch,
                                                                                                     batch_size, remain,
                                                                                                     query_outputs_train,
                                                                                                     *placeholders,
                                                                                                     only_forward_pass = only_forward,
                                                                                                     use_random_balanced_batch=False,
                                                                                                     mode="train",
                                                                                                     session=sess,
                                                                                                     final=True)
                final_predictions = np.append(final_predictions, train_predictions)

                train_batch_scores = 0
                if not (isinstance(train_batch_features, int)):
                    train_features = np.concatenate((train_features, train_batch_features[0]))

                # avg_cost = train_loss / int(remain)
                avg_cost = train_loss

                epoch_loss = epoch_loss + avg_cost
                epoch_accuracy = epoch_accuracy + train_accuracy * remain

            if epoch_step % save_step == 0:

                epoch_accuracy = epoch_accuracy / total_data_length
                total_batch, remain = divmod(total_test_length, test_batch_size)

                final_test_predictions = np.ones(0)
                test_features = np.ones(0)
                test_scores = np.ones(0)
                epoch_test_loss = 0
                epoch_test_accuracy = 0

                for i in range(total_batch):

                    test_loss, test_accuracy, test_predictions, test_batch_features, test_batch_scores = helper_functions.batch_iteration(
                        test_data,
                        one_hot_batch_test_label,
                        embedding_mat,
                        dictionary, i,
                        test_batch_size,
                        remain,
                        query_outputs_test,
                        *placeholders,
                        only_forward_pass=only_forward,
                        use_random_balanced_batch=False,
                        mode='test',
                        session=sess,
                        final=False)

                    final_test_predictions = np.append(final_test_predictions, test_predictions)

                    if i == 0:
                        test_features = test_batch_features
                        test_scores = test_batch_scores
                    else:
                        test_features = np.concatenate((test_features, test_batch_features))
                        test_scores = np.concatenate((test_scores, test_batch_scores))

                    # avg_cost = test_loss / int(batch_size)
                    avg_cost = test_loss

                    epoch_test_loss = epoch_test_loss + avg_cost
                    epoch_test_accuracy = epoch_test_accuracy + test_accuracy * test_batch_size



                test_loss, test_accuracy, test_predictions, test_batch_features, test_batch_scores = helper_functions.batch_iteration(
                    test_data,
                    one_hot_batch_test_label,
                    embedding_mat,
                    dictionary,
                    total_batch,
                    test_batch_size,
                    remain,
                    query_outputs_test,
                    *placeholders,
                    only_forward_pass=only_forward,
                    use_random_balanced_batch=False,
                    mode='test',
                    session=sess,
                    final=True)

                final_test_predictions = np.append(final_test_predictions, test_predictions)
                test_features = np.concatenate((test_features, test_batch_features))
                test_scores = np.concatenate((test_scores, test_batch_scores))

                # avg_cost = test_loss / int(remain)
                avg_cost = test_loss
                epoch_test_loss = epoch_test_loss + avg_cost
                epoch_test_accuracy = epoch_test_accuracy + test_accuracy * remain
                epoch_test_accuracy = epoch_test_accuracy / total_test_length




            helper_functions.print_result(epoch_step, epoch_loss, epoch_accuracy,
                                          epoch_test_loss, epoch_test_accuracy,
                                          final_test_predictions,
                                          test_label, model_name,resample_dataset)

            if only_forward:
                parameters_original = (train_features, train_label, test_features, test_label, model_name)
                file = {"file_write": True}

                print(train_features.shape)
                if resample_dataset:
                    resampled_features,resampled_labels =  ml_algorithms.resample_dataset(train_features, train_label, mode = 'smote')
                    parameters = (resampled_features,resampled_labels , test_features, test_label, model_name)
                #
                print("resample_dataset = " + str(resample_dataset) )
                ml_algorithms.svm_classification(*parameters_original, **file)
                ml_algorithms.random_forest(*parameters_original, **file)
                ml_algorithms.gradient_boosting(*parameters_original, **file)
                ml_algorithms.quadratic_discriminant_classifier(*parameters_original, **file)
                ml_algorithms.knearest_neighbors(*parameters_original, **file)

            # helper_functions.calculate_auc_score(test_scores, test_label)
            # helper_functions.visualize_features(train_features, train_label, mode='train' , method = 'pca',dimension='2d')
            # # helper_functions.visualize_features(train_features, train_label, mode='test',method = 'tsne',dimension='3d')
            # helper_functions.visualize_features(train_features, train_label, mode='train',method = 'tsne', dimension='3d')
            # helper_functions.visualize_features(train_features, train_label, mode='train',method = 'pca', dimension='3d')
            # #

        if save_results:
            save_dir = helper_functions.bidirectional_model_save(root_dir, train_loss, train_accuracy, test_loss, test_accuracy, max_epoch,
                                          keep_prob, lstmfw_size, lstmbw_size, num_layers, model_name, attention_size , save_name = save_name)
            print("save_dir :" + save_dir)
            saver.save(sess, save_dir)
























