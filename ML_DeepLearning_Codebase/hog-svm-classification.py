import cv2
import ml_algorithms
import helper_functions
import tensorflow as tf
import numpy as np
import data_load_functions


def hog_computation(data_image):
    winSize = (64, 64)
    blockSize = (16, 16)
    blockStride = (8, 8)
    cellSize = (8, 8)
    nbins = 9
    derivAperture = 1
    winSigma = 4.
    histogramNormType = 0
    L2HysThreshold = 2.0000000000000001e-01
    gammaCorrection = 0
    nlevels = 64
    hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins, derivAperture, winSigma,
                            histogramNormType, L2HysThreshold, gammaCorrection, nlevels)
    # compute(img[, winStride[, padding[, locations]]]) -> descriptors
    winStride = (8, 8)
    padding = (8, 8)
    locations = ((10, 20),)
    hist = hog.compute(data_image, winStride, padding, locations)
    return hist


def run_ml_classification(train_features, test_features, train_label, test_label):
    model_name = 'hog_features'
    parameters = (train_features, train_label, test_features, test_label, model_name)
    file = {"file_write": True, "use_count_data": False}

    print('starting ml classification')
    ml_algorithms.svm_classification(*parameters, **file)
    ml_algorithms.random_forest(*parameters, **file)
    ml_algorithms.knearest_neighbors(*parameters, **file)
    ml_algorithms.xgb_classifier(*parameters, **file)


def tensorflow_normalize(tensor, epsilon=1e-8):
    mean, variance = tf.nn.moments(tensor, axes=[0])
    meantensor = tensor - tf.expand_dims(mean, 0)
    variancetensor = meantensor / (tf.sqrt(tf.expand_dims(variance, 0) + epsilon))
    return variancetensor


def histogram_mapping(batch_data):
    seq_size = 1000
    embedding_size = 64
    data = batch_data.flatten()
    ref = np.histogram_bin_edges(data, bins=255)
    resbin = np.digitize(data, ref, right=True)
    return resbin.reshape((seq_size, embedding_size))


def yield_dataset_pipeline(graph, is_training=True):
    converted_train_data, train_label, converted_test_data, test_label, embedding_mat, diction, class_weight = \
        data_load_functions.load_data_v3_all_mixed(False)
    whole_data = np.concatenate((converted_train_data, converted_test_data))
    seq_size = 1000
    embedding_size = 64
    with graph.as_default():
        dataset = tf.data.Dataset.from_tensor_slices((whole_data))
        dataset = dataset.batch(batch_size=64)
        dataset = dataset.prefetch(1)
        iterator = dataset.make_one_shot_iterator()
        data_piece = iterator.get_next()
        embedded_mat_piece = tf.nn.embedding_lookup(tf.convert_to_tensor(embedding_mat), data_piece)
        embedded_mat_piece = tf.expand_dims(embedded_mat_piece, 3)
        embedded_mat_piece_norm = tensorflow_normalize(embedded_mat_piece)
        tf.map_fn()
        with tf.Session() as sess:
            tf.global_variables_initializer().run()
            image_type_data = sess.run([embedded_mat_piece_norm], feed_dict={data_piece: whole_data})
    all_images = [np.histogram(image, bins=256) for image in image_type_data]

    return image_type_data
