#
import numpy as np
import tensorflow as tf
from random import randint
import math as math



tf.reset_default_graph()

trainingsize = 100000
testsize = 1000

# Parameters
learning_rate = 0.01
training_epochs = 6
batch_size = 100
display_step = 1
beta = 0.001

snrdbvec = np.arange(0, 9)
SNR = 10 ** (snrdbvec / 10)
N0 = 1 / SNR
sigma = np.sqrt(N0 * 0.5)

chosentestsnrindex = 2

tf.reset_default_graph()
sess = tf.InteractiveSession()

traininginfoseq = generateInputVectors(blocklength, trainingsize)
trainingencodedseq = np.array(list(map(encode133171, traininginfoseq.tolist())))

snrnumeachclass = int(trainingsize / len(snrdbvec))

for i in range(trainingsize):
    trainingencodedseq[i, :] = modulateAwgn(trainingencodedseq[i, :], sigma[chosentestsnrindex])

traininginfoseq[np.where(traininginfoseq == 0)] = -1

testinfoseq = generateInputVectors(blocklength, testsize)
testcodedseq = np.array(list(map(encode133171, testinfoseq.tolist())))
for i in range(testsize):
    testcodedseq[i, :] = modulateAwgn(testcodedseq[i, :], sigma[chosentestsnrindex])

testinfoseq[np.where(testinfoseq == 0)] = -1

PAD = 0
EOS = 1

vocab_size = 1

encoder_hidden_units = 20
decoder_hidden_units = encoder_hidden_units

encoder_inputs = tf.placeholder(shape=(2 * blocklength, None), dtype=tf.float32, name='encoder_inputs')
decoder_targets = tf.placeholder(shape=(2 * blocklength, None), dtype=tf.float32, name='decoder_targets')
decoder_inputs = tf.placeholder(shape=(2 * blocklength, None), dtype=tf.float32, name='decoder_inputs')

decoder_targets = tf.expand_dims(decoder_targets, 2)
encoder_inputs = tf.expand_dims(encoder_inputs, 2)
decoder_inputs = tf.expand_dims(decoder_inputs, 2)

encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)

encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(
    encoder_cell, encoder_inputs,
    dtype=tf.float32, time_major=True,
)

del encoder_outputs

decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)

decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(
    decoder_cell, decoder_inputs,
    initial_state=encoder_final_state,
    dtype=tf.float32, time_major=True, scope="plain_decoder",
)

decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)

decoder_prediction = tf.argmax(decoder_logits, 2)

loss = tf.reduce_mean(tf.squared_difference(decoder_targets, decoder_logits))

train_op = tf.train.AdamOptimizer().minimize(loss)

batch_size = 1000


def next_feed_training(batch_size, epochind):
    encoder_inputs_ = trainingencodedseq[epochind * batch_size: (epochind + 1) * batch_size, :]
    decoder_targets_ = np.append(traininginfoseq[epochind * batch_size: (epochind + 1) * batch_size, :],
                                 np.tile(np.concatenate(([EOS], np.zeros(99))), (1000, 1)), axis=1)
    decoder_inputs_ = np.append(np.tile([EOS], (1000, 1)),
                                traininginfoseq[epochind * batch_size: (epochind + 1) * batch_size, :], axis=1)
    decoder_inputs_ = np.append(decoder_inputs_, np.zeros((1000, 99)), axis=1)

    encoder_inputs_ = np.transpose(encoder_inputs_)
    encoder_inputs_ = np.expand_dims(encoder_inputs_, axis=2)
    decoder_targets_ = np.transpose(decoder_targets_)
    decoder_targets_ = np.expand_dims(decoder_targets_, axis=2)
    decoder_inputs_ = np.transpose(decoder_inputs_)
    decoder_inputs_ = np.expand_dims(decoder_inputs_, axis=2)

    return {
        encoder_inputs: encoder_inputs_,
        decoder_inputs: decoder_inputs_,
        decoder_targets: decoder_targets_,
    }


def next_feed_test(batch_size, epochind):
    encoder_inputs_ = testcodedseq[epochind * batch_size: (epochind + 1) * batch_size, :]
    decoder_targets_ = np.append(testinfoseq[epochind * batch_size: (epochind + 1) * batch_size, :],
                                 np.tile(np.concatenate(([EOS], np.zeros(99))), (1000, 1)), axis=1)
    decoder_inputs_ = np.append(np.tile([EOS], (1000, 1)),
                                testinfoseq[epochind * batch_size: (epochind + 1) * batch_size, :], axis=1)
    decoder_inputs_ = np.append(decoder_inputs_, np.zeros((1000, 99)), axis=1)

    encoder_inputs_ = np.transpose(encoder_inputs_)
    encoder_inputs_ = np.expand_dims(encoder_inputs_, axis=2)
    decoder_targets_ = np.transpose(decoder_targets_)
    decoder_targets_ = np.expand_dims(decoder_targets_, axis=2)
    decoder_inputs_ = np.transpose(decoder_inputs_)
    decoder_inputs_ = np.expand_dims(decoder_inputs_, axis=2)

    return {
        encoder_inputs: encoder_inputs_,
        decoder_inputs: decoder_inputs_,
        decoder_targets: decoder_targets_,
    }


trainingpredictedset = np.zeros((trainingsize, blocklength))
sess.run(tf.global_variables_initializer())
epochnum = 5
batches_in_epoch = int(trainingsize / batch_size)
max_batches = batches_in_epoch * epochnum
batchindex = 0
ind = 0
loss_track = []
try:
    for batch in range(max_batches):
        fd = next_feed_training(batch_size, batchindex)
        _, l = sess.run([train_op, loss], fd)
        loss_track.append(l)
        batchindex = batchindex + 1
        if batch == 0 or batchindex - batches_in_epoch == 0:

            batchindex = 0
            print('batch {}'.format(batch))
            print('  minibatch loss: {}'.format(sess.run(loss, fd)))
            predict_ = sess.run(decoder_prediction, fd)
            ind = ind + 1
            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):
                print('  sample {}:'.format(i + 1))
                print('    input     > {}'.format(inp))
                print('    predicted > {}'.format(pred))
                if i >= 2:
                    break
            print()
    testdata = next_feed_test(testsize, 0)
    testprediction = sess.run(decoder_prediction, testdata)

except KeyboardInterrupt:
    print('training interrupted')

import matplotlib.pyplot as plt

plt.plot(loss_track)
print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track) * batch_size, batch_size))

testprediction = np.transpose(testprediction)
testprediction = testprediction[:, 0:100]
biterrnum = 0
for j in range(len(testprediction)):
    biterrnum = biterrnum + np.sum(np.logical_xor(testprediction[j, :], testinfoseq[j, :]))

ber = biterrnum / (len(testprediction) * blocklength)

print("ber: ", ber)
print("snrdb : ", snrdbvec[chosentestsnrindex])


