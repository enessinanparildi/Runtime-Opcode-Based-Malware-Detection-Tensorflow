import tensorflow as tf
from tensorflow import keras


def keras_autoencoder(input_img):

    x = keras.layers.Conv2D(16, (5, 5), activation='elu', padding='same')(input_img)
    x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)
    x = keras.layers.Conv2D(16, (5, 5), activation='elu', padding='same')(x)
    x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)
    x = keras.layers.Conv2D(8, (5, 5), activation='elu', padding='same')(x)
    x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)
    x = keras.layers.Conv2D(8, (5, 5), activation='elu', padding='same')(x)
    x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)
    x = keras.layers.Conv2D(4, (5, 5), activation='elu', padding='same')(x)
    x = keras.layers.MaxPooling2D((5, 5), padding='same')(x)
    x = keras.layers.Conv2D(4, (5, 5), activation='elu', padding='same')(x)
    encoded = keras.layers.Conv2D(4, (5, 5), activation='elu', padding='same')(x)
    # at this point the representation is (4, 4, 8) i.e. 128-dimensional
    print(encoded.shape)

    x = keras.layers.Conv2DTranspose(4, (5, 5), activation='elu', padding='same')(encoded)
    x = keras.layers.Conv2DTranspose(4, (5, 5), activation='elu', padding='same')(x)
    x = keras.layers.UpSampling2D((5, 5))(x)
    x = keras.layers.Conv2DTranspose(4, (5, 5), activation='elu', padding='same')(x)
    x = keras.layers.UpSampling2D((2, 2))(x)
    x = keras.layers.Conv2DTranspose(8, (5, 5), activation='elu', padding='same')(x)
    x = keras.layers.UpSampling2D((2, 2))(x)
    x = keras.layers.Conv2DTranspose(8, (5, 5), activation='elu', padding='same')(x)
    x = keras.layers.UpSampling2D((2, 2))(x)
    x = keras.layers.Conv2DTranspose(16, (5, 5), activation='elu', padding='same')(x)
    x = keras.layers.UpSampling2D((2, 2))(x)
    x = keras.layers.Conv2DTranspose(16, (5, 5), activation='elu', padding='same')(x)
    decoded = keras.layers.Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)
    print(decoded.shape)
    name = 'keras_autoencoder'
    return tf.squeeze(decoded), encoded, name


class cnnmodelgraphs:
    n_classes = 2

    def __init__(self, graph, keep_prob_fc, keep_prob_conv, batch_norm_switch, use_xavier_initial=True):
        self.graph = graph
        self.keep_prob_fc = keep_prob_fc
        self.keep_prob_conv = keep_prob_conv
        self.mode = batch_norm_switch
        self.use_xavier_initial = use_xavier_initial
        n_classes = 2

        with self.graph.as_default():
            self.weights = {
                # 5 x 5 convolution, 1 input image, 32 outputs
                'W_conv1': tf.Variable(tf.truncated_normal([5, 5, 1, 16], stddev=0.01)),
                # 5x5 conv, 32 inputs, 64 outputs
                'W_conv2': tf.Variable(tf.truncated_normal([5, 5, 16, 32], stddev=0.01)),

                'W_conv3': tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.01)),

                'W_fc': tf.Variable(tf.truncated_normal([125 * 13 * 64, 512], stddev=0.01)),
                # 1024 inputs, 10 outputs (class prediction)
                'out': tf.Variable(tf.truncated_normal([512, n_classes], stddev=0.01))
            }

            self.biases = {
                'b_conv1': tf.Variable(tf.random_normal([16])),
                'b_conv2': tf.Variable(tf.random_normal([32])),
                'b_conv3': tf.Variable(tf.random_normal([64])),
                'b_fc': tf.Variable(tf.random_normal([512])),
                'out': tf.Variable(tf.random_normal([n_classes]))
            }

            self.secondweights = {
                # 5 x 5 convolution, 1 input image RGB, 32 outputs
                'W_conv1': tf.Variable(tf.random_normal([5, 5, 1, 32])),
                # 5x5 conv, 32 inputs, 64 outputs
                'W_conv2': tf.Variable(tf.random_normal([5, 5, 32, 64])),

                'W_conv3': tf.Variable(tf.random_normal([5, 5, 64, 64])),

                'W_conv4': tf.Variable(tf.random_normal([5, 5, 64, 64])),

                'W_fc': tf.Variable(tf.random_normal([63 * 7 * 64, 1024])),

                'W_fc2': tf.Variable(tf.random_normal([1024, 256])),

                'out': tf.Variable(tf.random_normal([256, n_classes]))
            }
            self.secondbiases = {
                'b_conv1': tf.Variable(tf.random_normal([32])),
                'b_conv2': tf.Variable(tf.random_normal([64])),
                'b_conv3': tf.Variable(tf.random_normal([64])),
                'b_conv4': tf.Variable(tf.random_normal([64])),

                'b_fc': tf.Variable(tf.random_normal([1024])),

                'b_fc2': tf.Variable(tf.random_normal([256])),

                'out': tf.Variable(tf.random_normal([n_classes]))
            }

            if self.use_xavier_initial:
                initial = tf.contrib.layers.xavier_initializer(uniform=False, dtype=tf.float32)

                self.weights = {
                    # 5 x 5 convolution, 1 input image, 32 outputs
                    'W_conv1': tf.get_variable(name='W_conv1', shape=[5, 5, 1, 16], initializer=initial),
                    # 5x5 conv, 32 inputs, 64 outputs
                    'W_conv2': tf.get_variable(name='W_conv2', shape=[5, 5, 16, 32], initializer=initial),

                    'W_conv3': tf.get_variable(name='W_conv3', shape=[5, 5, 32, 64], initializer=initial),

                    'W_fc': tf.get_variable(name='W_fc', shape=[125 * 8 * 64, 512], initializer=initial),
                    # 1024 inputs, 10 outputs (class prediction)
                    'out': tf.get_variable(name='out', shape=[512, n_classes], initializer=initial)
                }

                self.biases = {
                    'b_conv1': tf.get_variable(name='b_conv1', shape=[16], initializer=initial),
                    'b_conv2': tf.get_variable(name='b_conv2', shape=[32], initializer=initial),
                    'b_conv3': tf.get_variable(name='b_conv3', shape=[64], initializer=initial),
                    'b_fc': tf.get_variable(name='b_fc', shape=[512], initializer=initial),
                    'out': tf.get_variable(name='out_b', shape=[n_classes], initializer=initial)
                }

                self.secondweights = {
                    'W_conv1': tf.get_variable(name='W_conv1s', shape=[5, 5, 1, 32], initializer=initial),
                    # 5x5 conv, 32 inputs, 64 outputs
                    'W_conv2': tf.get_variable(name='W_conv2s', shape=[5, 5, 32, 64], initializer=initial),

                    'W_conv3': tf.get_variable(name='W_conv3s', shape=[5, 5, 64, 64], initializer=initial),

                    'W_conv4': tf.get_variable(name='W_conv4s', shape=[5, 5, 64, 32], initializer=initial),

                    'W_fc': tf.get_variable(name='W_fcs', shape=[63 * 4 * 32, 1024], initializer=initial),

                    'W_fc2': tf.get_variable(name='W_fc2s', shape=[1024, 256], initializer=initial),

                    # 1024 inputs, 10 outputs (class prediction)
                    'out': tf.get_variable(name='out_sw', shape=[256, n_classes], initializer=initial)
                }

                self.secondbiases = {
                    'b_conv1': tf.get_variable(name='b_conv1s', shape=[32], initializer=initial),
                    'b_conv2': tf.get_variable(name='b_conv2s', shape=[64], initializer=initial),
                    'b_conv3': tf.get_variable(name='b_conv3s', shape=[64], initializer=initial),
                    'b_conv4': tf.get_variable(name='b_conv4s', shape=[32], initializer=initial),

                    'b_fc': tf.get_variable(name='b_fcs', shape=[1024], initializer=initial),

                    'b_fc2': tf.get_variable(name='b_fc2', shape=[256], initializer=initial),

                    'out': tf.get_variable(name='out_sb', shape=[n_classes], initializer=initial)
                }
                self.deconv_weigths = {
                    'W_conv1': tf.get_variable(name='deconv_W_conv1', shape=[5, 5, 1, 16], initializer=initial),
                    # 5x5 conv, 32 inputs, 64 outputs
                    'W_conv2': tf.get_variable(name='deconv_W_conv2', shape=[5, 5, 16, 32], initializer=initial),

                    'W_conv3': tf.get_variable(name='deconv_W_conv3', shape=[5, 5, 32, 2], initializer=initial),
                }
                self.auto_weights = {
                    # 5 x 5 convolution, 1 input image, 32 outputs
                    'W_conv1': tf.get_variable(name='auto_W_conv1', shape=[5, 5, 1, 16], initializer=initial),
                    # 5x5 conv, 32 inputs, 64 outputs
                    'W_conv2': tf.get_variable(name='auto_W_conv2', shape=[5, 5, 16, 32], initializer=initial),

                    'W_conv3': tf.get_variable(name='auto_W_conv3', shape=[5, 5, 32, 2], initializer=initial)

                }
                self.auto_bias = {
                    'b_conv1': tf.get_variable(name='auto_b_conv1', shape=[16], initializer=initial),
                    'b_conv2': tf.get_variable(name='auto_b_conv2', shape=[32], initializer=initial),
                    'b_conv3': tf.get_variable(name='auto_b_conv3', shape=[2], initializer=initial)
                }

    # def cnn_piece_for_lstm_network(self, x):
    #
    #     biases = self.biases

    def cnn_model_simple(self, x, activation='relu'):
        # x = tf.reshape(x, shape=[-1, 250, 250, 3])
        # Convolutional Layer #1,

        weigths = self.weights
        biases = self.biases
        keep_prob_conv = self.keep_prob_conv

        conv1 = tf.nn.conv2d(x, weigths['W_conv1'], strides=[1, 1, 1, 1], padding='SAME')

        if activation == 'relu':
            conv1 = tf.nn.relu(conv1 + biases['b_conv1'])
        elif activation == 'tanh':
            conv1 = tf.nn.tanh(conv1 + biases['b_conv1'])
        elif activation == 'elu':
            conv1 = tf.nn.elu(conv1 + biases['b_conv1'])
        elif activation == 'leaky_relu':
            conv1 = tf.nn.leaky_relu(conv1 + biases['b_conv1'])

        conv1_drop = tf.nn.dropout(conv1, keep_prob_conv)
        pool1 = tf.nn.max_pool(conv1_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        # Convolutional Layer #2 and Pooling Layer #2
        conv2 = tf.nn.conv2d(pool1, weigths['W_conv2'], strides=[1, 1, 1, 1], padding='SAME')
        conv2 = tf.nn.relu(conv2 + biases['b_conv2'])
        print(conv2.shape)
        if activation == 'relu':
            conv2 = tf.nn.relu(conv2 + biases['b_conv2'])
        elif activation == 'tanh':
            conv2 = tf.nn.tanh(conv1 + biases['b_conv2'])
        elif activation == 'elu':
            conv2 = tf.nn.elu(conv2 + biases['b_conv2'])
        elif activation == 'leaky_relu':
            conv2 = tf.nn.leaky_relu(conv2 + biases['b_conv2'])

        conv2_drop = tf.nn.dropout(conv2, keep_prob_conv)
        pool2 = tf.nn.max_pool(conv2_drop, ksize=[1, 2, 2, 1], strides=[1, 4, 4, 1], padding='SAME')

        fc = tf.reshape(pool2, [-1, pool2.shape[1] * pool2.shape[2] * pool2.shape[3]])

        print(fc)

        if activation == 'relu':
            fc = tf.nn.relu(tf.matmul(fc, weigths['W_fc']) + biases['b_fc'])
        elif activation == 'tanh':
            fc = tf.nn.tanh(tf.matmul(fc, weigths['W_fc']) + biases['b_fc'])
        elif activation == 'elu':
            fc = tf.nn.elu(tf.matmul(fc, weigths['W_fc']) + biases['b_fc'])
        elif activation == 'leaky_relu':
            fc = tf.nn.leaky_relu(tf.matmul(fc, weigths['W_fc']) + biases['b_fc'])

        fc_drop = tf.nn.dropout(fc, self.keep_prob_fc)

        fc_out = tf.matmul(fc_drop, weigths['out']) + biases['out']

        return fc_out, fc

    def cnn_graph_batchnorm(self, x):
        # x = tf.reshape(x, shape=[-1, 250, 250, 3])
        # Convolutional Layer #1
        weigths = self.weights
        biases = self.biases
        mode = self.mode

        conv1 = tf.nn.conv2d(x, weigths['W_conv1'], strides=[1, 1, 1, 1], padding='SAME')
        BN1 = tf.layers.batch_normalization(conv1, training=mode)
        conv1 = self.gelu(BN1 + biases['b_conv1'])
        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        # Convolutional Layer #2 and Pooling Layer #2
        conv2 = tf.nn.conv2d(pool1, weigths['W_conv2'], strides=[1, 1, 1, 1], padding='SAME')
        bn2 = tf.layers.batch_normalization(conv2, training=mode)
        conv2 = self.gelu(bn2 + biases['b_conv2'])
        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        conv3 = tf.nn.conv2d(pool2, weigths['W_conv3'], strides=[1, 1, 1, 1], padding='SAME')
        bn3 = tf.layers.batch_normalization(conv3, training=mode)
        conv3 = self.gelu(bn3 + biases['b_conv3'])
        pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        fc = tf.contrib.layers.flatten(pool3)

        fc = self.gelu(tf.matmul(fc, weigths['W_fc']) + biases['b_fc'])
        fc_drop = tf.nn.dropout(fc, self.keep_prob_fc)

        fc_out = tf.matmul(fc_drop, weigths['out']) + biases['out']
        name = 'cnn_graph_batchnorm'

        return fc_out, fc, name

    def cnn_graph_batchnorm_v2(self, x):

        # x = tf.reshape(x, shape=[-1, 250, 250, 3])
        weigths = self.weights
        biases = self.biases
        mode = self.mode
        keep_prob_conv = self.keep_prob_conv

        # alpha parameter = 0.2 on leaky relu

        conv1 = tf.nn.conv2d(x, weigths['W_conv1'], strides=[1, 1, 1, 1], padding='SAME')
        conv1 = tf.nn.leaky_relu(conv1 + biases['b_conv1'])
        bn1 = tf.layers.batch_normalization(conv1, training=mode)
        bn1 = tf.nn.dropout(bn1, keep_prob_conv)
        pool1 = tf.nn.max_pool(bn1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        conv2 = tf.nn.conv2d(pool1, weigths['W_conv2'], strides=[1, 1, 1, 1], padding='SAME')
        conv2 = tf.nn.leaky_relu(conv2 + biases['b_conv2'])
        bn2 = tf.layers.batch_normalization(conv2, training=mode)
        bn2 = tf.nn.dropout(bn2, keep_prob_conv)
        pool2 = tf.nn.max_pool(bn2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        conv3 = tf.nn.conv2d(pool2, weigths['W_conv3'], strides=[1, 1, 1, 1], padding='SAME')
        conv3 = tf.nn.leaky_relu(conv3 + biases['b_conv3'])
        bn3 = tf.layers.batch_normalization(conv3, training=mode)
        bn3 = tf.nn.dropout(bn3, keep_prob_conv)
        pool3 = tf.nn.max_pool(bn3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        fc = tf.contrib.layers.flatten(pool3)
        name = 'cnn_graph_batchnorm_v2'
        fc = tf.nn.leaky_relu(tf.matmul(fc, weigths['W_fc']) + biases['b_fc'])
        fc_drop = tf.nn.dropout(fc, self.keep_prob_fc)
        fc_out = tf.matmul(fc_drop, weigths['out']) + biases['out']

        return fc_out, fc, name

    def cnn_graph_batchnorm_v3(self, x):
        # x = tf.reshape(x, shape=[-1, 250, 250, 3])
        # Convolutional Layer #1
        weigths = self.weights
        biases = self.biases
        mode = self.mode
        print(x.shape)
        conv1 = tf.nn.conv2d(x, weigths['W_conv1'], strides=[1, 1, 1, 1], padding='SAME')
        bn1 = tf.layers.batch_normalization(conv1, training=mode)
        conv1 = tf.nn.leaky_relu(bn1 + biases['b_conv1'])
        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print(pool1.shape)
        # Convolutional Layer #2 and Pooling Layer #2
        conv2 = tf.nn.conv2d(pool1, weigths['W_conv2'], strides=[1, 1, 1, 1], padding='SAME')
        bn2 = tf.layers.batch_normalization(conv2, training=mode)
        conv2 = tf.nn.leaky_relu(bn2 + biases['b_conv2'])
        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print(pool2.shape)

        conv3 = tf.nn.conv2d(pool2, weigths['W_conv3'], strides=[1, 1, 1, 1], padding='SAME')
        bn3 = tf.layers.batch_normalization(conv3, training=mode)
        conv3 = tf.nn.leaky_relu(bn3 + biases['b_conv3'])
        pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print(pool3.shape)

        fc = tf.contrib.layers.flatten(pool3)

        fc = tf.nn.leaky_relu(tf.matmul(fc, weigths['W_fc']) + biases['b_fc'])
        fc_drop = tf.nn.dropout(fc, self.keep_prob_fc)
        Unnormalized_scores = tf.matmul(fc_drop, weigths['out']) + biases['out']

        name = 'cnn_graph_batchnorm_v3'
        return Unnormalized_scores, fc, name

    def advanced_cnn_graph_with_batchnorm(self, x):

        secondweights = self.secondweights
        secondbiases = self.secondbiases
        keep_prob_fc = self.keep_prob_fc
        mode = self.mode

        conv1 = tf.nn.conv2d(x, secondweights['W_conv1'], strides=[1, 1, 1, 1], padding='SAME')
        bn1 = tf.layers.batch_normalization(conv1, training=mode)
        conv1 = tf.nn.elu(bn1 + secondbiases['b_conv1'])
        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        conv2 = tf.nn.conv2d(pool1, secondweights['W_conv2'], strides=[1, 1, 1, 1], padding='SAME')
        bn2 = tf.layers.batch_normalization(conv2, training=mode)
        conv2 = tf.nn.elu(bn2 + secondbiases['b_conv2'])
        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        conv3 = tf.nn.conv2d(pool2, secondweights['W_conv3'], strides=[1, 1, 1, 1], padding='SAME')
        BN3 = tf.layers.batch_normalization(conv3, training=mode)
        conv3 = tf.nn.elu(BN3 + secondbiases['b_conv3'])
        pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        conv4 = tf.nn.conv2d(pool3, secondweights['W_conv4'], strides=[1, 1, 1, 1], padding='SAME')
        bn4 = tf.layers.batch_normalization(conv4, training=mode)
        conv4 = tf.nn.elu(bn4 + secondbiases['b_conv3'])
        pool4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        fc = tf.contrib.layers.flatten(pool4)

        fc = tf.nn.elu(tf.matmul(fc, secondweights['W_fc']) + secondbiases['b_fc'])
        fc_drop = tf.nn.dropout(fc, keep_prob_fc)

        fc2 = tf.nn.elu(tf.matmul(fc_drop, secondweights['W_fc2']) + secondbiases['b_fc2'])
        fc2_drop = tf.nn.dropout(fc2, keep_prob_fc)

        output = tf.matmul(fc2_drop, secondweights['out']) + secondbiases['out']

        name = 'advanced_cnn_graph_with_batchnorm'
        return output, fc2, name

    def advanced_cnn_graph_with_batchnorm_gelu(self, x):

        secondweights = self.secondweights
        secondbiases = self.secondbiases
        keep_prob_fc = self.keep_prob_fc
        mode = self.mode

        conv1 = tf.nn.conv2d(x, secondweights['W_conv1'], strides=[1, 1, 1, 1], padding='SAME')
        bn1 = tf.layers.batch_normalization(conv1, training=mode)
        conv1 = self.gelu(bn1 + secondbiases['b_conv1'])
        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        conv2 = tf.nn.conv2d(pool1, secondweights['W_conv2'], strides=[1, 1, 1, 1], padding='SAME')
        bn2 = tf.layers.batch_normalization(conv2, training=mode)
        conv2 = self.gelu(bn2 + secondbiases['b_conv2'])
        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        conv3 = tf.nn.conv2d(pool2, secondweights['W_conv3'], strides=[1, 1, 1, 1], padding='SAME')
        bn3 = tf.layers.batch_normalization(conv3, training=mode)
        conv3 = self.gelu(bn3 + secondbiases['b_conv3'])
        pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print(pool3.shape)

        conv4 = tf.nn.conv2d(pool3, secondweights['W_conv4'], strides=[1, 1, 1, 1], padding='SAME')
        bn4 = tf.layers.batch_normalization(conv4, training=mode)
        conv4 = self.gelu(bn4 + secondbiases['b_conv4'])
        pool4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print(pool4.shape)

        fc = tf.contrib.layers.flatten(pool4)

        fc = self.gelu(tf.matmul(fc, secondweights['W_fc']) + secondbiases['b_fc'])
        fc_drop = tf.nn.dropout(fc, keep_prob_fc)

        fc2 = self.gelu(tf.matmul(fc_drop, secondweights['W_fc2']) + secondbiases['b_fc2'])
        fc2_drop = tf.nn.dropout(fc2, keep_prob_fc)

        output = tf.matmul(fc2_drop, secondweights['out']) + secondbiases['out']

        name = 'advanced_cnn_graph_with_batchnorm_gelu'
        return output, fc, name

    def advanced_cnn_graph(self, x):

        secondweights = self.secondweights
        secondbiases = self.secondbiases
        keep_prob_conv = self.keep_prob_conv
        keep_prob_fc = self.keep_prob_fc

        conv1 = tf.nn.conv2d(x, secondweights['W_conv1'], strides=[1, 1, 1, 1], padding='SAME')
        conv1 = tf.nn.relu(conv1 + secondbiases['b_conv1'])
        conv1_drop = tf.nn.dropout(conv1, keep_prob_conv)

        pool1 = tf.nn.max_pool(conv1_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print(pool1.get_shape().as_list())
        conv2 = tf.nn.conv2d(pool1, secondweights['W_conv2'], strides=[1, 1, 1, 1], padding='SAME')
        conv2 = tf.nn.relu(conv2 + secondbiases['b_conv2'])
        conv2_drop = tf.nn.dropout(conv2, keep_prob_conv)

        pool2 = tf.nn.max_pool(conv2_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print(pool2.get_shape().as_list())

        conv3 = tf.nn.conv2d(pool2, secondweights['W_conv3'], strides=[1, 1, 1, 1], padding='SAME')
        conv3 = tf.nn.relu(conv3 + secondbiases['b_conv3'])
        conv3_drop = tf.nn.dropout(conv3, keep_prob_conv)

        pool3 = tf.nn.max_pool(conv3_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print(pool3.get_shape().as_list())

        conv4 = tf.nn.conv2d(pool3, secondweights['W_conv4'], strides=[1, 1, 1, 1], padding='SAME')
        conv4 = tf.nn.relu(conv4 + secondbiases['b_conv4'])
        conv4_drop = tf.nn.dropout(conv4, keep_prob_conv)

        pool4 = tf.nn.max_pool(conv4_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

        print(pool4.get_shape().as_list())

        fc = tf.contrib.layers.flatten(pool4)

        fc = tf.nn.relu(tf.matmul(fc, secondweights['W_fc']) + secondbiases['b_fc'])
        print(fc.get_shape().as_list())

        fc_drop = tf.nn.dropout(fc, keep_prob_fc)

        output = tf.matmul(fc_drop, secondweights['out']) + secondbiases['out']
        return output, fc

    def autoencoder(self, x):

        weigths = self.auto_weights
        biases = self.auto_bias
        mode = self.mode
        deconv_weigths = self.deconv_weigths

        print(x.shape)
        conv1 = tf.nn.conv2d(x, weigths['W_conv1'], strides=[1, 1, 1, 1], padding='SAME')
        print('conv1')
        print(conv1.shape)
        BN1 = tf.layers.batch_normalization(conv1, training=mode)
        conv1 = tf.nn.leaky_relu(BN1 + biases['b_conv1'])
        pool1 = tf.nn.max_pool(conv1, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')
        print('pool1')
        print(pool1.shape)
        # Convolutional Layer #2 and Pooling Layer #2
        conv2 = tf.nn.conv2d(pool1, weigths['W_conv2'], strides=[1, 1, 1, 1], padding='SAME')
        print('conv2')
        print(conv2.shape)
        bn2 = tf.layers.batch_normalization(conv2, training=mode)
        conv2 = tf.nn.leaky_relu(bn2 + biases['b_conv2'])
        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print('pool2')
        print(pool2.shape)

        conv3 = tf.nn.conv2d(pool2, weigths['W_conv3'], strides=[1, 1, 1, 1], padding='SAME')
        print('conv3')
        print(conv3.shape)
        bn3 = tf.layers.batch_normalization(conv3, training=mode)
        conv3 = tf.nn.leaky_relu(bn3 + biases['b_conv3'])
        pool3 = tf.nn.max_pool(conv3, ksize=[1, 5, 1, 1], strides=[1, 5, 1, 1], padding='SAME')
        print('pool3 bottleneck')
        print(pool3.shape)

        batch_size = tf.shape(pool3)[0]

        # [-1, 125, 8, 32]
        # [-1, 250, 16, 16]
        # [-1, 500, 32, 1]
        deconv1 = tf.nn.conv2d_transpose(pool3, deconv_weigths['W_conv3'],
                                         output_shape=tf.stack([batch_size, 125, 8, 32]), strides=[1, 5, 1, 1],
                                         padding='SAME')
        print('deconv1')
        print(deconv1.shape)
        bn_deconv = tf.layers.batch_normalization(deconv1, training=mode)
        bn_deconv = tf.nn.leaky_relu(bn_deconv)
        bn_deconv_out = self.avg_unpool2d(bn_deconv, factor=2)
        print(bn_deconv_out.shape)
        deconv1 = tf.nn.conv2d_transpose(bn_deconv_out, deconv_weigths['W_conv2'],
                                         output_shape=tf.stack([batch_size, 250, 16, 16]), strides=[1, 1, 1, 1],
                                         padding='SAME')
        print(deconv1.shape)
        bn_deconv = tf.layers.batch_normalization(deconv1, training=mode)
        bn_deconv = tf.nn.leaky_relu(bn_deconv)
        bn_deconv_out = self.avg_unpool2d(bn_deconv, factor=2)
        print(bn_deconv_out.shape)

        deconv1 = tf.nn.conv2d_transpose(bn_deconv_out, deconv_weigths['W_conv1'],
                                         output_shape=tf.stack([batch_size, 500, 32, 1]), strides=[1, 1, 1, 1],
                                         padding='SAME')
        print(deconv1.shape)
        bn_deconv = tf.layers.batch_normalization(deconv1, training=mode)
        bn_deconv = tf.nn.leaky_relu(bn_deconv)
        bn_deconv_out = self.avg_unpool2d(bn_deconv, factor=2)
        print(bn_deconv_out.shape)
        name = 'autoencoder'
        bn_deconv_out = tf.squeeze(bn_deconv_out)
        print(bn_deconv_out.shape)
        # conv3 = tf.nn.leaky_relu(bn3 + biases['b_conv3'])
        return bn_deconv_out, name

    def autoencoder_v2(self, x):

        weigths = self.auto_weights
        biases = self.auto_bias
        mode = self.mode
        deconv_weigths = self.deconv_weigths

        print(x.shape)
        conv1 = tf.nn.conv2d(x, weigths['W_conv1'], strides=[1, 1, 1, 1], padding='SAME')
        print('conv1')
        print(conv1.shape)
        BN1 = tf.layers.batch_normalization(conv1, training=mode)
        conv1 = tf.nn.elu(BN1 + biases['b_conv1'])
        pool1, argmax1 = tf.nn.max_pool_with_argmax(conv1, ksize=[1, 5, 5, 1], strides=[1, 5, 5, 1], padding='SAME')

        print('pool1')
        print(pool1.shape)
        # Convolutional Layer #2 and Pooling Layer #2
        conv2 = tf.nn.conv2d(pool1, weigths['W_conv2'], strides=[1, 1, 1, 1], padding='SAME')
        print('conv2')
        print(conv2.shape)
        bn2 = tf.layers.batch_normalization(conv2, training=mode)
        conv2 = tf.nn.elu(bn2 + biases['b_conv2'])
        pool2, argmax2 = tf.nn.max_pool_with_argmax(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print('pool2')
        print(pool2.shape)

        conv3 = tf.nn.conv2d(pool2, weigths['W_conv3'], strides=[1, 1, 1, 1], padding='SAME')
        print('conv3')
        print(conv3.shape)
        bn3 = tf.layers.batch_normalization(conv3, training=mode)
        conv3 = tf.nn.elu(bn3 + biases['b_conv3'])
        pool3, argmax3 = tf.nn.max_pool_with_argmax(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
        print('pool3 bottleneck')
        print(pool3.shape)
        print(argmax3.shape)

        batch_size = tf.shape(pool3)[0]

        # [-1, 125, 8, 32]
        # [-1, 250, 16, 16]
        # [-1, 500, 32, 1]
        unpool_out = self.unpool2d(pool3, stride=2, mask=argmax3)
        print(unpool_out.shape)
        deconv1 = tf.nn.conv2d_transpose(unpool_out, deconv_weigths['W_conv3'],
                                         output_shape=tf.stack([batch_size, 100, 10, 32]), strides=[1, 1, 1, 1],
                                         padding='SAME')
        print('deconv1')
        print(deconv1.shape)
        bn_deconv = tf.layers.batch_normalization(deconv1, training=mode)
        bn_deconv_out = tf.nn.elu(bn_deconv)
        print(bn_deconv_out.shape)

        unpool_out2 = self.unpool2d(bn_deconv_out, stride=2, mask=argmax2)
        deconv1 = tf.nn.conv2d_transpose(unpool_out2, deconv_weigths['W_conv2'],
                                         output_shape=tf.stack([batch_size, 200, 20, 16]), strides=[1, 1, 1, 1],
                                         padding='SAME')
        print(deconv1.shape)
        bn_deconv = tf.layers.batch_normalization(deconv1, training=mode)
        bn_deconv = tf.nn.elu(bn_deconv)
        bn_deconv_out = self.unpool2d(bn_deconv, stride=5, mask=argmax1)
        print(bn_deconv_out.shape)

        deconv1 = tf.nn.conv2d_transpose(bn_deconv_out, deconv_weigths['W_conv1'],
                                         output_shape=tf.stack([batch_size, 1000, 100, 1]), strides=[1, 1, 1, 1],
                                         padding='SAME')
        print(deconv1.shape)
        bn_deconv = tf.layers.batch_normalization(deconv1, training=mode)
        bn_deconv = tf.nn.elu(bn_deconv)

        print(bn_deconv.shape)
        name = 'autoencoder_v2'
        bn_deconv_out = tf.squeeze(bn_deconv)
        print(bn_deconv.shape)
        # conv3 = tf.nn.leaky_relu(bn3 + biases['b_conv3'])
        return bn_deconv_out, name

    def gelu(self, input_tensor):
        cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
        return input_tensor * cdf

    def avg_unpool2d(self, x, factor):
        """
        Performs "average un-pooling", i.e. nearest neighbor upsampling,
        without the faulty `tf.image.resize_nearest_neighbor` op.
        """
        x = tf.transpose(x, [1, 2, 3, 0])
        x = tf.expand_dims(x, 0)
        x = tf.tile(x, [factor ** 2, 1, 1, 1, 1])
        x = tf.batch_to_space_nd(x, [factor, factor], [[0, 0], [0, 0]])
        x = tf.transpose(x[0], [3, 0, 1, 2])
        return x

    def unpool2d(self, net, mask, stride):
        assert mask is not None
        with tf.name_scope('UnPool2D'):
            ksize = [1, stride, stride, 1]
            input_shape = net.get_shape().as_list()
            batch_shape = tf.shape(net)[0]
            print(batch_shape)
            #  calculation new shape
            output_shape = (batch_shape, input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])
            output_shape = tf.to_int32(output_shape)
            mask2 = tf.to_int32(mask)
            # calculation indices for batch, height, width and feature maps
            one_like_mask = tf.ones_like(mask, dtype=tf.int32)
            batch_range = tf.reshape(tf.range(batch_shape, dtype=tf.int32), shape=[batch_shape, 1, 1, 1])
            b = one_like_mask * batch_range
            y = mask2 // (output_shape[2] * output_shape[3])
            x = mask2 % (output_shape[2] * output_shape[3]) // output_shape[3]
            feature_range = tf.range(output_shape[3], dtype=tf.int32)
            print(feature_range)
            f = one_like_mask * feature_range
            # transpose indices & reshape update values to one dimension
            updates_size = tf.size(net)
            indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))
            values = tf.reshape(net, [updates_size])
            ret = tf.scatter_nd(indices, values, output_shape)
        return ret
