import logging
import tensorflow as tf
import numpy as np
from tqdm import tqdm
import helper_functions
import data_load_functions
from tensorflow.python.ops import control_flow_ops
from sklearn.metrics import confusion_matrix
from graph_training_functions import GraphHelperFunctions
import ml_algorithms


class Transformer(GraphHelperFunctions):
    logging.basicConfig(filename='transformer.log', format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S',
                        level=logging.DEBUG)

    def __init__(self, dropout_rate):
        super(GraphHelperFunctions, self).__init__()
        self.batch_size = 8
        self.test_batch_size = 8

        self.learning_rate = 0.002  #
        self.model_name = 'transformer_model_v16_predict_new_malware'
        self.save_name = 'transformer_model_v16_predict_new_malware'
        self.root_dir = 'D:\\thesis_code_base\\saveddata\\Networks\\transformer\\' + self.model_name
        self.num_epochs = 10

        # words whose occurred less than min_cnt are encoded as <UNK>.
        # self.hidden_units and  self.embedding_hidden_unit must be equal
        # making these four parameter larger could give better performance
        self.hidden_units = 8
        self.embedding_hidden_units = 8
        self.num_blocks = 2  # number of encoder/decoder blocks
        self.num_heads = 4

        self.dropout_rate = dropout_rate

        # If True, use sinusoid. If false, positional embedding.
        self.sinusoid = False
        self.graph = None
        self.maxlen = 1000
        self.converted_train_data = None
        self.train_label = None
        self.converted_test_data = None
        self.test_label = None
        self.new_malware_test_ratio = 0.999
        self.change_graph = True
        self.save_results = True
        self.new_model = False
        self.config = None
        self.model_type_name = "Transformer"

    def normalize(self, inputs,
                  epsilon=1e-8,
                  scope="ln",
                  reuse=None):
        with tf.variable_scope(scope, reuse=reuse):
            inputs_shape = inputs.get_shape()
            params_shape = inputs_shape[-1:]

            mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)
            beta = tf.Variable(tf.zeros(params_shape))
            gamma = tf.Variable(tf.ones(params_shape))
            normalized = (inputs - mean) / ((variance + epsilon) ** .5)
            outputs = tf.add(tf.multiply(gamma, normalized), beta)
            return outputs

    def layer_norm(input_tensor, name=None):
        """Run layer normalization on the last dimension of the tensor."""
        return tf.contrib.layers.layer_norm(
            inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)

    def multihead_attention(self, queries,
                            keys,
                            num_units=None,
                            num_heads=8,
                            dropout_rate=0.0,
                            is_training=True,
                            causality=False,
                            scope="multihead_attention",
                            reuse=None):

        with tf.variable_scope(scope, reuse=reuse):
            # Set the fall back option for num_units
            if num_units is None:
                num_units = queries.get_shape().as_list[-1]

            # Linear projections
            Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu)  # (N, T_q, C)
            K = tf.layers.dense(keys, num_units, activation=tf.nn.relu)  # (N, T_k, C)
            V = tf.layers.dense(keys, num_units, activation=tf.nn.relu)  # (N, T_k, C)

            # Split and concat
            Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)  # (h*N, T_q, C/h)
            K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)
            V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)

            # Multiplication
            outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (h*N, T_q, T_k)

            # Scale
            outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)

            # Key Masking
            # if False:
            #     key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)
            #     key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)
            #     key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)
            #
            #     paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)
            #     outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)  # (h*N, T_q, T_k)

            # Causality = Future blinding
            if causality:
                diag_vals = tf.ones_like(outputs[0, :, :])  # (T_q, T_k)
                tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense()  # (T_q, T_k)
                masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])  # (h*N, T_q, T_k)

                paddings = tf.ones_like(masks) * (-2 ** 32 + 1)
                outputs = tf.where(tf.equal(masks, 0), paddings, outputs)  # (h*N, T_q, T_k)

            # Activation
            outputs = tf.nn.softmax(outputs)  # (h*N, T_q, T_k)

            # Query Masking
            # query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)
            # query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)
            # query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)
            # outputs *= query_masks  # broadcasting. (N, T_q, C)

            # Dropouts
            outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))

            # Weighted sum
            outputs = tf.matmul(outputs, V_)  # ( h*N, T_q, C/h)

            # Restore shape
            outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # (N, T_q, C)

            # Residual connection
            outputs += queries

            # Normalize
            outputs = self.normalize(outputs)  # (N, T_q, C)

        return outputs

    def feedforward(self, inputs,
                    num_units=(2048, 256),
                    scope="multihead_attention",
                    reuse=None):

        with tf.variable_scope(scope, reuse=reuse):
            # Inner layer
            params = {"inputs": inputs, "filters": num_units[0], "kernel_size": 1,
                      "activation": tf.nn.relu, "use_bias": True}
            outputs = tf.layers.conv1d(**params)

            # Readout layer
            params = {"inputs": outputs, "filters": num_units[1], "kernel_size": 1,
                      "activation": None, "use_bias": True}
            outputs = tf.layers.conv1d(**params)

            # Residual connection
            outputs += inputs

            # Normalize
            outputs = self.normalize(outputs)

        return outputs

    def label_smoothing(self, inputs, epsilon=0.1):
        K = inputs.get_shape().as_list()[-1]  # number of channels
        return ((1 - epsilon) * inputs) + (epsilon / K)

    def positional_encoding(self, inputs,
                            num_units,
                            zero_pad=True,
                            scale=True,
                            scope="positional_encoding",
                            reuse=None):

        N, T = inputs.get_shape().as_list()
        with tf.variable_scope(scope, reuse=reuse):
            position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])

            # First part of the PE function: sin and cos argument
            position_enc = np.array([
                [pos / np.power(10000, 2. * i / num_units) for i in range(num_units)]
                for pos in range(T)])

            # Second part, apply the cosine to even columns and sin to odds.
            position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i
            position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1

            # Convert to a tensor
            lookup_table = tf.convert_to_tensor(position_enc)

            if zero_pad:
                lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),
                                          lookup_table[1:, :]), 0)
            outputs = tf.nn.embedding_lookup(lookup_table, position_ind)

            if scale:
                outputs = outputs * num_units ** 0.5

            return outputs

    def embedding(self, inputs,
                  vocab_size,
                  num_units,
                  zero_pad=True,
                  scale=True,
                  scope="embedding",
                  reuse=None):

        with tf.variable_scope(scope, reuse=reuse):
            lookup_table = tf.get_variable('lookup_table',
                                           dtype=tf.float32,
                                           shape=[vocab_size, num_units],
                                           initializer=tf.contrib.layers.xavier_initializer())
            if zero_pad:
                lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),
                                          lookup_table[1:, :]), 0)
            outputs = tf.nn.embedding_lookup(lookup_table, inputs)

            if scale:
                outputs = outputs * (num_units ** 0.5)

        return outputs

    def main_graph(self, is_training=True, use_train_data=True):

        n_classes = 2
        tf.reset_default_graph()
        self.graph = tf.Graph()
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.config = config
        with self.graph.as_default():

            x_raw, y_raw, diction, class_weight = self.load_data(is_train=use_train_data)
            print(x_raw.shape)
            print(is_training)
            print(y_raw.shape)

            x, y, init_train = self.get_input_data_pipeline_transformer(x_raw, y_raw, is_train=use_train_data)

            # Encoder
            with tf.variable_scope("encoder"):
                ## Embedding
                enc = self.embedding(x,
                                     vocab_size=len(diction),
                                     num_units=self.embedding_hidden_units,
                                     scale=True,
                                     scope="enc_embed")

                ## Positional Encoding
                if self.sinusoid:
                    enc += self.positional_encoding(x,
                                                    num_units=self.hidden_units,
                                                    zero_pad=False,
                                                    scale=False,
                                                    scope="enc_pe")
                else:
                    enc += self.embedding(
                        tf.tile(tf.expand_dims(tf.range(tf.shape(x)[1]), 0), [tf.shape(x)[0], 1]),
                        vocab_size=self.maxlen,
                        num_units=self.hidden_units,
                        zero_pad=False,
                        scale=False,
                        scope="enc_pe")

                ## Dropout
                enc = tf.layers.dropout(enc,
                                        rate=self.dropout_rate,
                                        training=tf.convert_to_tensor(is_training))

                ## Blocks
                for i in range(self.num_blocks):
                    with tf.variable_scope("num_blocks_{}".format(i)):
                        ### Multihead Attention
                        enc = self.multihead_attention(queries=enc,
                                                       keys=enc,
                                                       num_units=self.hidden_units,
                                                       num_heads=self.num_heads,
                                                       dropout_rate=self.dropout_rate,
                                                       is_training=is_training,
                                                       causality=False)

                        ### Feed Forward
                        enc = self.feedforward(enc, num_units=[4 * self.hidden_units, self.hidden_units])

            # Final linear projection
            with tf.variable_scope("dense_layer"):
                enc = tf.reshape(enc, shape=(-1, int(enc.get_shape()[1]) * int(enc.get_shape()[2])))
                logits = tf.layers.dense(enc, 2)

            with tf.variable_scope("results"):
                logits_normalized = tf.nn.softmax(logits)
                binary_predictions = tf.argmax(logits, 1)
                correct = tf.equal(binary_predictions, tf.argmax(y, 1))
                accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
            self.graph.add_to_collection('acc', accuracy)
            self.graph.add_to_collection('preds', binary_predictions)
            self.graph.add_to_collection('label', tf.argmax(y, 1))
            self.graph.add_to_collection('scores', logits_normalized)
            self.graph.add_to_collection('encoded', enc)

            istarget = tf.to_float(tf.not_equal(tf.argmax(y, 1), 0))

            tf.summary.scalar('acc', accuracy)

            global_step = tf.Variable(0, name='global_step', trainable=False)
            self.graph.add_to_collection('global_step', global_step)

            if is_training:
                # Loss
                # y_smoothed = self.label_smoothing(tf.one_hot(y, depth=n_classes))
                # weight = tf.constant([0.3])
                # # # weight = 1
                # # loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=logits_normalized, targets=y,
                #                                                                pos_weight=weight))

                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))
                # mean_loss = tf.reduce_sum(loss * istarget) / (tf.reduce_sum(istarget))

                # Training Scheme
                with tf.variable_scope("optimizer"):
                    optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.98,
                                                       epsilon=1e-8, name="adam_optimizer")
                # optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate)
                train_op = optimizer.minimize(loss, global_step=global_step)
                self.graph.add_to_collection('optim_op', train_op)
                self.graph.add_to_collection('loss', loss)
                self.graph.add_to_collection('optimizer', optimizer)

                # Summary
                tf.summary.scalar('mean_loss', loss)
                # scaffold = tf.train.Scaffold(
                #     local_init_op=control_flow_ops.group(tf.local_variables_initializer(),
                #                                           init_train))
                # init_op = tf.variables_initializer(self.graph.get_collection_ref('optimizer')[0].variables()),
            else:
                scaffold = tf.train.Scaffold(
                    local_init_op=control_flow_ops.group(tf.local_variables_initializer(),
                                                         init_train))

            tf.summary.merge_all()
            # summary_hook = tf.train.SummarySaverHook(save_steps=1,
            #                                          output_dir="summaries", summary_op=tf.summary.merge_all())
            # vl = [v for v in tf.global_variables() if "Adam" not in v.name]
            saver = tf.train.Saver()
            self.graph.add_to_collection('saver', saver)
            self.graph.add_to_collection('config', config)
            # self.saver_hook = tf.train.CheckpointSaverHook(
            #     saver=saver,
            #     checkpoint_dir=self.logdir,
            #     save_steps=5,
            # )
            #
            # self.graph.add_to_collection('scaffold', scaffold)
            # sess.run(tf.variables_initializer(self.graph.get_collection_ref('optimizer')[0].variables()))

            #
            # self.graph.add_to_collection('summary_hook', summary_hook)

    def load_data(self, is_train=True):
        self.converted_train_data, self.train_label, self.converted_test_data, self.test_label, \
        embedding_mat, dictionary, class_weight = \
            data_load_functions.load_data_v3_all_mixed(load_larger_test=False, modify_opcode=True)
        self.data_load_func_name = "load_data_v3_all_mixed(load_larger_test=False, modify_opcode=self.modify_jnz_jne)"

        self.new_malware_predict_dataset()

        if is_train:
            return self.converted_train_data, self.train_label, dictionary, class_weight
        else:
            return self.converted_test_data, self.test_label, dictionary, class_weight

    def get_input_data_pipeline_transformer(self, data, labels, is_train):
        with self.graph.as_default():
            if is_train:
                label_processed = self.label_smoothing(tf.one_hot(labels, 2))
            else:
                label_processed = tf.one_hot(labels, 2)

            dataset = tf.data.Dataset.from_tensor_slices(
                (data, label_processed))
            #
            # if is_train:
            #     dataset = dataset.repeat(self.num_epochs)

            batches = dataset.batch(self.batch_size)
            batches = batches.prefetch(1)
            # batches = batches.apply(tf.data.experimental.prefetch_to_device('/device:GPU:%d' % (0)))
            iterator = batches.make_initializable_iterator()

            init_train = iterator.make_initializer(batches, name='train_init')

            data_piece, label_piece = iterator.get_next()
            return data_piece, label_piece, init_train

    def train_graph(self):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()
        else:
            restore_op = None
            main_graph = self.graph

        if self.change_graph:
            restore_op = self.graph.get_collection_ref('saver')[0]
            main_graph = self.graph

        with main_graph.as_default():
            query_list = [main_graph.get_collection_ref(str)[0] for str in
                          ['optim_op', 'loss', 'acc', 'preds', 'scores', 'label']]
            with tf.Session(config=self.config) as session:

                if not self.new_model:
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                for epoch_step in range(self.num_epochs):

                    final_predictions = []
                    epoch_loss = 0
                    epoch_accuracy = 0
                    step = 0
                    session.run(main_graph.get_operation_by_name('train_init'))

                    while True:
                        step = step + 1
                        try:

                            _, batch_loss, batch_acc, batch_preds, batch_scores, batch_labels = session.run(query_list)

                            epoch_loss = epoch_loss + batch_loss
                            epoch_accuracy = epoch_accuracy + batch_acc
                            final_predictions.append(batch_preds)
                        except tf.errors.OutOfRangeError:
                            print('epoch_train_end')
                            epoch_loss = epoch_loss / (step - 1)
                            epoch_accuracy = epoch_accuracy / (step - 1)
                            break
                    print("Epoch: %.4d cost=  %.9f ", (epoch_step + 1), epoch_loss)
                    logging.info("Epoch: %.4d cost=  %.9f", (epoch_step + 1), epoch_loss)
                    print("Epoch:", '%.4d' % (epoch_step + 1), "acc=", "%9f" % epoch_accuracy)
                    logging.info("Epoch: %.4d acc =  %.9f ", (epoch_step + 1), epoch_accuracy)

                    predict_set = np.concatenate(final_predictions)
                    print(confusion_matrix(self.train_label, predict_set))

                if self.save_results:
                    if not self.new_model:
                        restore_op.save(session, self.root_dir + '\\' + self.model_name)
                    else:
                        main_graph.get_collection_ref('saver')[0].save(session, self.root_dir + '\\' + self.model_name)

    def evaluation(self, labels):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()
        else:
            restore_op = None
            main_graph = None

        if self.change_graph:
            restore_op = self.graph.get_collection_ref('saver')[0]
            main_graph = self.graph

        with main_graph.as_default():

            query_list = [main_graph.get_collection_ref(str)[0] for str in
                          ['acc', 'preds', 'scores', 'encoded']]

            with tf.Session(config=self.config) as session:
                if not self.new_model:
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                session.run(main_graph.get_operation_by_name('train_init'))

                step = 0
                loss = 0
                accuracy = 0

                final_predictions = []
                train_features = []  # type: list
                train_scores = []
                while True:
                    step = step + 1
                    try:
                        batch_accuracy, batch_predictions, batch_scores, batch_features = session.run(
                            query_list)

                        final_predictions.append(batch_predictions)
                        train_features.append(batch_features)
                        train_scores.append(batch_scores)
                        print(batch_scores)

                        # loss = loss + batch_loss
                        accuracy = accuracy + batch_accuracy

                    except tf.errors.OutOfRangeError:
                        print('epoch_test_end')
                        # loss = loss / (step - 1)
                        accuracy = accuracy / (step - 1)
                        break

                # print("Epoch: %.4d cost=  %.9f ", (0 + 1), loss)
                # logging.info("Epoch: %.4d cost=  %.9f", (0 + 1), loss)
                print("Epoch:", '%.4d' % (0 + 1), "acc=", "%9f" % accuracy)
                logging.info("Epoch: %.4d acc =  %.9f ", (0 + 1), accuracy)

                predict_set = np.concatenate(final_predictions)
                # feature_set = np.concatenate(train_features)
                feature_set = None
                score_set = np.concatenate(train_scores)
                print(predict_set.shape)
                print(self.test_label.shape)
                print(confusion_matrix(labels, predict_set))
        opt_thres = helper_functions.choose_optimal_threshold(score_set, labels)
        print('optimal_threshold =' + str(opt_thres))

        return predict_set, feature_set, score_set, opt_thres



    #
def create_graph_for_train():
    network = Transformer(dropout_rate=0.5)
    network.main_graph()
    # network.prapare_graph()
    network.train_graph()


def create_graph_for_evaluate():
    network = Transformer(dropout_rate=0.0)
    network.main_graph(is_training=False, use_train_data=False)
    predict_set, feature_vectors_test, score_set_test, opt_thres_useless = network.evaluation(network.test_label)


def create_graph_for_features():
    network = Transformer(dropout_rate=0.0)
    network.main_graph(is_training=False, use_train_data=True)
    predict_set, feature_vectors_train, score_set_train, opt_thres = network.evaluation(network.train_label)
    network.main_graph(is_training=False, use_train_data=False)
    predict_set, feature_vectors_test, score_set_test, opt_thres_useless = network.evaluation(network.test_label)
    run_ml_classification(feature_vectors_train, feature_vectors_test, network.train_label, network.test_label,
                          score_set_test, opt_thres)

# def create_graph_for_test_train():
#     network = Transformer(dropout_rate=0.0)
#     network.main_graph(is_training=False, use_train_data=False)
#     predict_set, feature_set, score_set = network.evaluation()
#

def run_ml_classification(train_features, test_features, train_label, test_label, scores, opt_thres):
    get_visualizations = False
    use_cross_validation = False
    model_name = 'Transformer'
    helper_functions.results_new_threshold(scores[:, 1], test_label, opt_thres, 0, model_name,
                                           False)
    parameters = (train_features, train_label, test_features, test_label, model_name)
    file = {"file_write": True, "use_count_data": False}
    print('starting ml classification')
    if use_cross_validation:
        print('starting ml classification with cross validation')
        ml_algorithms.hyperparameter_random_search(*parameters, class_type='knn', **file)
        ml_algorithms.hyperparameter_random_search(*parameters, class_type='svm', **file)
        ml_algorithms.hyperparameter_random_search(*parameters, class_type='rf', **file)
    else:
        ml_algorithms.svm_classification(*parameters, **file)
        ml_algorithms.random_forest(*parameters, **file)
        ml_algorithms.knearest_neighbors(*parameters, **file)
        # ml_algorithms.xgb_classifier(*parameters, **file)

    if get_visualizations:
        helper_functions.visualize_features(train_features, train_label, test_features, test_label,
                                            name=model_name, mode='train',
                                            method='tsne', dimension='2d')
        helper_functions.visualize_features(train_features, train_label, test_features, test_label,
                                            name=model_name, mode='train',
                                            method='pca', dimension='2d')


if __name__ == "__main__":
    create_graph_for_train()
#
# num_examples =..  # Dataset size
# num_parallel_calls = 16  # CPU threads to use for decoding
# prefetch_size = 16  # Queue up multiple batches CPU side
#
# # ...
#
# # Infinitely loop the dataset, shuffling once per epoch (in memory).
# # Safe to do when the dataset pipeline is currently light weight references
# # to data such as integer id's of examples or string filenames...
# data = data.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=num_examples))
#
# # Turn our lightweight reference to data into an actual example.
# # Image id -> Image / Filename -> Image.
# data = data.map(decode_example, num_parallel_calls=num_parallel_calls)
#
# # Stack decoded examples into constant sized batches.
# # Throwing away the remainder allows the pipeline to report a fixed sized batch size,
# # aiding in model definition downstream.
# data = data.batch(batch_size, drop_remainder=True)
#
# # Queue up a number of batches on the CPU side
# data = data.prefetch(prefetch_size)
#
# # Queue up batches asynchronously onto the GPU
# # As long as there is a pool of batches CPU side a GPU prefetch of 1 is sufficient.
# data = data.apply(tf.data.experimental.prefetch_to_device('/device:GPU:%d' % (device_num)))
# def prapare_graph(self):
#     with self.graph.as_default():
#         with tf.Session() as session:
#             session.run(tf.variables_initializer(self.graph.get_collection_ref('optimizer')[0].variables()))
#
#         print(self.graph.get_all_collection_keys())


# def train_graph(self):
#
#     with self.graph.as_default():
#         # with tf.Session() as session:
#
#         # print(self.graph.get_all_collection_keys())
#
#         # Start session
#         # sv = tf.train.Supervisor(graph=self.graph,
#         #                          logdir=self.logdir,
#         #                          save_model_secs=0)
#         # with sv.managed_session() as sess:
#         #     for epoch in range(1, self.num_epochs + 1):
#         #         sess.run(self.graph.get_collection_ref('iter_op')[0].initializer)
#         #         while not sv.should_stop():
#         #             sess.run(self.graph.get_collection_ref('train_op')[0])
#         #         gs = sess.run(self.graph.get_collection_ref('global_step')[0])
#         #         sv.saver.save(sess, self.logdir + '/model_epoch_%02d_gs_%d' % (epoch, gs))
#         final_predictions = []
#
#         with tf.train.MonitoredTrainingSession(
#                 scaffold=self.graph.get_collection_ref('scaffold')[0],
#                 summary_dir=self.logdir + '//summary//',
#                 hooks=[self.saver_hook]) as sess:
#             while not sess.should_stop():
#                 _, preds, acc, mean_loss, logits = sess.run(
#                     [self.graph.get_collection_ref('train_op')[0], self.graph.get_collection_ref('preds')[0],
#                      self.graph.get_collection_ref('acc')[0], self.graph.get_collection_ref('mean_loss')[0],
#                      self.graph.get_collection_ref('logits_norm')[0]])
#                 final_predictions.append(preds)
#
#                 # if len(preds) is not self.batch_size:
#                 #     final_predictions = np.concatenate(final_predictions, axis=None)
#                 #     print(confusion_matrix(self.train_label, final_predictions))
#                 #     final_predictions = []
#
#                 print(mean_loss)
#                 print(acc)
#                 print(preds)
#                 print(logits)
#             # final_predictions = np.concatenate(final_predictions, axis=None)
#
#     print("Done")

# def evaluation(self):
#     with self.graph.as_default():
#         # Start session
#         final_predictions = []
#         encoded_all = []
#         with tf.train.MonitoredTrainingSession(
#                 scaffold=self.graph.get_collection_ref('scaffold')[0],
#                 summary_dir=self.logdir + '//summary//',
#                 checkpoint_dir=self.logdir) as sess:
#             a = 0
#             while not sess.should_stop():
#                 a = a + 1
#                 predictions, acc, labels, encoded_batch = sess.run(
#                     [self.graph.get_collection_ref('preds')[0], self.graph.get_collection_ref('acc')[0],
#                      self.graph.get_collection_ref('labels')[0], self.graph.get_collection_ref('encoded')[0]])
#                 final_predictions.append(predictions)
#                 encoded_all.append(encoded_batch)
#                 print(acc)
#                 print(predictions)
#                 print(labels)
#         final_predictions = np.concatenate(final_predictions, axis=None)
#         encoded_all_final = np.concatenate(encoded_all, axis=None)
#         print(a)
#         print(confusion_matrix(self.test_label, final_predictions))
#     return encoded_all_final

    # def train_test_graph_run(self):
    #
    #     if not self.new_model:
    #         restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
    #         main_graph = tf.get_default_graph()
    #     else:
    #         restore_op = None
    #         main_graph = self.graph
    #
    #     if self.change_graph:
    #         restore_op = self.graph.get_collection_ref('saver')[0]
    #         main_graph = self.graph
    #
    #     with main_graph.as_default():
    #         query_list_train = [main_graph.get_collection_ref(str)[0] for str in
    #                       ['optim_op', 'loss', 'acc', 'preds', 'scores', 'label']]
    #
    #         query_list_test = [main_graph.get_collection_ref(str)[0] for str in
    #                       ['acc', 'preds', 'scores', 'encoded']]
    #
    #         print(main_graph.get_all_collection_keys())
    #         print(main_graph.get_collection_ref('iterators'))
    #         with tf.Session(config=self.config) as session:
    #
    #             if not self.new_model:
    #                 restore_op.restore(session, self.root_dir + '\\' + self.model_name)
    #             else:
    #                 session.run(tf.global_variables_initializer())
    #
    #             for epoch_step in range(self.num_epochs):
    #
    #
    #                 final_predictions = []
    #                 epoch_loss = 0
    #                 epoch_accuracy = 0
    #                 step = 0
    #                 session.run(main_graph.get_operation_by_name('train_init'))
    #
    #                 while True:
    #                     step = step + 1
    #                     try:
    #
    #                         _, batch_loss, batch_acc, batch_preds, batch_scores, batch_labels = session.run(query_list_train)
    #
    #                         epoch_loss = epoch_loss + batch_loss
    #                         epoch_accuracy = epoch_accuracy + batch_acc
    #                         final_predictions.append(batch_preds)
    #                     except tf.errors.OutOfRangeError:
    #                         print('epoch_train_end')
    #                         epoch_loss = epoch_loss / (step - 1)
    #                         epoch_accuracy = epoch_accuracy / (step - 1)
    #                         break
    #                 print("Epoch: %.4d cost=  %.9f ", (epoch_step + 1), epoch_loss)
    #                 logging.info("Epoch: %.4d cost=  %.9f", (epoch_step + 1), epoch_loss)
    #                 print("Epoch:", '%.4d' % (epoch_step + 1), "acc=", "%9f" % epoch_accuracy)
    #                 logging.info("Epoch: %.4d acc =  %.9f ", (epoch_step + 1), epoch_accuracy)
    #
    #                 predict_set = np.concatenate(final_predictions)
    #                 print(confusion_matrix(self.train_label, predict_set))
    #
    #
    #                 session.run(main_graph.get_operation_by_name('train_init'))
    #
    #                 step = 0
    #                 loss = 0
    #                 accuracy = 0
    #
    #                 final_predictions = []
    #                 train_features = []  # type: list
    #                 train_scores = []
    #                 while True:
    #                     step = step + 1
    #                     try:
    #                         batch_accuracy, batch_predictions, batch_scores, batch_features = session.run(
    #                             query_list_test)
    #
    #                         final_predictions.append(batch_predictions)
    #                         train_features.append(batch_features)
    #                         train_scores.append(batch_scores)
    #                         print(batch_scores)
    #
    #                         # loss = loss + batch_loss
    #                         accuracy = accuracy + batch_accuracy
    #
    #                     except tf.errors.OutOfRangeError:
    #                         print('epoch_test_end')
    #                         # loss = loss / (step - 1)
    #                         accuracy = accuracy / (step - 1)
    #                         break
    #
    #                 # print("Epoch: %.4d cost=  %.9f ", (0 + 1), loss)
    #                 # logging.info("Epoch: %.4d cost=  %.9f", (0 + 1), loss)
    #                 print("Epoch:", '%.4d' % (0 + 1), "acc=", "%9f" % accuracy)
    #                 logging.info("Epoch: %.4d acc =  %.9f ", (0 + 1), accuracy)
    #
    #                 predict_set = np.concatenate(final_predictions)
    #                 feature_set = np.concatenate(train_features)
    #                 score_set = np.concatenate(train_scores)
    #                 print(predict_set.shape)
    #                 print(self.test_label.shape)
    #                 predict_set = np.concatenate(final_predictions)
    #                 print(confusion_matrix(self.test_label, predict_set))
    #                 # self.run_ml_classification(train_set, test_set, None, None)
    #
    #             if self.save_results:
    #                 if not self.new_model:
    #                     restore_op.save(session, self.root_dir + '\\' + self.model_name)
    #                 else:
    #                     main_graph.get_collection_ref('saver')[0].save(session, self.root_dir + '\\' + self.model_name)
    #