from boruta import boruta_py

from imblearn.combine import SMOTEENN, SMOTETomek
from imblearn.over_sampling import SMOTE, ADASYN

from scipy.stats import randint as sp_randint

from sklearn.cluster import DBSCAN
from sklearn.cluster import KMeans

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MaxAbsScaler
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF

from sklearn.svm import LinearSVC

from skrebate import ReliefF
from xgboost import XGBClassifier

import numpy as np

from pyts.classification import SAXVSM

import helper_functions
import data_load_functions

result_dir = 'D:\\thesis_code_base\\saveddata\\Networks\\results'


def cross_val_score():
    return


def svm_classification(train_features, train_labels, test_features, test_labels, model_name=None, file_write=True,
                       use_count_data=False):
    # scaling = MaxAbsScaler().fit(train_features)
    scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_features)
    X_train = scaling.transform(train_features)
    X_test = scaling.transform(test_features)

    clf = LinearSVC(random_state=0, tol=1e-5, class_weight='balanced')
    clf.fit(X_train, train_labels)
    predictions = clf.predict(X_test)

    if (file_write and model_name is not None):
        model_dir = result_dir + '\\' + model_name
        file = open(model_dir + '_class_results.txt', 'a')
        file.write('svm classification = ' + '\n')
        file.close()
    else:
        model_dir = None

    print('svm classification = ')
    print(helper_functions.save_classification_statistics(model_dir, predictions, test_labels, 0, file_write,
                                                          use_count_data))


def quadratic_discriminant_classifier(train_features, train_labels, test_features, test_labels, model_name=None,
                                      file_write=True, use_count_data=False):
    clf = QuadraticDiscriminantAnalysis()
    clf.fit(train_features, train_labels)
    predictions = clf.predict(test_features)

    if (file_write and model_name is not None):
        model_dir = result_dir + '\\' + model_name
        file = open(model_dir + '_class_results.txt', 'a')
        file.write('quad discriminant classification = ' + '\n')
        file.close()
    else:
        model_dir = None

    print('quad discriminant classification = ')
    print(helper_functions.save_classification_statistics(model_dir, predictions, test_labels, 0, file_write,
                                                          use_count_data))


def random_forest(train_features, train_labels, test_features, test_labels, model_name=None, file_write=True,
                  use_count_data=False):
    clf = RandomForestClassifier(n_estimators=30,
                                 criterion='gini',
                                 max_depth=None,
                                 min_samples_split=2,
                                 min_samples_leaf=1,
                                 min_weight_fraction_leaf=0.0,
                                 max_features='auto',
                                 max_leaf_nodes=None,
                                 min_impurity_decrease=0.0,
                                 min_impurity_split=None,
                                 bootstrap=True,
                                 oob_score=False,
                                 n_jobs=-1,
                                 random_state=None,
                                 verbose=0,
                                 warm_start=False,
                                 class_weight="balanced_subsample")

    clf.fit(train_features, train_labels)
    predicted_labels = clf.predict(test_features)

    if (file_write and model_name is not None):
        model_dir = result_dir + '\\' + model_name
        file = open(model_dir + '_class_results.txt', 'a')
        file.write('rf classification = ' + '\n')
        file.close()
    else:
        model_dir = None

    print('rf classification = ')
    print(helper_functions.save_classification_statistics(model_dir, predicted_labels, test_labels, 0, file_write,
                                                          use_count_data))


def SAXVSM_classification():
    converted_train_data, train_label, converted_test_data, test_label, \
    embedding_mat, dictionary, class_weight = \
        data_load_functions.load_data_v3_all_mixed(load_larger_test=False, modify_opcode=True)
    # SAXVSM transformation
    saxvsm = SAXVSM(n_bins=3, strategy='quantile', window_size=2,
                    sublinear_tf=True)
    saxvsm.fit(converted_train_data, train_label)
    tfidf = saxvsm.tfidf_
    vocabulary_length = len(saxvsm.vocabulary_)
    X_new = saxvsm.decision_function(converted_train_data)
    predicted_labels = saxvsm.predict(converted_train_data)
    print('SAXVSM classification = ')
    print(helper_functions.save_classification_statistics(binary_predictions=predicted_labels, truth_labels=test_label,
                                                          epoch_num=0,
                                                          model_dir=None, file_write=None))


def gp_classifier(train_features, train_labels, test_features, test_labels, model_name=None, file_write=True,
                  use_count_data=False):
    kernel = 1.0 * RBF(1.0)
    clf = GaussianProcessClassifier(kernel=kernel, random_state=0, multi_class='one_vs_one').fit(
        train_features.toarray(), train_labels)
    predicted_labels = clf.predict(test_features.toarray())

    if (file_write and model_name is not None):
        model_dir = result_dir + '\\' + model_name
        file = open(model_dir + '_class_results.txt', 'a')
        file.write('gp classification')
        file.close()
    else:
        model_dir = None

    print('gp classification')
    print(helper_functions.save_classification_statistics(model_dir, predicted_labels, test_labels, 0, file_write,
                                                          use_count_data))


def knearest_neighbors(train_features, train_labels, test_features, test_labels, model_name=None, file_write=True,
                       use_count_data=False):
    n = 5
    clf = KNeighborsClassifier(n_neighbors=n,
                               weights='distance',
                               algorithm='auto',
                               leaf_size=30,
                               p=2,
                               metric='minkowski',
                               metric_params=None, n_jobs=-1)
    clf.fit(train_features, train_labels)
    predicted_labels = clf.predict(test_features)

    if (file_write and model_name is not None):
        model_dir = result_dir + '\\' + model_name
        file = open(model_dir + '_class_results.txt', 'a')
        file.write('knn classification  n = ' + str(n) + '\n')
        file.close()
    else:
        model_dir = None

    print('knn classification  n = ' + str(n))
    print(helper_functions.save_classification_statistics(model_dir, predicted_labels, test_labels, 0, file_write,
                                                          use_count_data))


def gradient_boosting(train_features, train_labels, test_features, test_labels, model_name=None,
                      file_write=True, use_count_data=False):
    clf = GradientBoostingClassifier(loss="deviance",
                                     learning_rate=0.1,
                                     n_estimators=100,
                                     subsample=1.0,
                                     criterion='friedman_mse',
                                     min_samples_split=2,
                                     min_samples_leaf=1,
                                     min_weight_fraction_leaf=0.0,
                                     max_depth=3,
                                     min_impurity_decrease=0.0,
                                     min_impurity_split=None,
                                     init=None,
                                     random_state=None,
                                     max_features=None,
                                     verbose=0,
                                     max_leaf_nodes=None,
                                     warm_start=False,
                                     presort='auto')
    clf.fit(train_features, train_labels)
    predicted_labels = clf.predict(test_features)

    if (file_write and model_name is not None):
        model_dir = result_dir + '\\' + model_name
        file = open(model_dir + '_class_results.txt', 'a')
        file.write('gradient boosting classification = ' + '\n')  # if __name__ == '__main__':
        file.close()
    else:
        model_dir = None
    print('gradient boosting classification = ')

    print(helper_functions.save_classification_statistics(model_dir, predicted_labels, test_labels, 0, file_write,
                                                          use_count_data))


def xgb_classifier(train_features, train_labels, test_features, test_labels, model_name=None, file_write=True,
                   use_count_data=False):
    if use_count_data:
        objective = 'multi:softmax'
        eval_metric = 'merror'
        # eval_metric = 'mlogloss'
    else:
        objective = 'binary:logistic'
        eval_metric = 'auc'

    clf = XGBClassifier(max_depth=100,
                        min_child_weight=1,
                        learning_rate=0.1,
                        n_estimators=9000,
                        n_jobs=-1,
                        silent=True,
                        objective=objective,
                        gamma=0,
                        max_delta_step=0,
                        subsample=0.9,
                        colsample_bytree=1,
                        colsample_bylevel=1,
                        reg_alpha=0,
                        reg_lambda=0,
                        scale_pos_weight=1,
                        seed=1,
                        missing=None)

    clf.fit(train_features, train_labels, eval_metric=eval_metric, verbose=True,
            eval_set=[(train_features, train_labels), (test_features, test_labels)], early_stopping_rounds=100)
    predicted_labels = clf.predict(test_features)
    evals_result = clf.evals_result()

    helper_functions.plot_logloss(evals_result['validation_0'][eval_metric], evals_result['validation_1'][eval_metric])

    if (file_write and model_name is not None):
        model_dir = result_dir + '\\' + model_name
        file = open(model_dir + '_class_results.txt', 'a')
        file.write('xgb boosting classification = ' + '\n')  # if __name__ == '__main__':
        file.close()
    else:
        model_dir = None
    print('xgb boosting classification = ')
    print(helper_functions.save_classification_statistics(model_dir, predicted_labels, test_labels, 0, file_write,
                                                          use_count_data))


def mlp_classifier(train_features, train_labels, test_features, test_labels, model_name=None, file_write=True,
                   use_count_data=False):
    clf = MLPClassifier(hidden_layer_sizes=(32, 32, 16, 16),
                        activation='relu',
                        solver='adam',
                        alpha=0.0001,
                        batch_size='auto',
                        learning_rate='invscaling',
                        learning_rate_init=0.007,
                        power_t=0.5,
                        max_iter=200,
                        shuffle=True,
                        random_state=None,
                        tol=0.000000001,
                        verbose=True,
                        warm_start=False,
                        momentum=0.9,
                        nesterovs_momentum=True,
                        early_stopping=False,
                        validation_fraction=0.1,
                        beta_1=0.9,
                        beta_2=0.999,
                        epsilon=1e-08)

    clf.fit(train_features, train_labels)
    predicted_labels = clf.predict(test_features)

    if (file_write and model_name is not None):
        model_dir = result_dir + '\\' + model_name
        file = open(model_dir + '_class_results.txt', 'a')
        file.write('mlp classification = ' + '\n')  # if __name__ == '__main__':
        file.close()
    else:
        model_dir = None
    print('mlp classification = ')

    print(helper_functions.save_classification_statistics(model_dir, predicted_labels, test_labels, 0, file_write,
                                                          use_count_data))


def adaboost_classifier(train_features, train_labels, test_features, test_labels, model_name=None, file_write=True,
                        use_count_data=False):
    clf = AdaBoostClassifier(base_estimator=None,
                             n_estimators=2000,
                             learning_rate=0.2,
                             algorithm='SAMME.R',
                             random_state=None)

    clf.fit(train_features, train_labels)
    predicted_labels = clf.predict(test_features)

    if (file_write and model_name is not None):
        model_dir = result_dir + '\\' + model_name
        file = open(model_dir + '_class_results.txt', 'a')
        file.write('adaboost classification = ' + '\n')
        file.close()
    else:
        model_dir = None
    print('adaboost classification = ')

    print(helper_functions.save_classification_statistics(model_dir, predicted_labels, test_labels, 0, file_write,
                                                          use_count_data))


def hyperparameter_random_search(train_features, train_labels, test_features, test_labels, model_name=None,
                                 file_write=True,
                                 use_count_data=False, class_type='knn'):
    if (class_type == 'knn'):
        clf = KNeighborsClassifier(algorithm='auto', n_jobs=-1)
        param_dist = {"n_neighbors": [1, 3, 5, 7, 11, 19, 23, 33, 51],
                      "weights": ['distance', 'uniform']}
    elif (class_type == 'svm'):
        scaling = MinMaxScaler(feature_range=(-1, 1)).fit(train_features)
        train_features = scaling.transform(train_features)
        test_features = scaling.transform(test_features)
        clf = LinearSVC(random_state=0, tol=1e-5, class_weight='balanced')
        param_dist = {"C": np.linspace(0.1, 20, num=20)}
    elif (class_type == 'rf'):
        clf = RandomForestClassifier()
        param_dist2 = {"max_depth": [3, None],
                       "max_features": sp_randint(1, 6),
                       "min_samples_split": sp_randint(2, 7),
                       "bootstrap": [True, False],
                       "criterion": ["gini", "entropy"],
                       "n_estimators": [10, 30, 50, 100]}
        param_dist = {"max_depth": [3, None],
                      "max_features": np.arange(1, 6),
                      "min_samples_split": np.arange(2, 7),
                      "bootstrap": [True, False],
                      "criterion": ["gini", "entropy"],
                      "n_estimators": [10, 30, 50, 100]}
    else:
        clf = None
        param_dist = None

    # run randomized search
    random_search = GridSearchCV(clf, param_grid=param_dist, cv=5, scoring='roc_auc')
    # n_iter_search = 20
    # random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
    #                              n_iter=n_iter_search, cv=5, scoring='f1')

    random_search.fit(train_features, train_labels)
    predicted_labels = random_search.predict(test_features)

    if (file_write and model_name is not None):
        model_dir = result_dir + '\\' + model_name
        file = open(model_dir + '_class_results.txt', 'a')
        file.write('random  hyperparameter search result = ' + str(class_type) + '\n')
        file.close()
    else:
        model_dir = None
    print('random search result = ' + str(class_type))

    print(helper_functions.save_classification_statistics(model_dir, predicted_labels, test_labels, 0, file_write,
                                                          use_count_data))


def tfidf_transform(frequency_data, use_idf):
    transformer = TfidfTransformer(norm="l2",
                                   use_idf=use_idf,
                                   smooth_idf=True,
                                   sublinear_tf=False)
    return transformer.fit_transform(frequency_data)


def resample_dataset(dataset, labels, mode):
    if (mode == 'smote'):
        kinds = ['regular', 'borderline1', 'borderline2', 'svm']
        dataset_resampled, labels_resampled = SMOTE(kind=kinds[2]).fit_sample(dataset, labels)
    elif (mode == 'adasyn'):
        dataset_resampled, labels_resampled = ADASYN().fit_sample(dataset, labels)
    elif (mode == 'smoteenn'):
        smote_enn = SMOTEENN(random_state=0)
        dataset_resampled, labels_resampled = smote_enn.fit_sample(dataset, labels)
    elif (mode == 'smotetomek'):
        smote_tomek = SMOTETomek(random_state=0)
        dataset_resampled, labels_resampled = smote_tomek.fit_sample(dataset, labels)
    else:
        dataset_resampled = None
        labels_resampled = None

    return dataset_resampled, labels_resampled


def feature_selection(train_features, test_features, labels, n_features_to_select=100):
    selector = ReliefF(n_features_to_select=n_features_to_select, n_neighbors=100, n_jobs=-1)
    selector = selector.fit(train_features, labels)
    top_train = selector.transform(train_features)
    top_test = selector.transform(test_features)
    return top_train, top_test


def k_means_clustering(features):
    kmeans_func = KMeans(n_clusters=2,
                         init='k-means++',
                         n_init=10,
                         max_iter=300,
                         tol=0.0001,
                         precompute_distances='auto',
                         verbose=0,
                         random_state=None,
                         copy_x=True,
                         algorithm='auto',
                         n_jobs=-1)
    cluster_labels = kmeans_func.fit_predict(features)
    return cluster_labels


def dbscan_clustering(train_features):
    cls = DBSCAN(eps=0.5,
                 min_samples=5,
                 metric='euclidean',
                 metric_params=None,
                 algorithm='auto',
                 leaf_size=30,
                 p=None,
                 n_jobs=- 1)

    cluster_labels = cls.fit_predict(train_features, y=None, sample_weight=None)
    return cluster_labels


def boruta_feature_selection(train_features, test_features, labels, dataset_type):
    train_features_dense = train_features
    test_features_dense = test_features
    if dataset_type == 'count':
        train_features_dense = train_features.toarray()
        test_features_dense = test_features.toarray()

    forest = RandomForestClassifier(n_jobs=-1, class_weight=None)
    feat_selector = boruta_py.BorutaPy(forest, n_estimators='auto', verbose=1000)
    feat_selector.fit(train_features_dense, labels)
    X_filtered = feat_selector.transform(train_features_dense)
    test_filtered = feat_selector.transform(test_features_dense)
    return X_filtered, test_filtered
