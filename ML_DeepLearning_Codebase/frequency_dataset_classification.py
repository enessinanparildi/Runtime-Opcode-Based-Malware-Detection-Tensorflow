import ml_algorithms
import data_load_functions
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
import numpy as np


def freqdataset_classification(transform_type, use_feature_selection=True):
    add_crpto_dataset = True
    pca_reduction = False
    frequency_data, frequency_numberlabels, binary_labels = data_load_functions.load_freq_dataset()
    crpto_data, crpto_labels = data_load_functions.load_freq_dataset_newcrypto(subset_size=9000)
    print(crpto_labels)
    if add_crpto_dataset:
        frequency_data = np.concatenate((frequency_data, crpto_data), axis=0)
        frequency_numberlabels = np.concatenate((frequency_numberlabels, crpto_labels), axis=None)
        print(frequency_data.shape)
        print(frequency_numberlabels.shape)
    frequency_numberlabels = frequency_numberlabels.astype(int)
    if pca_reduction:
        pca = PCA(n_components=300)
        frequency_data = pca.fit_transform(frequency_data)
        print(pca.explained_variance_ratio_)

    if transform_type == 'tfid':
        norm_frequency_data = ml_algorithms.tfidf_transform(frequency_data, use_idf=True)
    elif transform_type == 'tf':
        norm_frequency_data = ml_algorithms.tfidf_transform(frequency_data, use_idf=False)
    else:
        norm_frequency_data = frequency_data

    train_features, test_features, train_label, test_label = train_test_split(norm_frequency_data,
                                                                              frequency_numberlabels,
                                                                              test_size=0.3,
                                                                              random_state=42)
    # print(np.unique(train_label))
    # print(np.unique(test_label))

    parameters = (train_features, train_label, test_features, test_label)

    file = {"model_name": None, "file_write": True, "use_count_data": True}

    if use_feature_selection:
        filtered_train_features, filtered_test_features = ml_algorithms.boruta_feature_selection(train_features,
                                                                                                 test_features,
                                                                                                 train_label,
                                                                                                 dataset_type='count')
        parameters = (filtered_train_features, train_label, filtered_test_features, test_label)

    # file["model_name"] = 'svm'
    # results_svm = ml_algorithms.svm_classification(*parameters, **file)
    # # results_ada = ml_algorithms.adaboost_classifier(*parameters, **file)
    file["model_name"] = 'random_forest'
    results_rf = ml_algorithms.random_forest(*parameters, **file)
    file["model_name"] = 'gradient_boosting'
    results_xgb = ml_algorithms.xgb_classifier(*parameters, **file)
    # file["model_name"] = 'gaussian_process'
    # ml_algorithms.gp_classifier(*parameters, **file)
    # results_gb = ml_algorithms.gradient_boosting(*parameters, **file)
    # results_qd = ml_algorithms.quadratic_discriminant_classifier(*parameters, **file)
    # results_knn = ml_algorithms.knearest_neighbors(*parameters, **file)


if __name__ == "__main__":
    freqdataset_classification('tf', False)

    # result1 = pool.submit(svm_classification, *parameters, **file)
    # result2 = pool.submit(random_forest, *parameters, **file)
    # result3 = pool.submit(gradient_boosting, *parameters, **file)
    # result4 = pool.submit(knearest_neighbors, *parameters, **file)
