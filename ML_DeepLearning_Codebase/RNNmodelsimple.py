import numpy as np
import pandas as pd
import tensorflow as tf

seq = dataset

input_size = 1
num_steps = 5
lstm_size = 128

num_layers = 1
max_epoch = 130
keep_prob = 0.9
split = 2800
batch_size = 100
n_classes = 2


tf.reset_default_graph()
lstm_graph = tf.Graph()

seq = [np.array(seq[i * input_size: (i + 1) * input_size])
       for i in range(len(seq) // input_size)]

# Split into groups of `num_steps`
X = np.array([seq[i: i + num_steps] for i in range(len(seq) - num_steps)])
y = np.array([seq[i + num_steps] for i in range(len(seq) - num_steps)])

x_train = X[0:split, :, :]
y_train = y[0:split, :]

x_test = X[split + 1:, :, :]
y_test = y[split + 1:, :]

total_size = x_train.shape[0]

with lstm_graph.as_default():

    inputs = tf.placeholder("float", [None, num_steps, input_size])
    targets = tf.placeholder("float", [None, n_classes])

    def _create_one_cell():
        lstm_cell = tf.contrib.rnn.LSTMCell(lstm_size, state_is_tuple=True)
        if keep_prob < 1.0:
            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)
        return lstm_cell

    cell = tf.contrib.rnn.MultiRNNCell(
        [_create_one_cell() for _ in range(num_layers)],
        state_is_tuple=True
    ) if num_layers > 1 else _create_one_cell()

    #    learningrate = 0.1
    val, _ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)
    val = tf.transpose(val, [1, 0, 2])
    # Choosing only last output for sequence classification
    last = tf.gather(val, int(val.get_shape()[0]) - 1, name="last_lstm_output")

    weight = tf.Variable(tf.truncated_normal([lstm_size, input_size]))
    bias = tf.Variable(tf.constant(0.1, shape=[input_size]))

    prediction = tf.matmul(last, weight) + bias

    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))
    optimizer = tf.train.AdamOptimizer().minimize(cost)

with tf.Session(graph=lstm_graph) as sess:
    tf.global_variables_initializer().run()
    test_data_feed = {inputs: x_test, targets: y_test}

    for epoch_step in range(max_epoch):

        total_batch = int(total_size / batch_size)

        for i in range(total_batch):
            batch_x = x_train[i * batch_size: (i + 1) * batch_size, :, :]
            batch_y = y_train[i * batch_size: (i + 1) * batch_size, :]
            train_data_feed = {  inputs: batch_x, targets: batch_y }
            train_loss, _ = sess.run([optimizer, cost], train_data_feed)

        if epoch_step % 1 == 0:
            test_loss, _pred, = sess.run([cost, prediction], test_data_feed)
            train_loss, _ = sess.run([cost, prediction], train_data_feed)
            print("Epoch test loss %d :" % (epoch_step), test_loss)
            print("Epoch train loss %d :" % (epoch_step), train_loss)

    whole_train_data_feed = {inputs: x_train, targets: y_train}
    train_final_prediction, train_final_loss = sess.run([prediction, cost], whole_train_data_feed)

    final_prediction, final_loss = sess.run([prediction, cost], test_data_feed)

y_pred = final_prediction.reshape(-1, 1)
x_pred = train_final_prediction.reshape(-1, 1)

