from collections import Counter

from os import listdir
from os import makedirs

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import scipy.io as sio
import pandas as pd

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import fbeta_score
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

from sklearn.preprocessing import OneHotEncoder
from sklearn.utils.class_weight import compute_class_weight

from tabulate import tabulate

import ml_algorithms
import opcode_embeddings
import preprocessing


def bidirectional_model_save(root_dir, train_loss, train_accuracy, test_lost, test_accuracy, max_epoch, keep_prob,
                             lstmfw_size, lstmbw_size, num_layers, model_name, attention_size, save_name=None):
    details = 'train_loss = ' + str(train_loss)[0:13] + '_' + 'train_accuracy = ' + \
              str(train_accuracy)[0:13] + '_' + 'test_lost = ' \
              + str(test_lost)[0:13] + '_' + 'test_accuracy = ' + str(test_accuracy)[0:13] \
              + '_max_epoch = ' + str(max_epoch) + "__" + '_keep_prob = ' + str(keep_prob) + "__" \
              + '_lstmfw_size = ' + str(lstmfw_size) + "__" + '_lstmbw_size = ' + str(
        lstmbw_size) + "__" + 'num_layers = ' + str(num_layers) + 'attention_size = ' + str(attention_size)
    if save_name is None:
        parameters = model_name
    else:
        parameters = save_name
    full_dir = root_dir + "\\" + parameters
    file = open(full_dir + '.txt', 'w')
    file.write(details)
    file.close()

    return full_dir


def bidirectional_make_new_model_path(root_dir, max_epoch, keep_prob, lstmfw_size, lstmbw_size, num_layers,
                                      attention_size):
    parameters = '_max_epoch = ' + str(max_epoch) + "__" + '_keep_prob = ' + str(keep_prob) + "__" \
                 + '_lstmfw_size = ' + str(lstmfw_size) + "__" + '_lstmbw_size = ' + \
                 str(lstmbw_size) + "__" + 'num_layers = ' + str(num_layers) + 'attention_size = ' + str(attention_size)
    full_dir = root_dir + "\\" + parameters
    makedirs(full_dir)
    return full_dir


def bidirectional_model_restore(root_dir, index):
    files_dirs = listdir(root_dir)
    requested_file_dir = files_dirs[index]
    full_directory = root_dir + "\\" + requested_file_dir
    full_directory_meta = full_directory + ".meta"
    return full_directory, full_directory_meta


def bidirectional_model_dir_pull(root_dir, index):
    files_dirs = listdir(root_dir)
    requested_file_dir = files_dirs[index]
    return requested_file_dir


def cnn_model_save(root_dir, feature_size, batch_size, model_type, model_name, data_load_func, save_name):
    details = 'feature_size = ' + str(feature_size) + '_' + 'batch_size = ' + \
              str(batch_size) + '_' + 'model_type = ' \
              + str(model_type) + '_' + str(data_load_func)
    if save_name is None:
        parameters = model_name
    else:
        parameters = save_name
    full_dir = root_dir + "\\" + parameters
    file = open(full_dir + '.txt', 'w')
    file.write(details)
    file.close()

    return full_dir


def plot_clustering_labels(cluster_labels, features, ground_truth_labels):
    pca = PCA(n_components=2)
    pca.fit(features)
    x_embedded = pca.transform(features)

    plt.subplot(121)

    X_embedded_cluster1 = x_embedded[np.where(cluster_labels == 1)[0], :]
    X_embedded_cluster2 = x_embedded[np.where(cluster_labels == 0)[0], :]

    plt.scatter(X_embedded_cluster1[:, 0], X_embedded_cluster1[:, 1], s=2, c='r')
    plt.scatter(X_embedded_cluster2[:, 0], X_embedded_cluster2[:, 1], s=2, c='b')
    plt.title('cluster_labels')

    plt.subplot(122)

    X_embedded_cluster1 = x_embedded[np.where(ground_truth_labels == 1)[0], :]
    X_embedded_cluster2 = x_embedded[np.where(ground_truth_labels == 0)[0], :]

    plt.scatter(X_embedded_cluster1[:, 0], X_embedded_cluster1[:, 1], s=2, c='r')
    plt.scatter(X_embedded_cluster2[:, 0], X_embedded_cluster2[:, 1], s=2, c='b')
    plt.title('ground_truth_labels')
    plt.show()


def plot_logloss(train_set, validation_set):
    n_iteration = len(train_set)
    plt.plot(np.arange(n_iteration), np.array(train_set), color='b', label='train')
    plt.plot(np.arange(n_iteration), np.array(validation_set), color='g', label="validation")
    plt.legend()
    plt.grid()
    plt.show()


def get_class_weight_for_cross_entropy_loss(train_labels):
    weight_vector = compute_class_weight('balanced', np.unique(train_labels), train_labels)
    weight_vector = weight_vector / weight_vector[0]
    return weight_vector[1]


def predict_custom_threshold(class_scores, threshold):
    return [1 if score >= threshold else 0 for score in class_scores]


def choose_optimal_threshold(class_scores, truth_labels, metric='mcc'):
    threshold_set = np.linspace(0.01, 1, 200)
    prediction_set = [predict_custom_threshold(list(class_scores[:, 1]), t) for t in threshold_set]

    if metric == 'f1':
        score_set = [f1_score(truth_labels, pred, pos_label=0) for pred in prediction_set]
    elif metric == 'acc':
        score_set = [accuracy_score(truth_labels, pred) for pred in prediction_set]
    elif metric == 'precision':
        score_set = [precision_score(truth_labels, pred, pos_label=0) for pred in prediction_set]
    elif metric == 'recall':
        score_set = [recall_score(truth_labels, pred, pos_label=0) for pred in prediction_set]
    elif metric == 'mcc':
        score_set = [matthews_corrcoef(truth_labels, pred) for pred in prediction_set]
    elif metric == 'fbeta':
        score_set = [fbeta_score(truth_labels, pred, beta=3) for pred in prediction_set]
    else:
        score_set = None

    opt_thres = threshold_set[int(np.argmax(score_set))]
    print('optimal threshold = ' + str(opt_thres))
    print(np.max(score_set))
    print(confusion_matrix(prediction_set[int(np.argmax(score_set))], truth_labels))

    return opt_thres


def results_new_threshold(scores, truth_labels, threshold, epoch_num, model_name, resample_switch):
    new_predictions = np.array([1 if score >= threshold else 0 for score in list(scores)])

    result_dir = 'D:\\thesis_code_base\\saveddata\\Networks\\results'

    file = open(result_dir + '\\' + model_name + '_class_results.txt', 'a')
    file.write('new epoch' + '\n')
    file.write('resample dataset =' + str(resample_switch) + '\n')
    file.write('classification with threshold adjustment' + '\n')
    file.close()

    print('classification with threshold adjustment')
    print(save_classification_statistics(result_dir + '\\' + model_name, new_predictions, truth_labels, epoch_num))

    new_predictions = np.array([1 if score >= 0.5 else 0 for score in list(scores)])

    file = open(result_dir + '\\' + model_name + '_class_results.txt', 'a')
    file.write('new epoch' + '\n')
    file.write('resample dataset =' + str(resample_switch) + '\n')
    file.write('classification with 0.5 threshold ' + '\n')
    file.close()

    print('classification with 0.5 threshold adjustment')
    print(save_classification_statistics(result_dir + '\\' + model_name, new_predictions, truth_labels, epoch_num))


def save_classification_statistics(model_dir, binary_predictions, truth_labels, epoch_num, file_write=True,
                                   use_count_data=False):
    print('use_count_data :' + str(use_count_data))
    if not use_count_data:
        target_names = ['benign', 'malware']
        conf_mat = confusion_matrix(truth_labels, binary_predictions)
        f1score = f1_score(truth_labels, binary_predictions, average='micro', pos_label=0)
        accuracy = accuracy_score(truth_labels, binary_predictions)
        mcc_score = matthews_corrcoef(truth_labels, binary_predictions)
        precision = precision_score(truth_labels, binary_predictions, pos_label=0)
        recall = recall_score(truth_labels, binary_predictions, pos_label=0)
        table = tabulate(conf_mat, target_names, tablefmt="fancy_grid")
        print(table)
        print(classification_report(truth_labels, binary_predictions, target_names=target_names, digits=5))
    else:
        target_names = ['adware', 'backdoor', 'browsermodifier', 'PWS', 'ransom', 'rogue', 'softwarebundler',
                        'trojan', 'trojandownloader', 'trojandropper', 'trojanspy', 'virtool', 'virus', 'worms',
                        'null', 'benign', 'crypto']
        precision = None
        recall = None
        mcc_score = None
        conf_mat = confusion_matrix(truth_labels, binary_predictions)
        f1score = f1_score(truth_labels, binary_predictions, average='weighted')
        accuracy = accuracy_score(truth_labels, binary_predictions)
        table = tabulate(conf_mat, target_names, tablefmt="fancy_grid")
        print(table)
        # print(np.unique(truth_labels))
        ypred = binary_predictions.astype(int)
        print(ypred)
        confusion_matrix_pretty(truth_labels, ypred, model_dir + 'conf2_mat.png', labels=np.arange(17),
                                ymap=target_names)
        print(classification_report(truth_labels, binary_predictions, target_names=target_names, digits=5))

    print('acc:' + str(np.trace(conf_mat) / len(truth_labels)))
    if file_write and model_dir is not None:
        file = open(model_dir + '_class_results.txt', 'a')
        file.write(
            'epoch_num = ' + str(epoch_num) + " = " + str(
                (conf_mat, accuracy, f1score, precision, recall, mcc_score)) + '\n')
        file.close()

    return conf_mat, accuracy, f1score, precision, recall, mcc_score


def pull_balanced_batch(data, label, batch_size):
    positive_indexes = np.where(label == 1)[0]
    negative_indexes = np.where(label == 0)[0]
    chosen_positive_indexes = np.random.choice(positive_indexes, int(batch_size * 0.5), replace=False)
    chosen_negative_indexes = np.random.choice(negative_indexes, int(batch_size * 0.5), replace=False)
    chosen_indexes = np.concatenate((chosen_positive_indexes, chosen_negative_indexes))
    batch_x = data[chosen_indexes, :]
    batch_y = label[chosen_indexes]
    return batch_x, batch_y


def batch_iteration(data, label, embedding_mat, dictionary, batch_index, batch_size, remain, query_outputs,
                    input_tensor, label_tensor, batch_size_tensor, keep_prob_tensor, batch_norm_switch,
                    only_forward_pass, mode, session, use_random_balanced_batch,
                    split_length=200, final=False, use_oversampling=False):
    global size

    if use_random_balanced_batch:
        batch_x, batch_y = pull_balanced_batch(data, label, batch_size)
        size = batch_size
    else:
        if not final:
            final_index = (batch_index + 1) * batch_size
            size = batch_size
        else:
            final_index = None
            size = remain

        batch_x = data[batch_index * batch_size: final_index, :]
        batch_y = label[batch_index * batch_size: final_index]

    embedding_batch_x = opcode_embeddings.get_sequence_batch_with_embedding(embedding_mat, batch_x, dictionary)

    feature_size = embedding_mat.shape[1]
    total_seq_length = 1000
    num_steps = int(total_seq_length / split_length)

    if use_oversampling:
        embedding_batch_x = embedding_batch_x.reshape((size, feature_size * total_seq_length))
        resampled_features, resampled_labels = ml_algorithms.resample_dataset(embedding_batch_x, batch_y, mode='smote')
        resampled_features = resampled_features.reshape((size, feature_size, total_seq_length))
        embedding_batch_x = resampled_features
        batch_y = resampled_labels

    embedding_batch_x = embedding_batch_x.reshape((size, split_length, feature_size, num_steps))

    if mode == "train":
        if only_forward_pass:
            data_feed = {input_tensor: embedding_batch_x,
                         batch_norm_switch: False,
                         batch_size_tensor: size,
                         keep_prob_tensor: 1.0}

            extracted_features = session.run(query_outputs, data_feed)
            return 0, 0, 0, extracted_features
        else:
            data_feed = {input_tensor: embedding_batch_x,
                         label_tensor: batch_y,
                         batch_norm_switch: True,
                         batch_size_tensor: size}
            _, train_loss, train_accuracy, train_predictions = session.run(query_outputs[0:4],
                                                                           data_feed)
            return train_loss, train_accuracy, train_predictions, 0

    elif mode == "test":
        data_feed = {input_tensor: embedding_batch_x,
                     label_tensor: batch_y,
                     batch_size_tensor: size,
                     batch_norm_switch: False,
                     keep_prob_tensor: 1.0}

        test_loss, test_accuracy, test_predictions, test_features, test_scores = session.run(query_outputs,
                                                                                             data_feed)
        return test_loss, test_accuracy, test_predictions, test_features, test_scores
    else:
        return None


def print_result(epoch_step, epoch_loss, epoch_accuracy, test_loss, test_accuracy, test_predictions, test_label,
                 model_name, resample_switch):
    result_dir = 'D:\\thesis_code_base\\saveddata\\Networks\\results'

    file = open(result_dir + '\\' + model_name + '_class_results.txt', 'a')
    file.write('new epoch' + '\n')
    file.write('resample dataset =' + str(resample_switch) + '\n')
    # file.write('class_weight =' + str(resample_switch) + '\n')
    file.close()

    print('\n')
    print("Epoch train loss %d :" % epoch_step, epoch_loss)
    print("Epoch train accuracy %d :" % epoch_step, epoch_accuracy)
    # print(save_classification_statistics(root_dir + '\\model_v5',final_predictions, t_label, epoch_step)
    print("Epoch test loss %d :" % epoch_step, test_loss)
    print("Epoch test accuracy %d :" % epoch_step, test_accuracy)
    print(save_classification_statistics(result_dir + '\\' + model_name, test_predictions, test_label, epoch_step))
    print('\n')


def one_hot_coder(dataset):
    enc = OneHotEncoder()
    one_hot = enc.fit_transform(dataset)
    return one_hot.toarray()


def calculate_auc_score(scores, true_labels):
    scores = scores[:, 1]
    score = roc_auc_score(true_labels, scores)
    fpr, tpr, _ = roc_curve(true_labels, scores)
    print("auc_score = " + str(score))
    plt.plot(fpr, tpr, label="auc score=" + str(score))
    plt.legend(loc=4)


def train_data_visualize(train_features, train_labels, predict_labels):
    pca = PCA(n_components=2)
    pca.fit(train_features)
    X_embedded = pca.transform(train_features)
    fig = plt.figure()
    ax = fig.add_subplot(121)
    x_embedded_old = X_embedded[:-79, :]
    X_embedded_new = X_embedded[-79:, :]
    for ind, label in enumerate(train_labels[:-79]):

        i = x_embedded_old[ind][0]
        j = x_embedded_old[ind][1]

        if int(label) == 1:
            if ind == 0:
                ax.scatter(i, j, s=1, c='r')
            else:
                ax.scatter(i, j, s=1, c='r')
        else:
            ax.scatter(i, j, s=1, c='b')

    for ind, label in enumerate(train_labels[-79:]):

        i = X_embedded_new[ind][0]
        j = X_embedded_new[ind][1]

        if int(label) == 1:
            if ind == 0:
                ax.scatter(i, j, s=1, c='g')
            else:
                ax.scatter(i, j, s=1, c='g')
        else:
            ax.scatter(i, j, s=1, c='c')
    ax2 = fig.add_subplot(122)
    for ind, label in enumerate(predict_labels[:-79]):

        i = x_embedded_old[ind][0]
        j = x_embedded_old[ind][1]

        if int(label) == 1:
            if ind == 0:
                ax2.scatter(i, j, s=1, c='r')
            else:
                ax2.scatter(i, j, s=1, c='r')
        else:
            ax2.scatter(i, j, s=1, c='b')

    print(predict_labels[-79:])
    for ind, label in enumerate(predict_labels[-79:]):

        i = X_embedded_new[ind][0]
        j = X_embedded_new[ind][1]

        if int(label) == 1:
            if ind == 0:
                ax2.scatter(i, j, s=1, c='g')
            else:
                ax2.scatter(i, j, s=1, c='g')
        else:
            ax2.scatter(i, j, s=1, c='c')

    # plt.savefig(root_dir_for_plots)
    plt.show()


def visualize_features(train_features, train_labels, test_features, test_labels, mode, method, name, dimension='2d'):
    features = np.concatenate((train_features, test_features))
    labels = np.concatenate((train_labels, test_labels))
    if dimension == '2d':
        explained_variance_ratio = None
        if method == 'tsne':
            x_embedded = TSNE(n_components=2, init='pca', n_iter=700, perplexity=30).fit_transform(features)
            dicti = {'val': x_embedded}
            sio.savemat("C:\\Users\\Ben\\Google Drive\\bigproject\\images\\plots\\cnntsnefeatures2d_" + str(name),
                        dicti)
            fig = plt.figure()
        else:
            pca = PCA(n_components=2)
            pca.fit(features)
            x_embedded = pca.transform(features)
            explained_variance_ratio = pca.explained_variance_ratio_
            fig = plt.figure()

        ax = fig.add_subplot(111)
        X_embedded_train = x_embedded[:len(train_labels), :]
        x_embedded_test = x_embedded[len(train_labels):, :]
        for ind, label in enumerate(train_labels):

            i = X_embedded_train[ind][0]
            j = X_embedded_train[ind][1]

            if int(label) == 1:
                if ind == 0:
                    ax.scatter(i, j, s=1, c='r')
                else:
                    ax.scatter(i, j, s=1, c='r')
            else:
                ax.scatter(i, j, s=1, c='b')

        for ind, label in enumerate(test_labels):

            i = x_embedded_test[ind][0]
            j = x_embedded_test[ind][1]

            if int(label) == 1:
                if ind == 0:
                    ax.scatter(i, j, s=1, c='m', marker="x")
                else:
                    ax.scatter(i, j, s=1, c='m')
            else:
                ax.scatter(i, j, s=1, c='c', marker="x")

    else:
        if method == 'tsne':
            x_embedded = TSNE(n_components=3, init='pca', n_iter=500, perplexity=30).fit_transform(features)
            dicti = {'val': x_embedded}
            sio.savemat("C:\\Users\\Ben\\Google Drive\\bigproject\\images\\plots\\cnntsnefeatures3d_" + str(name),
                        dicti)
            explained_variance_ratio = None
        else:
            pca = PCA(n_components=3)
            pca.fit(features)
            x_embedded = pca.transform(features)
            explained_variance_ratio = pca.explained_variance_ratio_
            dicti = {'val': x_embedded, 'var': explained_variance_ratio}
            sio.savemat("C:\\Users\\Ben\\Google Drive\\bigproject\\images\\plots\\cnnpcafeatures3d_test_" + str(name),
                        dicti)

        dicti = {'val': labels}
        sio.savemat("C:\\Users\\Ben\\Google Drive\\bigproject\\images\\plots\\labels_test", dicti)

    if mode == 'test' and method == 'pca':
        plt.title(
            'Red: Malware  Blue: Benign test data ' + "explained_variance_ratio = " + str(explained_variance_ratio))
    elif mode == 'train' and method == 'pca':
        plt.title(
            'Red: Malware  Blue: Benign train data' + "explained_variance_ratio = " + str(explained_variance_ratio))
    elif mode == 'test' and method == 'tsne':
        plt.title('Red: Malware  Blue: Benign test data tsne visualization')
    elif mode == 'train' and method == 'tsne':
        plt.title('Red: Malware  Blue: Benign train data tsne visualization')

    root_dir_for_plots = "C:\\Users\\Ben\\Google Drive\\bigproject\\images\\plots\\features_graph"
    plt.savefig(root_dir_for_plots)
    plt.show()


def extract_features_from_graph(train_data, embedding_mat, dictionary, query_outputs_feature_extract, input_tensor,
                                batch_size_tensor, keep_prob_tensor, Session):
    embedding_train = opcode_embeddings.get_sequence_batch_with_embedding(embedding_mat, train_data, dictionary)
    # embedding_test = opcode_embeddings.get_sequence_batch_with_embedding(embedding_mat, test_data, dictionary)
    feature_size = embedding_mat.shape[1]
    total_seq_length = 1000
    split_length = 100
    num_steps = int(total_seq_length / split_length)

    embedding_batch_x = embedding_train.reshape((len(train_data), split_length, feature_size, num_steps))

    train_data_feed = {input_tensor: embedding_batch_x, batch_size_tensor: len(train_data), keep_prob_tensor: 1.0}
    # test_data_feed = {input_tensor: embedding_test, batch_size_tensor : len(test_data) , keep_prob_tensor: 1.0 }

    train_features = Session.run(query_outputs_feature_extract, train_data_feed)
    # test_features = Session.run(query_outputs_feature_extract, test_data_feed)

    return train_features


def confusion_matrix_pretty(y_true: object, y_pred: object, filename: object, labels: object, ymap: object = None,
                            figsize: object = (20, 20)) -> object:
    if ymap is not None:
        y_pred = [ymap[yi] for yi in y_pred]
        y_true = [ymap[yi] for yi in y_true]
        labels = [ymap[yi] for yi in labels]
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_sum = np.sum(cm, axis=1, keepdims=True)
    cm_perc = cm / cm_sum.astype(float) * 100
    annot = np.empty_like(cm).astype(str)
    nrows, ncols = cm.shape
    for i in range(nrows):
        for j in range(ncols):
            c = cm[i, j]
            p = cm_perc[i, j]
            if i == j:
                s = cm_sum[i]
                annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
            elif c == 0:
                annot[i, j] = ''
            else:
                annot[i, j] = '%.1f%%\n%d' % (p, c)
    cm = pd.DataFrame(cm, index=labels, columns=labels)
    cm.index.name = 'Actual'
    cm.columns.name = 'Predicted'
    fig, ax = plt.subplots(figsize=figsize)
    sns.heatmap(cm, annot=annot, fmt='', ax=ax)
    plt.savefig(filename)
