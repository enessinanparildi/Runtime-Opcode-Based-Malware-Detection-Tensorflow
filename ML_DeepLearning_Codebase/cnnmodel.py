import numpy as np
import tensorflow as tf
from numpy.core.multiarray import ndarray

import ml_algorithms
import cnnmodeldefinitions
import helper_functions


def cnn_graph_run():
    new_model = True
    resample_dataset = False
    use_imbalanced = False
    use_class_weights = True
    save_network = True
    use_random_balanced_batch = False
    use_feature_selection = True

    train_model = True
    only_forward = True

    model_name = 'cnn_model_v8'
    save_name = 'cnn_model_v8'
    root_dir = 'C:\\Users\\Ben\\Google Drive\\bigproject\\codes\\saveddata\\Networks\\convolutional_networks'

    max_epoch = 30

    keep_prob_fc = 0.3
    keep_prob_conv = 0.9

    batch_size = 16
    test_batch_size = 32

    save_step = 1
    learning_rate = 0.00001
    n_classes = 2

    length_of_sequence = 1000

    if not train_model or only_forward:
        max_epoch = 1
        use_random_balanced_batch = False

    train_data, train_label, test_data, test_label, \
    one_hot_batch_train_label, one_hot_batch_test_label, \
    dictionary, embedding_mat, class_weight = helper_functions.prapare_data_v2(use_imbalanced)

    feature_size = embedding_mat.shape[1]
    total_data_length = train_data.shape[0]
    total_test_length = test_data.shape[0]

    tf.reset_default_graph()
    cnn_graph = tf.Graph()

    with cnn_graph.as_default():

        input_tensor = tf.placeholder("float", [None, length_of_sequence, feature_size, 1])
        label_tensor = tf.placeholder("float", [None, n_classes])
        batch_norm_switch = tf.placeholder(tf.bool, shape=())
        keep_prob_tensor = tf.placeholder_with_default(keep_prob_fc, shape=())
        batch_size_tensor = tf.placeholder_with_default(batch_size, shape=())

        modeldef = cnnmodeldefinitions.cnnmodelgraphs(cnn_graph, keep_prob_tensor, keep_prob_tensor, batch_norm_switch)

        unnormalized_scores, features, name = modeldef.advanced_cnn_graph_with_batchnorm(input_tensor)

        model_type_name = name

        if not use_class_weights:
            loss = tf.reduce_mean(
                tf.nn.softmax_cross_entropy_with_logits_v2(logits=unnormalized_scores, labels=label_tensor))
        else:
            weight = tf.constant([class_weight])
            loss = tf.reduce_mean(
                tf.nn.weighted_cross_entropy_with_logits(logits=unnormalized_scores, targets=label_tensor,
                                                         pos_weight=weight))

        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)

        with tf.control_dependencies(update_ops):
            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())

        binary_predictions = tf.argmax(unnormalized_scores, 1)
        correct = tf.equal(binary_predictions, tf.argmax(label_tensor, 1))
        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
        scores = tf.nn.softmax(unnormalized_scores)

        saver = tf.train.Saver()

    with tf.Session(graph=cnn_graph) as sess:

        tf.global_variables_initializer().run()

        # Restoring
        if not new_model:
            new_saver = tf.train.import_meta_graph(root_dir + '\\' + model_name + '.meta')
            new_saver.restore(sess, root_dir + '\\' + model_name)

        query_outputs_train = [train_op, loss, accuracy, binary_predictions, features, scores]
        query_outputs_test = [loss, accuracy, binary_predictions, features, scores]
        placeholders = (input_tensor, label_tensor, keep_prob_tensor, batch_size_tensor, batch_norm_switch)

        for epoch_step in range(max_epoch):

            epoch_loss = 0
            epoch_accuracy = 0

            total_batch, remain = divmod(total_data_length, batch_size)
            final_predictions = np.ones(0)
            train_features = np.ones(0)
            train_scores = np.ones(0)

            if train_model:

                for i in range(total_batch):

                    train_loss, train_accuracy, train_predictions, train_batch_features, train_batch_scores = helper_functions.batch_iteration(
                        train_data,
                        one_hot_batch_train_label,
                        embedding_mat,
                        dictionary, i,
                        batch_size,
                        remain,
                        query_outputs_train,
                        *placeholders,
                        only_forward_pass=only_forward,
                        mode="train",
                        session=sess,
                        use_random_balanced_batch=use_random_balanced_batch,
                        split_length=length_of_sequence,
                        final=False)

                    final_predictions = np.append(final_predictions, train_predictions)

                    if not (isinstance(train_batch_features, int) or isinstance(train_batch_scores, int)):
                        if i == 0:
                            train_features = train_batch_features
                            train_scores = train_batch_scores
                        else:
                            train_features = np.concatenate((train_features, train_batch_features))
                            train_scores = np.concatenate((train_scores, train_batch_scores))

                    avg_cost = train_loss * int(batch_size)
                    # avg_cost = train_loss
                    epoch_loss = epoch_loss + avg_cost

                    epoch_accuracy = epoch_accuracy + train_accuracy * batch_size

                train_loss, train_accuracy, train_predictions, train_batch_features, train_batch_scores = helper_functions.batch_iteration(
                    train_data,
                    one_hot_batch_train_label,
                    embedding_mat,
                    dictionary,
                    total_batch,
                    batch_size, remain,
                    query_outputs_train,
                    *placeholders,
                    only_forward_pass=only_forward,
                    split_length=length_of_sequence,
                    use_random_balanced_batch=use_random_balanced_batch,
                    mode="train",
                    session=sess,
                    final=True)

                final_predictions = np.append(final_predictions, train_predictions)

                if not (isinstance(train_batch_features, int) or isinstance(train_batch_scores, int)):
                    train_features = np.concatenate((train_features, train_batch_features))
                    train_scores = np.concatenate((train_scores, train_batch_scores))

                avg_cost = train_loss * int(remain)
                # avg_cost = train_loss

                epoch_loss = epoch_loss + avg_cost
                epoch_loss = epoch_loss / total_data_length

                epoch_accuracy = epoch_accuracy + train_accuracy * remain

                if only_forward:
                    opt_thres = helper_functions.choose_optimal_threshold(train_scores, train_label)

            if epoch_step % save_step == 0:

                epoch_accuracy = epoch_accuracy / total_data_length
                total_batch, remain = divmod(total_test_length, test_batch_size)

                final_test_predictions = np.ones(0)
                test_features = np.ones(0)  # type: ndarray
                test_scores = np.ones(0)
                epoch_test_loss = 0
                epoch_test_accuracy = 0

                for i in range(total_batch):
                    test_loss, test_accuracy, test_predictions, test_batch_features, test_batch_scores = helper_functions.batch_iteration(
                        test_data,
                        one_hot_batch_test_label,
                        embedding_mat,
                        dictionary, i,
                        test_batch_size,
                        remain,
                        query_outputs_test,
                        *placeholders,
                        only_forward_pass=only_forward,
                        split_length=length_of_sequence,
                        use_random_balanced_batch=False,
                        mode='test',
                        session=sess,
                        final=False)

                    final_test_predictions = np.append(final_test_predictions, test_predictions)

                    if i == 0:
                        test_features = test_batch_features
                        test_scores = test_batch_scores
                    else:
                        test_features = np.concatenate((test_features, test_batch_features))
                        test_scores = np.concatenate((test_scores, test_batch_scores))

                    avg_cost = test_loss * int(test_batch_size)
                    # avg_cost = test_loss

                    epoch_test_loss = epoch_test_loss + avg_cost
                    epoch_test_accuracy = epoch_test_accuracy + test_accuracy * test_batch_size

                test_loss, test_accuracy, test_predictions, test_batch_features, test_batch_scores = helper_functions.batch_iteration(
                    test_data,
                    one_hot_batch_test_label,
                    embedding_mat,
                    dictionary,
                    total_batch,
                    test_batch_size,
                    remain,
                    query_outputs_test,
                    *placeholders,
                    only_forward_pass=only_forward,
                    split_length=length_of_sequence,
                    use_random_balanced_batch=False,
                    mode='test',
                    session=sess,
                    final=True)

                final_test_predictions = np.append(final_test_predictions, test_predictions)
                test_features = np.concatenate((test_features, test_batch_features))
                test_scores = np.concatenate((test_scores, test_batch_scores))

                avg_cost = test_loss * int(remain)
                epoch_test_loss = epoch_test_loss + avg_cost
                epoch_test_accuracy = epoch_test_accuracy + test_accuracy * remain
                epoch_test_accuracy = epoch_test_accuracy / total_test_length
                epoch_test_loss = epoch_test_loss / total_test_length

            helper_functions.print_result(epoch_step, epoch_loss, epoch_accuracy, epoch_test_loss, epoch_test_accuracy,
                                          final_test_predictions, test_label, model_name, resample_dataset)

            if only_forward:

                helper_functions.results_new_threshold(test_scores[:, 1], test_label, opt_thres, 0, model_name,
                                                       resample_dataset)

                parameters_original = (train_features, train_label, test_features, test_label, model_name)
                file = {"file_write": True, "use_count_data": False}

                print(train_features.shape)
                print('resample_dataset = ' + str(resample_dataset))
                print('feature_selection = ' + str(use_feature_selection))

                if use_feature_selection:
                    train_features, test_features = ml_algorithms.boruta_feature_selection(train_features,
                                                                                           test_features, train_label,
                                                                                           dataset_type='sequence')

                    print(train_features.shape)
                    print(test_features.shape)

                if use_imbalanced and resample_dataset:
                    resampled_features, resampled_labels = ml_algorithms.resample_dataset(train_features, train_label,
                                                                                          mode='smote')

                    resampled_test_features, resampled_test_labels = ml_algorithms.resample_dataset(test_features,
                                                                                                    test_label,
                                                                                                    mode='smote')
                    print(resampled_features.shape)
                    parameters = (resampled_features, resampled_labels, test_features, test_label, model_name)
                else:
                    parameters = parameters_original
                # #
                ml_algorithms.svm_classification(*parameters, **file)
                ml_algorithms.random_forest(*parameters, **file)
                ml_algorithms.quadratic_discriminant_classifier(*parameters, **file)
                ml_algorithms.knearest_neighbors(*parameters_original, **file)
                ml_algorithms.xgb_classifier(*parameters, **file)

            helper_functions.calculate_auc_score(test_scores, test_label)

            # helper_functions.visualize_features(train_features, train_label, mode='train' , method = 'pca',dimension='3d')
            # helper_functions.visualize_features(train_features, train_label, mode='train',method = 'tsne',dimension='3d')
            # helper_functions.visualize_features(train_features, train_label, mode='train', method='pca', dimension='2d')
            # helper_functions.visualize_features(train_features, train_label, mode='train', method='tsne',dimension='2d')
            # helper_functions.visualize_features(train_features, train_label, mode='train',method = 'pca', dimension='2d')
            # helper_functions.visualize_features(train_features, train_label, mode='test',method = 'pca', dimension='3d')
            # #
            # #

        if save_network:
            save_dir = helper_functions.cnn_model_save(root_dir, feature_size, batch_size, model_type_name, model_name,
                                                       save_name=save_name)
            print(save_dir)
            saver.save(sess, save_dir)


