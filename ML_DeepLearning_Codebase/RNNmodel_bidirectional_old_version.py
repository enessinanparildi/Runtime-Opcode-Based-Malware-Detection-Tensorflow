import numpy as np
import tensorflow as tf

import ml_algorithms
import opcode_embeddings
import helper_functions
from tensorflow.python.ops.rnn import dynamic_rnn


# Also equals embedding size
def bidirectional_phased_lstm_graph_run():

    # embedding_mat = normalize(embedding_mat)

    new_model = True
    resample_dataset = False
    use_imbalanced = False
    use_attention = True
    use_peepholes = True
    use_feature_selection = False
    train_model = True
    save_results = True
    only_forward = False

    model_name = 'model_v1_phased'
    save_name  = 'model_v1_phased'
    root_dir = 'D:\\thesis_code_base\\saveddata\\Networks\\lstm_bidirectional'

    num_layers = 1
    max_epoch = 10
    keep_prob = 0.6

    batch_size = 32
    test_batch_size = 256


    lstmfw_size = 128
    lstmbw_size = 128
    attention_size = 70
    rate = 0.001
    n_classes = 2

    if only_forward:
        max_epoch = 1

    root_dir_data = 'D:\\thesis_code_base\\saveddata\\Embedding_matrix_networks\\'



    converted_train_data, train_label, converted_test_data, test_label, embedding_mat, class_weight = helper_functions.prapare_data(use_imbalanced)


    total_seq_length = 1000
    # converted_train_data = converted_train_data[0:int(0.95 * len(converted_train_data)), :]
    # train_label = train_label[0:int(0.95 * len(train_label))]

    feature_size = embedding_mat.shape[1]
    total_data_length = converted_train_data.shape[0]
    total_test_length = converted_test_data.shape[0]


    tf.reset_default_graph()
    lstm_graph = tf.Graph()

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True


    with lstm_graph.as_default():

        one_hot_batch_train_label = tf.one_hot(train_label, n_classes)
        one_hot_batch_test_label = tf.one_hot(test_label, n_classes)

        training_dataset = tf.data.Dataset.from_tensor_slices((converted_train_data, one_hot_batch_train_label))
        test_dataset = tf.data.Dataset.from_tensor_slices((converted_test_data, one_hot_batch_test_label))

        train_batches = training_dataset.batch(batch_size)
        test_batches = test_dataset.batch(test_batch_size)

        time = tf.placeholder(tf.float32, (None, 1000 , 1))
        x_ = tf.placeholder(tf.float32, (batch_size, 100, 1000))

        handle = tf.placeholder(tf.string, shape=[])
        iterator = tf.data.Iterator.from_string_handle(
            handle, train_batches.output_types, train_batches.output_shapes)

        data_piece, label_piece = iterator.get_next()
        embedded_mat_piece = tf.nn.embedding_lookup(tf.convert_to_tensor(embedding_mat), data_piece)
        # print(embedded_mat_piece)
        # embedded_mat_piece = tf.transpose(embedded_mat_piece,[0,2,1])
        # print(embedded_mat_piece )
        # # embedded_only_batch = tf.expand_dims(embedded_mat_piece[:,:,0],2)
        # second_time_list = [(tf.expand_dims(time[:,:,i],2), tf.expand_dims(embedded_mat_piece[:,i,:],2)) for i in range(total_seq_length)]

        #
        # second_time_single = (embedded_only_batch,embedded_mat_piece)

        # data_time_list = [embedded_mat_piece[:,i,:] for i in range(total_seq_length)]
        #
        # data_time_tuple  = tf.split(embedded_mat_piece,axis = 1,num_or_size_splits  =total_seq_length  )
        # data_time_tuple  =  tf.map_fn(lambda i : tf.squeeze(i),data_time_tuple ,dtype=(tf.float32) )

        training_iterator = train_batches.make_initializable_iterator()
        test_iterator = test_batches.make_initializable_iterator()

        keep_prob_tensor = tf.placeholder_with_default(keep_prob, shape=())


        def create_one_phased_fw_cell():
            lstm_fw_cell = tf.contrib.rnn.PhasedLSTMCell(lstmfw_size, use_peepholes=True)
            if keep_prob < 1.0:
                lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell, output_keep_prob=keep_prob_tensor)
            return lstm_fw_cell

        def create_one_phased_bw_cell():
            lstmbw_cell = tf.contrib.rnn.PhasedLSTMCell(lstmfw_size, use_peepholes=True)
            if keep_prob < 1.0:
                lstmbw_cell = tf.contrib.rnn.DropoutWrapper(lstmbw_cell, output_keep_prob=keep_prob_tensor)
            return lstmbw_cell

        cell_fw_list = [create_one_phased_fw_cell() for _ in range(num_layers)]
        cell_bw_list = [create_one_phased_bw_cell() for _ in range(num_layers)]
        lstmbw_cell = create_one_phased_fw_cell()
        # stack = tf.nn.rnn_cell.MultiRNNCell(create_one_phased_fw_cell() , state_is_tuple=True)
        # all_time_output, _ = dynamic_rnn(lstmbw_cell,  (time,embedded_mat_piece),dtype=tf.float32)

        output, out_state_list_fw = tf.nn.bidirectional_dynamic_rnn(create_one_phased_fw_cell() ,create_one_phased_bw_cell() , (time, embedded_mat_piece), dtype=tf.float32)

        # all_time_output, out_state_list_fw, out_state_list_bw = tf.   contrib.rnn.stack_bidirectional_dynamic_rnn(cell_fw_list, cell_bw_list,
        #                                                                                                        inputs=embedded_mat_piece, dtype=tf.float32)
        #

        all_time_output = tf.concat(list(output), axis=2, name='concat')
        # for time vector averaging
        if use_attention:
            attention_output = apply_attention(all_time_output, attention_size)
            time_average_output = attention_output
        else:
            time_average_output = tf.reduce_mean(all_time_output, 1)



        # concatoutputvec = tf.transpose(concatoutputvec, [1, 0, 2])
        # Choosing only last output for sequence classification
        # last = tf.gather(concatoutputvec, int(concatoutputvec.get_shape()[0]) - 1, name="last_lstm_output")

        # Softmax regression multi- class logistic regression
        weight = tf.Variable(tf.truncated_normal([lstmfw_size + lstmbw_size, n_classes]))
        bias = tf.Variable(tf.constant(0.1, shape=[n_classes]))

        prediction_score = tf.matmul(time_average_output, weight) + bias
        # All time averaging
        # use only final state
        # all_final_state = tf.concat((final_state_fw[0].h,final_state_bw[0].h) ,axis = 1, name='concat' )
        # prediction_vector = tf.matmul(all_final_state, weight) + bias


        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction_score, labels=label_piece))
        optimizer = tf.train.AdamOptimizer(learning_rate= rate).minimize(loss)


        binary_predictions = tf.argmax(prediction_score, 1)
        correct = tf.equal(binary_predictions, tf.argmax(label_piece, 1))
        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))


        saver = tf.train.Saver()


        config = tf.ConfigProto(
            device_count={'GPU': 0}
        )

    time_data = np.reshape(np.tile(np.array(range(total_seq_length)), (batch_size, 1)),
                           (batch_size, total_seq_length, 1))

    with tf.Session(graph=lstm_graph ) as session:

        tf.global_variables_initializer().run()

        training_handle = session.run(training_iterator.string_handle())
        # Restoring
        if (not new_model):
            new_saver = tf.train.import_meta_graph(root_dir + '\\' + model_name + '.meta' )
            new_saver.restore(session, root_dir + '\\' + model_name)


        query_outputs_train = [optimizer, loss, accuracy, binary_predictions, time_average_output]
        query_outputs_test = [loss, accuracy, binary_predictions, time_average_output, prediction_score]
        query_outputs_feature_extract = [time_average_output]

        total_train_batch, remain_train = divmod(total_data_length, batch_size)
        total_batch, remain_test = divmod(total_test_length, test_batch_size)


        if only_forward:
            query_outputs_train = query_outputs_feature_extract

        for epoch_step in range(max_epoch):

            epoch_loss = 0
            epoch_accuracy = 0
            step = 0


            session.run(training_iterator.initializer)

            final_predictions = []
            train_features = []  # type: ndarray
            train_scores = []

            if train_model:
                while True:
                    step = step + 1
                    if step == total_train_batch + 1:
                        time_data = np.reshape(np.tile(np.array(range(total_seq_length)), (remain_train , 1)),
                                               (remain_train , total_seq_length, 1))

                    try:
                        if only_forward:
                            data_feed = {keep_prob_tensor: 1.0,
                                         time:time_data,
                                         handle:training_handle}
                            train_batch_features, train_batch_scores = session.run(query_outputs_train[-2:], data_feed)

                            train_features = train_features.append(train_batch_features)
                            train_scores = train_scores.append(train_batch_scores)

                        else:
                            data_feed = {handle: training_handle,
                                         time: time_data}

                            _, train_loss, train_accuracy, train_predictions = session.run(query_outputs_train[0:4],data_feed)
                            # step = step + 1
                            # print(step)
                            # print(data[0][0])
                            # print(data.shape)
                            print('packet done')
                            print(train_loss)

                            final_predictions.append(np.array(train_predictions))
                            avg_cost = train_loss * int(train_predictions.shape[0])

                            epoch_loss = epoch_loss + avg_cost
                            epoch_accuracy = epoch_accuracy + train_accuracy

                    except tf.errors.OutOfRangeError:
                        print('train_end')
                        break

                final_predictions = np.concatenate(tuple(final_predictions))
                if only_forward:
                    train_features = np.concatenate(tuple(train_features))
                    train_scores = np.concatenate(tuple(train_scores))

                epoch_accuracy = epoch_accuracy / total_data_length
                epoch_loss = epoch_loss / total_test_length

            if only_forward:
                opt_thres = helper_functions.choose_optimal_threshold(train_scores, train_label)


            test_handle = session.run(test_iterator.string_handle())
            session.run(test_iterator.initializer)


            all_test_predictions = []
            all_test_features = []  # type: ndarray
            all_test_scores = []
            epoch_test_loss = 0
            epoch_test_accuracy = 0

            while True:
                try:
                    data_feed = {handle: test_handle,
                                 keep_prob_tensor: 1.0}

                    test_loss, test_accuracy, test_predictions, test_features, test_scores = session.run(query_outputs_test,data_feed)

                    all_test_predictions.append(np.array(test_predictions))
                    all_test_features.append(np.array(test_features))
                    all_test_scores.append(np.array(test_scores))

                    avg_cost = test_loss * int(test_predictions.shape[0])

                    epoch_test_loss = epoch_test_loss + avg_cost
                    epoch_test_accuracy = epoch_test_accuracy + test_accuracy

                except tf.errors.OutOfRangeError:
                    break

            final_test_predictions = np.concatenate(tuple(all_test_predictions))
            test_features = np.concatenate(tuple(all_test_features))
            test_scores = np.concatenate(tuple(all_test_scores))
            epoch_test_accuracy = epoch_test_accuracy/total_test_length
            epoch_test_loss = epoch_test_loss/total_test_length

            helper_functions.print_result(epoch_step, epoch_loss, epoch_accuracy, epoch_test_loss, epoch_test_accuracy,
                                          final_test_predictions, test_label, model_name, resample_dataset)

            if only_forward:

                helper_functions.results_new_threshold(test_scores[:, 1], test_label, opt_thres, 0, model_name,
                                                       resample_dataset)

                parameters_original = (train_features, train_label, test_features, test_label, model_name)
                file = {"file_write": True, "use_count_data": False}

                print(train_features.shape)
                print('resample_dataset = ' + str(resample_dataset))
                print('feature_selection = ' + str(use_feature_selection))

                if use_feature_selection:
                    train_features, test_features = ml_algorithms.boruta_feature_selection(train_features,
                                                                                           test_features, train_label,
                                                                                           dataset_type='sequence')

                    print(train_features.shape)
                    print(test_features.shape)

                if use_imbalanced and resample_dataset:
                    resampled_features, resampled_labels = ml_algorithms.resample_dataset(train_features, train_label,
                                                                                          mode='smote')

                    resampled_test_features, resampled_test_labels = ml_algorithms.resample_dataset(test_features,
                                                                                                    test_label,
                                                                                                    mode='smote')
                    print(resampled_features.shape)
                    parameters = (resampled_features, resampled_labels, test_features, test_label, model_name)
                else:
                    parameters = parameters_original
                # #
                ml_algorithms.svm_classification(*parameters, **file)
                ml_algorithms.random_forest(*parameters, **file)
                ml_algorithms.quadratic_discriminant_classifier(*parameters, **file)
                ml_algorithms.knearest_neighbors(*parameters_original, **file)
                ml_algorithms.xgb_classifier(*parameters, **file)


            # helper_functions.calculate_auc_score(test_scores, test_label)
            # helper_functions.visualize_features(train_features, train_label, mode='train' , method = 'pca',dimension='2d')
            # # helper_functions.visualize_features(train_features, train_label, mode='test',method = 'tsne',dimension='3d')
            # helper_functions.visualize_features(train_features, train_label, mode='train',method = 'tsne', dimension='3d')
            # helper_functions.visualize_features(train_features, train_label, mode='train',method = 'pca', dimension='3d')
            # #

        if save_results:
            save_dir = helper_functions.bidirectional_model_save(root_dir, train_loss, train_accuracy, test_loss, test_accuracy, max_epoch,
                                          keep_prob, lstmfw_size, lstmbw_size, num_layers, model_name, attention_size , save_name = save_name)
            print("save_dir :" + save_dir)
            saver.save(session, save_dir)