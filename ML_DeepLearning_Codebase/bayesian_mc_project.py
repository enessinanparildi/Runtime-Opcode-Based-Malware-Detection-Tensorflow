import numpy as np
import preprocessing

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfTransformer

from math import log
from math import exp
from math import pi
from math import sqrt

from scipy.stats import multivariate_normal
from scipy.stats import uniform

from itertools import accumulate
from itertools import tee
from itertools import repeat

from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.tsa.stattools import acf

import matplotlib.pyplot as plt

from joblib import Parallel, delayed


def softmax(scores):
    e_x = np.exp(scores - np.max(scores))
    return e_x / e_x.sum()


class network_definition:

    def __init__(self, layer_info_tuple, input_train, input_test, labels):
        self.layer_info_tuple = layer_info_tuple
        self.layer_num = len(layer_info_tuple)
        self.input_train = input_train
        self.input_feature_size = input_train.shape[1]
        self.labels = labels
        self.input_test = input_test

    def one_layer_forward_pass(self, input, weights, bias):
        input = input
        lin_output = np.dot(input, weights) + np.reshape(bias, (1, bias.shape[0]))
        output = self.tanh(lin_output)
        return output

    def softmax(self, input):
        return (np.exp(input).T / np.sum(np.exp(input), axis=1)).T

    def sigmoid(self, input):
        return 1 / (1 + np.exp(-input))

    def tanh(self, input):
        return np.tanh(input)

    def forward_neural_network_calculation(self, all_weight_set, all_bias_set, input):
        first_output = self.one_layer_forward_pass(input, all_weight_set[0], all_bias_set[0])
        second_output = self.one_layer_forward_pass(first_output, all_weight_set[1], all_bias_set[1])
        third_output = self.one_layer_forward_pass(second_output, all_weight_set[2], all_bias_set[2])
        return third_output

    def open_vals_set(self, vals):

        if len(vals) != self.get_total_weight_dimension():
            print('invalid_shape')

        lenw1 = self.input_feature_size * self.layer_info_tuple[0]
        lenw2 = self.layer_info_tuple[0] * self.layer_info_tuple[1]
        lenw3 = self.layer_info_tuple[1] * self.layer_info_tuple[2]
        lenb1 = self.layer_info_tuple[0]
        lenb2 = self.layer_info_tuple[1]
        lenb3 = self.layer_info_tuple[2]

        allsizes = (self.input_feature_size,) + self.layer_info_tuple
        lengths = [lenw1, lenw2, lenw3, lenb1, lenb2, lenb3]
        accleng = tuple(accumulate(lengths))
        splittedvals = [vals[i: j] for i, j in zip((0,) + accleng, accleng)]
        splitted_weigths = splittedvals[:3]
        splitted_biases = splittedvals[3:]

        a, b = tee(list(allsizes))
        next(b, None)
        shaple_tuples = list(zip(a, b))

        shaped_weigths = [np.array(weigth).reshape((shape_tuple[0], shape_tuple[1])) for weigth, shape_tuple in
                          zip(splitted_weigths, shaple_tuples)]
        shaped_biases = [np.array(bias) for bias in splitted_biases]

        return shaped_weigths, shaped_biases

    def get_total_weight_dimension(self):
        weight_dim_num = self.input_feature_size * self.layer_info_tuple[0] + \
                         self.layer_info_tuple[0] * self.layer_info_tuple[1] + \
                         self.layer_info_tuple[1] * self.layer_info_tuple[2]
        bias_dim_num = sum(self.layer_info_tuple)
        return weight_dim_num + bias_dim_num

    def label_eval(self, score_label_tuple):
        score, label = score_label_tuple
        score = score[0]
        if label == 1:
            return log(self.sigmoid(np.array(score[1]).reshape((1, -1))))
        else:
            return log(1 - self.sigmoid(np.array(score[0]).reshape((1, -1))))

    def label_eval_v2(self, whole_data_tuple):
        all_weight_instance, all_bias_instance, label, x = whole_data_tuple
        score = self.forward_neural_network_calculation(all_weight_instance, all_bias_instance, x)
        score = score[0]
        print(score[0])
        print(score[1])

        if label == 1:
            return log(self.sigmoid(np.array(score[1]).reshape((1, -1))))
        else:
            return log(1 - self.sigmoid(np.array(score[0]).reshape((1, -1))))

    def likehood_sample_v2(self, all_weight_instance, all_bias_instance):
        # #scores = Parallel(n_jobs=-1,prefer='threads')(delayed(self.forward_neural_network_calculation)(
        # all_weight_instance, all_bias_instance, x) for x in  self.input_train) logits = sum(Parallel(n_jobs=-1,
        # prefer='threads' )(delayed(self.label_eval)(x) for x in zip(scores, self.labels)))
        tuple_set = zip(repeat(all_weight_instance, len(self.labels)), repeat(all_bias_instance, len(self.labels)),
                        self.labels, self.input_train)
        logits = sum(map(self.label_eval_v2, tuple_set))
        # logits = sum(Parallel(n_jobs=-1,prefer='threads' )(delayed(self.label_eval_v2)(x) for x in tuple_set))
        return logits

    def standard_multivariate_log_normal(self, input):
        inputflat = input.flatten()
        dim = len(inputflat)
        res = exp(-0.5 * np.dot(inputflat, inputflat)) / (sqrt(pow(2 * pi, dim)))
        return log(res)

    def test_likehood_query(self, instance_parameter, test_query_labels, test_query_data):
        all_weights_instance, all_biases_instance = self.open_vals_set(instance_parameter)
        scores = self.forward_neural_network_calculation(all_weights_instance, all_biases_instance, test_query_data)
        prob_scores = np.apply_along_axis(softmax, arr=scores, axis=1)
        print(prob_scores)
        return prob_scores[0, int(test_query_labels)]

    def get_predictive_dist_estimate(self, samples, test_query_labels, test_query_data):
        # num_of_piece = cpu_count()-1
        # split_data = [ samples[val:val+int(len(samples)/num_of_piece)] for val in range(0, len(samples), int(len(samples)/num_of_piece))]
        scores = Parallel(n_jobs=-1, verbose=0)(
            delayed(self.test_likehood_query)(instance_val, test_query_labels, test_query_data) for instance_val in
            samples)

        return (sum(scores) / len(scores), scores)

    # def get_gradient_wrt_weigths(self):
    #     1 - np.power(tanh(t), 2)
    #
    def likehood_sample(self, all_weight_instance, all_bias_instance):
        #
        scores = self.forward_neural_network_calculation(all_weight_instance, all_bias_instance, self.input_train)
        prob_scores = np.apply_along_axis(softmax, arr=scores, axis=1)
        logits = np.log(np.array([prob_scores[i, int(label)] for i, label in enumerate(self.labels)]))
        # print('sum_of_logit ' + str(np.sum(logits)))
        return np.sum(logits)

    def log_prior_sample(self, all_weights, all_biases):
        all_weights.extend(all_biases)
        scores = sum([self.standard_multivariate_log_normal(x) for x in all_weights])
        # print('sum_of_scores ' + str(scores))

        return scores

    def get_unnormalized_weight_posterior(self, instance_parameter):
        all_weights_instance, all_biases_instance = self.open_vals_set(instance_parameter)
        # return exp(self.likehood_sample(all_weights_instance, all_biases_instance)
        #           + self.log_prior_sample(all_weights_instance, all_biases_instance))

        return exp(self.likehood_sample(all_weights_instance, all_biases_instance)
                   + self.log_prior_sample(all_weights_instance, all_biases_instance))


class independence_sampler:

    def __init__(self, stdval, num_of_run, network, name, burn_length=1000):
        self.stdval = stdval
        self.num_of_run = num_of_run
        self.burn_length = burn_length
        self.network = network
        self.input_dimension = network.get_total_weight_dimension()
        self.sample_set = np.zeros((self.num_of_run + 1, self.input_dimension))
        self.samplename = name
        self.acceptence_rate = 0

    def main_op(self, sample):
        proposal_val = multivariate_normal.rvs(mean=np.zeros(self.input_dimension), cov=self.stdval)
        print(proposal_val)
        print(sample)

        q1 = multivariate_normal.logpdf(proposal_val, mean=np.zeros(self.input_dimension), cov=self.stdval)
        q2 = multivariate_normal.logpdf(sample, mean=np.zeros(self.input_dimension), cov=self.stdval)

        try:
            A = log(self.network.get_unnormalized_weight_posterior(proposal_val)) + q2 - \
                log(self.network.get_unnormalized_weight_posterior(sample)) - q1

        except:
            print('exception')
            A = 2

        U = log(uniform.rvs())
        if U < A:
            self.acceptence_rate = self.acceptence_rate + 1
            return proposal_val
        else:
            return sample

    def main_loop(self):
        initial_sample = multivariate_normal.rvs(mean=np.zeros(self.input_dimension), cov=1)
        self.sample_set[0, :] = initial_sample
        for i in range(self.num_of_run):
            self.sample_set[i + 1, :] = self.main_op(self.sample_set[i, :])
            print('one_loop_of metropolis step number=' + str(i) + '\033[94m')
        np.save('ind_sample_set' + str(self.samplename), self.get_samples())

    def get_acceptence_rate(self):
        return self.acceptence_rate / self.num_of_run

    def get_samples(self):
        return self.sample_set[self.burn_length:, :]

    # def calculate_target_mean(self):


class random_walk_metropolis:

    def __init__(self, stdval, num_of_run, network, name, burn_length=1000):
        self.stdval = stdval
        self.num_of_run = num_of_run
        self.burn_length = burn_length
        self.network = network
        self.input_dimension = network.get_total_weight_dimension()
        self.sample_set = np.zeros((self.num_of_run + 1, self.input_dimension))
        self.samplename = name
        self.acceptence_rate = 0

    def metropolis_main_op(self, sample):
        proposal_val = multivariate_normal.rvs(mean=sample, cov=self.stdval)

        try:
            A = self.network.get_unnormalized_weight_posterior(
                proposal_val) / self.network.get_unnormalized_weight_posterior(sample)
        except:
            print('exception')
            A = 2

        U = uniform.rvs()
        if U < A:
            self.acceptence_rate = self.acceptence_rate + 1
            return proposal_val
        else:
            return sample

    def main_loop(self):
        initial_sample = multivariate_normal.rvs(mean=np.zeros(self.input_dimension), cov=1)
        self.sample_set[0, :] = initial_sample
        for i in range(self.num_of_run):
            self.sample_set[i + 1, :] = self.metropolis_main_op(self.sample_set[i, :])
            print('one_loop_of metropolis step number=' + str(i) + '\033[94m')
        np.save('rwn_sample_set' + str(self.samplename), self.get_samples())

    def get_acceptence_rate(self):
        return self.acceptence_rate / self.num_of_run

    def get_samples(self):
        return self.sample_set[self.burn_length:, :]

    # def calculate_target_mean(self):


class metropolis_hastings:

    def __init__(self, stdval, num_of_run, network, name, burn_length=1000):
        self.stdval = stdval
        self.num_of_run = num_of_run
        self.burn_length = burn_length
        self.network = network
        self.input_dimension = network.get_total_weight_dimension()
        self.sample_set = np.zeros((self.num_of_run + 1, self.input_dimension))
        self.samplename = str(name) + 'hastings'
        self.acceptence_rate = 0

    def metropolis_main_op(self, sample):
        # proposal multivariate normal with identity covariance matrix

        norm = np.linalg.norm(sample)
        print(norm)
        sigmaval1 = self.stdval * ((1 + norm) ** 2) * np.diagflat(np.ones(self.input_dimension))

        proposal_val = multivariate_normal.rvs(mean=sample, cov=sigmaval1)

        norm2 = np.linalg.norm(proposal_val)
        sigmaval2 = self.stdval * ((1 + norm2) ** 2) * np.diagflat(np.ones(self.input_dimension))

        q1 = multivariate_normal.logpdf(proposal_val, mean=sample, cov=sigmaval1)
        q2 = multivariate_normal.logpdf(sample, mean=proposal_val, cov=sigmaval2)
        # multivariate gausssian density # Ratio and random uniform

        try:
            A = log(self.network.get_unnormalized_weight_posterior(proposal_val)) + q2 - \
                log(self.network.get_unnormalized_weight_posterior(sample)) - q1
        except:
            A = 2

        U = log(uniform.rvs())
        if U < A:
            self.acceptence_rate = self.acceptence_rate + 1
            return proposal_val
        else:
            return sample

    def main_loop(self):
        initial_sample = multivariate_normal.rvs(mean=np.zeros(self.input_dimension), cov=1)
        self.sample_set[0, :] = initial_sample
        for i in range(self.num_of_run):
            self.sample_set[i + 1, :] = self.metropolis_main_op(self.sample_set[i, :])
            print('one_loop_of metropolis-hastings step number=' + str(i) + '\033[92m')
        np.save('rwn_sample_set' + str(self.samplename), self.get_samples())

    def get_acceptence_rate(self):
        return self.acceptence_rate / self.num_of_run

    def get_samples(self):
        return self.sample_set[self.burn_length:, :]

    def thin_samples(self, step=200):
        self.sample_set = self.sample_set[:len(self.sample_set):step, :]


class importance_sampler:

    def __init__(self, num_of_run, network):
        self.num_of_run = num_of_run
        self.input_dimension = network.get_total_weight_dimension()
        self.network = network
        self.sample_set = np.zeros((self.num_of_run, self.input_dimension))

    def importance_main_op(self, sample_val, test_data, test_label):
        dist_vals_denom = multivariate_normal.logpdf(sample_val, mean=np.zeros(self.input_dimension), cov=1)
        unnormlized_posterior_vals = self.network.get_unnormalized_weight_posterior(sample_val)
        likehood_vals = self.network.test_likehood_query(sample_val, test_label, test_data)

        numeratorator = log(likehood_vals) + unnormlized_posterior_vals - (dist_vals_denom)

        demoninator = unnormlized_posterior_vals - (dist_vals_denom)

        self.num_of_run = self.num_of_run - 1
        print('one_loop_of importance-sampling step number =  ' + str(self.num_of_run) + '\033[0;35m')
        print(exp(numeratorator))
        print(exp(demoninator))

        return exp(numeratorator), exp(demoninator)

    def importance_mapping(self, test_data, test_label):
        vals = multivariate_normal.rvs(size=self.num_of_run, mean=np.zeros((self.input_dimension)), cov=1)
        setofresults = [self.importance_main_op(val, test_data, test_label) for val in vals]
        # setofresults = Parallel(n_jobs=-1,verbose=30)(delayed(self.importance_main_op)(val, test_data, test_label) for val in vals)
        np.save('importance_sampling_set', setofresults)
        numval, den_val = list(zip(*setofresults))
        finalresult = sum(numval) / sum(den_val)
        return finalresult

    def run(self, test_data, test_label):
        return self.importance_mapping(test_data, test_label)


class adaptive_mc:

    def __init__(self, stdval, num_of_run, network, name, burn_length=20000):
        self.stdval = stdval
        self.num_of_run = num_of_run
        self.burn_length = burn_length
        self.network = network
        self.input_dimension = network.get_total_weight_dimension()
        self.mult = ((2.38) ** 2) / self.input_dimension
        self.sample_set = np.zeros((self.num_of_run + 1, self.input_dimension))
        self.samplename = str(name) + 'adaptive'
        self.acceptence_rate = 0
        self.counter = 0
        self.epsilon = 0.1

    def adaptive_main_op(self, sample):
        # proposal multivariate normal with identity covariance matrix

        self.counter = self.counter + 1

        if (self.counter < self.input_dimension ** 2):
            propSigma = 0.1 * np.eye(self.input_dimension)
        else:
            covsofar = np.cov(np.transpose(self.sample_set[:self.counter, :]))
            propSigma = self.mult * covsofar + self.epsilon * np.eye(self.input_dimension)

        proposal_val = multivariate_normal.rvs(mean=sample, cov=propSigma)  # proposal value
        # multivariate gausssian density # Ratio and random uniform
        try:
            A = log(self.network.get_unnormalized_weight_posterior(proposal_val)) - \
                log(self.network.get_unnormalized_weight_posterior(sample))
        except:
            A = 2
        print(A)
        U = log(uniform.rvs())
        if U < A:
            self.acceptence_rate = self.acceptence_rate + 1
            return proposal_val
        else:
            return sample

    def main_loop(self):
        initial_sample = multivariate_normal.rvs(mean=np.zeros(self.input_dimension), cov=1)
        self.sample_set[0, :] = initial_sample
        for i in range(self.num_of_run):
            self.sample_set[i + 1, :] = self.adaptive_main_op(self.sample_set[i, :])
            print('one_loop_of adaptive step number=' + str(i) + '\033[92m')
        np.save('rwn_sample_set' + str(self.samplename), self.get_samples())

    def get_acceptence_rate(self):
        return self.acceptence_rate / self.num_of_run

    def get_samples(self):
        return self.sample_set[self.burn_length:, :]

    def thin_samples(self, step=200):
        self.sample_set = self.sample_set[:len(self.sample_set):step, :]


def thin_samples(samples, step=100):
    samples = samples[::step, :]
    return samples


def iid_error(sample_set):
    return np.std(sample_set) / sqrt(len(sample_set))


def autocorr_graph(sample_set):
    plot_acf(sample_set, lags=100)
    plt.show()


def varfact(sample_set):
    acfset = acf(x=sample_set)
    return 2 * np.sum(acfset) - 1


def load_samples(thin=False):
    # rwn_sample_set9adaptive
    # rwn_sample_set8hastings
    # rwn_sample_set8
    # ind_sample_set9
    #

    samples = np.load('ind_sample_set11.npy')
    print('sample_set_size:' + str(samples.shape))
    if thin:
        samples = thin_samples(samples[2000:, :])
    return samples


def run_error_stats(network, test_data, test_label, ind):
    samples = load_samples()
    estimate_mean, result_set = network.get_predictive_dist_estimate(samples, test_label, test_data)
    print('ind_sample_set11')
    print(ind)
    print('ground truth label : ' + str(test_label))
    print('uncertainty estimate for test instance : ' + str(estimate_mean))
    varf = varfact(result_set)
    iiderr = iid_error(result_set)
    print('varfact : ' + str(varf))
    print('iid_error : ' + str(iiderr))
    print('true_error : ' + str(iiderr * sqrt(varf)))
    print('confidence_interval: (' + str(estimate_mean - 1.96 * iiderr) + ' -- ' + str(
        estimate_mean + 1.96 * iiderr) + ')')

    print(samples[:, 8])
    histogram_graph(samples[:, 1])
    autocorr_graph(result_set)


def histogram_graph(values):
    print(values.shape)
    plt.hist(values, density=True, facecolor='g', bins=1000)
    plt.ylabel('Probability')
    plt.title('Posterior distribution of a weight')
    plt.text(1.0, 1.0, ' posterior mean = ' + str(np.mean(values)) + '  posterior std = ' + str(np.std(values)))
    plt.grid(True)
    plt.show()


def choose_fraction_dataset(dataset, labels, size):
    class_zero_inds = np.where(labels == 0)[0]
    class_one_inds = np.where(labels == 1)[0]
    class_zero_chosen_inds = np.random.choice(class_zero_inds, size)
    class_one_chosen_inds = np.random.choice(class_one_inds, size)
    all_inds = np.concatenate((class_one_chosen_inds, class_zero_chosen_inds), axis=0)
    np.random.shuffle(all_inds)
    chosen_dataset = dataset[all_inds, :]
    chosen_labels = labels[all_inds]
    return chosen_dataset, chosen_labels


def tfidf_transform(frequency_data, use_idf):
    transformer = TfidfTransformer(norm="l2",
                                   use_idf=use_idf,
                                   smooth_idf=True,
                                   sublinear_tf=False)
    return transformer.fit_transform(frequency_data)


def get_freq_dataset():
    frequency_data_dir = "C:\\Users\\Ben\\Google Drive\\bigproject\\dataset\\all_malware_benign_master.csv"
    frequency_data, frequency_datalabels, frequency_numberlabels, uniquelabels, classsizes = preprocessing.process_data_v2(
        frequency_data_dir)
    binary_labels = np.ones(len(frequency_numberlabels))
    binary_labels[np.where(np.array(frequency_numberlabels) == 3)[0]] = 0
    inds = np.arange(len(frequency_numberlabels))
    np.random.shuffle(inds)
    inds.astype(np.int_)
    print(inds)
    return frequency_data[inds, :], np.array(frequency_numberlabels)[inds], binary_labels[inds]


def prapare_dataset(indut_dim, size):
    frequency_data, frequency_numberlabels, binary_labels = get_freq_dataset()
    norm_frequency_data = tfidf_transform(frequency_data, use_idf=False)
    whole_dataset, whole_labels = choose_fraction_dataset(norm_frequency_data, binary_labels, size=size)
    whole_dataset = whole_dataset.toarray()
    reduced_whole_dataset = PCA(n_components=indut_dim).fit_transform(whole_dataset)
    whole_dataset = StandardScaler().fit_transform(reduced_whole_dataset)
    X_train, X_test, y_train, y_test = train_test_split(whole_dataset, whole_labels, test_size=.1, random_state=42)

    return X_train, X_test, y_train, y_test


# def plot_distribition()
#

def save_dataset(indut_dim, size, name):
    X_train, X_test, y_train, y_test = prapare_dataset(indut_dim=indut_dim, size=size)
    np.save('mcdatatrain' + str(name) + '.npy', X_train)
    np.save('mcdatatest' + str(name) + '.npy', X_test)
    np.save('mcdatatrainlabels' + str(name) + '.npy', y_train)
    np.save('mcdatatestlabels' + str(name) + '.npy', y_test)


def main_function():
    indut_dim = 4
    stdval_has = 1
    stdval_rwm = 0.04
    num_of_run_rwm = 100000
    size = 300
    save_new = False
    name = 11
    namefiles = 7
    run_sampler_rwm = False
    run_sampler_hastings = False
    run_sampler_importance = False
    run_sampler_independence = True
    run_sampler_adaptive = False

    if save_new:
        save_dataset(indut_dim, size, name)

    X_train = np.load('mcdatatrain' + str(namefiles) + '.npy')
    X_test = np.load('mcdatatest' + str(namefiles) + '.npy')
    y_train = np.load('mcdatatrainlabels' + str(namefiles) + '.npy')
    y_test = np.load('mcdatatestlabels' + str(namefiles) + '.npy')

    print(X_train.shape)

    bayes_net = network_definition((2, 5, 2), X_train, X_test, y_train)
    if run_sampler_rwm:
        print('dimensionality = ' + str(bayes_net.get_total_weight_dimension()))
        rwm_sampler = random_walk_metropolis(stdval=stdval_rwm, num_of_run=num_of_run_rwm, network=bayes_net, name=name)
        rwm_sampler.main_loop()
        print('acceptence_rate_rwm = ' + str(rwm_sampler.get_acceptence_rate()))

    if run_sampler_hastings:
        print('dimensionality = ' + str(bayes_net.get_total_weight_dimension()))
        mh_sampler = metropolis_hastings(stdval=stdval_has, num_of_run=num_of_run_rwm, network=bayes_net, name=name)
        mh_sampler.main_loop()
        print('acceptence_rate_mh = ' + str(mh_sampler.get_acceptence_rate()))

    if run_sampler_importance:
        imp_sampler = importance_sampler(num_of_run=num_of_run_rwm, network=bayes_net)
        result = imp_sampler.run(X_test[20], y_test[20])
        print('importance sampling result = ' + str(result))

    if run_sampler_independence:
        ind_sampler = independence_sampler(stdval=stdval_has, num_of_run=num_of_run_rwm, network=bayes_net, name=name)
        ind_sampler.main_loop()
        print('acceptence_rate_indepen = ' + str(ind_sampler.get_acceptence_rate()))
        print(ind_sampler.get_acceptence_rate())

    if run_sampler_adaptive:
        adap_sampler = adaptive_mc(stdval=stdval_has, num_of_run=num_of_run_rwm, network=bayes_net, name=name)
        adap_sampler.main_loop()
        print('acceptence_rate_indepen = ' + str(adap_sampler.get_acceptence_rate()))

    # print(X_test[15])
    ind = 20
    run_error_stats(bayes_net, X_test[ind], y_test[ind], ind)


if __name__ == "__main__":
    main_function()
