import tensorflow as tf
import helper_functions
import numpy as np
import cnnmodeldefinitions
import ml_algorithms


def tensorflow_normalize(tensor):
    mean,variance = tf.nn.moments(tensor,axes=[0])
    meantensor = tensor - tf.expand_dims(mean,0)
    variancetensor = meantensor /tf.sqrt(tf.expand_dims(variance,0))
    return variancetensor

def cnn_run():

    new_model = False
    resample_dataset = False
    use_imbalanced = False
    use_class_weights = False
    save_network = True
    use_random_balanced_batch = True
    use_feature_selection = True

    train_model = True
    only_forward = True

    model_name = 'cnn_model_v8'
    save_name = 'cnn_model_v8'
    root_dir = 'C:\\Users\\Ben\\Google Drive\\bigproject\\codes\\saveddata\\Networks\\convolutional_networks'

    max_epoch = 30

    keep_prob_fc = 0.7
    keep_prob_conv = 0.9

    batch_size = 16
    test_batch_size = 32

    save_step = 1
    learning_rate = 0.00001
    n_classes = 2

    length_of_sequence = 1000

    if not train_model or only_forward:
        max_epoch = 1
        use_random_balanced_batch = False

    converted_train_data, train_label, converted_test_data, test_label, embedding_mat, class_weight = helper_functions.prapare_data(use_imbalanced)

    converted_train_data = converted_train_data[0:int(0.95*len(converted_train_data)),:]
    train_label = train_label[0:int(0.95*len(train_label))]

    feature_size = embedding_mat.shape[1]
    total_data_length = converted_train_data.shape[0]
    total_test_length = converted_test_data.shape[0]

    tf.reset_default_graph()
    cnn_graph = tf.Graph()

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True

    with cnn_graph.as_default():

        one_hot_batch_train_label = tf.one_hot(train_label, n_classes)
        one_hot_batch_test_label = tf.one_hot(test_label, n_classes)

        training_dataset = tf.data.Dataset.from_tensor_slices(( converted_train_data,one_hot_batch_train_label))
        test_dataset = tf.data.Dataset.from_tensor_slices((converted_test_data, one_hot_batch_test_label))


        train_batches = training_dataset.batch(batch_size)
        test_batches = test_dataset.batch(test_batch_size)

        handle = tf.placeholder(tf.string, shape=[])
        iterator = tf.data.Iterator.from_string_handle(
            handle, train_batches.output_types, train_batches.output_shapes)

        data_piece, label_piece = iterator.get_next()
        embedded_mat_piece = tf.nn.embedding_lookup(tf.convert_to_tensor(embedding_mat), data_piece)
        embedded_mat_piece = tf.expand_dims(embedded_mat_piece, 3)
        embedded_mat_piece = tensorflow_normalize(embedded_mat_piece)

        training_iterator = train_batches.make_initializable_iterator()
        test_iterator = test_batches.make_initializable_iterator()

        batch_norm_switch = tf.placeholder(tf.bool, shape=())
        keep_prob_tensor = tf.placeholder_with_default(keep_prob_fc, shape=())

        model_definition = cnnmodeldefinitions.cnnmodelgraphs(cnn_graph, keep_prob_tensor, keep_prob_tensor, batch_norm_switch)

        unnormalized_scores, features, name = model_definition.advanced_cnn_graph_with_batchnorm(embedded_mat_piece)

        model_type_name = name

        if not use_class_weights:
            loss = tf.reduce_mean(
                tf.nn.softmax_cross_entropy_with_logits_v2(logits=unnormalized_scores, labels=label_piece))
        else:
            weight = tf.constant([class_weight])
            loss = tf.reduce_mean(
                tf.nn.weighted_cross_entropy_with_logits(logits=unnormalized_scores, targets=label_piece,
                                                         pos_weight=weight))

        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)

        with tf.control_dependencies(update_ops):
            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())

        binary_predictions = tf.argmax(unnormalized_scores, 1)
        correct = tf.equal(binary_predictions, tf.argmax(label_piece, 1))
        accuracy = tf.reduce_sum(tf.cast(correct, 'float'))
        scores = tf.nn.softmax(unnormalized_scores)

        saver = tf.train.Saver()

    with tf.Session(graph=cnn_graph , config= config) as session:

        tf.global_variables_initializer().run()

        training_handle = session.run(training_iterator.string_handle())

        # Restoring
        if not new_model:
            new_saver = tf.train.import_meta_graph(root_dir + '\\' + model_name + '.meta')
            new_saver.restore(session, root_dir + '\\' + model_name)

        query_outputs_train = [train_op, loss, accuracy, binary_predictions, features, scores]
        query_outputs_test = [loss, accuracy, binary_predictions, features, scores]

        for epoch_step in range(max_epoch):

            epoch_loss = 0
            epoch_accuracy = 0
            step = 0

            session.run(training_iterator.initializer)

            final_predictions = []
            train_features = []  # type: ndarray
            train_scores = []

            if train_model:
                while True:
                    try:
                        if only_forward:
                            data_feed = {batch_norm_switch: False,
                                         keep_prob_tensor: 1.0,
                                         handle:training_handle}
                            train_batch_features, train_batch_scores = session.run(query_outputs_train[-2:], data_feed)

                            train_features = train_features.append(train_batch_features)
                            train_scores = train_scores.append(train_batch_scores)
                            print(train_scores )

                        else:
                            data_feed = {batch_norm_switch: True,
                                         handle: training_handle}

                            _, train_loss, train_accuracy, train_predictions = session.run(query_outputs_train[0:4],data_feed)

                            final_predictions.append(np.array(train_predictions))
                            avg_cost = train_loss * int(train_predictions.shape[0])

                            epoch_loss = epoch_loss + avg_cost
                            epoch_accuracy = epoch_accuracy + train_accuracy
                    except tf.errors.OutOfRangeError:
                        print('train_end')
                        break

                final_predictions = np.concatenate(tuple(final_predictions))
                if only_forward:
                    train_features = np.concatenate(tuple(train_features))
                    train_scores = np.concatenate(tuple(train_scores))
                epoch_accuracy = epoch_accuracy / total_data_length
                epoch_loss = epoch_loss / total_test_length

            if only_forward:
                opt_thres = helper_functions.choose_optimal_threshold(train_scores, train_label)




            test_handle = session.run(test_iterator.string_handle())
            session.run(test_iterator.initializer)


            all_test_predictions = []
            all_test_features = []  # type: ndarray
            all_test_scores = []
            epoch_test_loss = 0
            epoch_test_accuracy = 0

            while True:
                try:
                    data_feed = {batch_norm_switch: False,
                                 handle: test_handle,
                                 keep_prob_tensor: 1.0}

                    test_loss, test_accuracy, test_predictions, test_features, test_scores = session.run(query_outputs_test,data_feed)

                    all_test_predictions.append(np.array(test_predictions))
                    all_test_features.append(np.array(test_features))
                    all_test_scores.append(np.array(test_scores))

                    avg_cost = test_loss * int(test_predictions.shape[0])

                    epoch_test_loss = epoch_test_loss + avg_cost
                    epoch_test_accuracy = epoch_test_accuracy + test_accuracy

                except tf.errors.OutOfRangeError:
                    break

            final_test_predictions = np.concatenate(tuple(all_test_predictions))
            test_features = np.concatenate(tuple(all_test_features))
            test_scores = np.concatenate(tuple(all_test_scores))
            epoch_test_accuracy = epoch_test_accuracy/total_test_length
            epoch_test_loss = epoch_test_loss/total_test_length

            helper_functions.print_result(epoch_step, epoch_loss, epoch_accuracy, epoch_test_loss, epoch_test_accuracy,
                                          final_test_predictions, test_label, model_name, resample_dataset)





            if only_forward:

                helper_functions.results_new_threshold(test_scores[:, 1], test_label, opt_thres, 0, model_name,
                                                       resample_dataset)

                parameters_original = (train_features, train_label, test_features, test_label, model_name)
                file = {"file_write": True, "use_count_data": False}

                print(train_features.shape)
                print('resample_dataset = ' + str(resample_dataset))
                print('feature_selection = ' + str(use_feature_selection))

                if use_feature_selection:
                    train_features, test_features = ml_algorithms.boruta_feature_selection(train_features,
                                                                                           test_features, train_label,
                                                                                           dataset_type='sequence')

                    print(train_features.shape)
                    print(test_features.shape)

                if use_imbalanced and resample_dataset:
                    resampled_features, resampled_labels = ml_algorithms.resample_dataset(train_features, train_label,
                                                                                          mode='smote')

                    resampled_test_features, resampled_test_labels = ml_algorithms.resample_dataset(test_features,
                                                                                                    test_label,
                                                                                                    mode='smote')
                    print(resampled_features.shape)
                    parameters = (resampled_features, resampled_labels, test_features, test_label, model_name)
                else:
                    parameters = parameters_original
                # #
                ml_algorithms.svm_classification(*parameters, **file)
                ml_algorithms.random_forest(*parameters, **file)
                ml_algorithms.quadratic_discriminant_classifier(*parameters, **file)
                ml_algorithms.knearest_neighbors(*parameters_original, **file)
                ml_algorithms.xgb_classifier(*parameters, **file)

            # helper_functions.calculate_auc_score(test_scores, test_label)

            # helper_functions.visualize_features(train_features, train_label, mode='train' , method = 'pca',dimension='3d')
            # helper_functions.visualize_features(train_features, train_label, mode='train',method = 'tsne',dimension='3d')
            # helper_functions.visualize_features(train_features, train_label, mode='train', method='pca', dimension='2d')
            # helper_functions.visualize_features(train_features, train_label, mode='train', method='tsne',dimension='2d')
            # helper_functions.visualize_features(train_features, train_label, mode='train',method = 'pca', dimension='2d')
            # helper_functions.visualize_features(train_features, train_label, mode='test',method = 'pca', dimension='3d')
            # #
            # #

        if save_network:
            save_dir = helper_functions.cnn_model_save(root_dir, feature_size, batch_size, model_type_name, model_name,
                                                       save_name=save_name)
            print(save_dir)
            saver.save(session, save_dir)