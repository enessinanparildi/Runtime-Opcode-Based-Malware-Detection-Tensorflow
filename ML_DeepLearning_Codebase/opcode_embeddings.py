import numpy as np
import pandas as pd
import tensorflow as tf
from nltk.util import ngrams

import math
import preprocessing

from os import listdir
from tensorflow import keras

import matplotlib.pyplot as plt
import gensim as gn

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from operator import itemgetter
from collections import OrderedDict, Counter
from scipy.stats import multinomial

numpy_data_root_dir = 'E:\\All_code\\ML_DeepLearning_Codebase\\saveddata\\Fixed_numpy_dataset'
embedding_mat_root_dir = 'E:\\All_code\\ML_DeepLearning_Codebase\\saveddata\\Embedding_matrix'
raw_data_root_dir = 'E:\\All_code\\Datasets'


# single skip-gram sample (x,y) (opcode1,opcode2)
# To Do soft sliding window
def prepare_skip_gram_from_sample(sample, window_size):
    skip_gram_list = []
    start_offset = int((window_size - 1) * 0.5)
    end_offset = int(-1 * start_offset)
    for index, code in enumerate(np.transpose(sample)):
        for j in range(end_offset, start_offset):
            curr_index = index + j
            if 0 <= curr_index < len(sample):
                skip_gram_list.append((code, sample[curr_index]))

    return skip_gram_list


def generate_ngram(seq_data, n=2):
    n_gram_sequence_dataset = [list(ngrams(sample, n)) for sample in seq_data]
    flatten_list = []
    for x in n_gram_sequence_dataset:
        flatten_list.extend(x)
    unique_vals = list(set(flatten_list))
    feature_set = np.zeros((len(seq_data), len(unique_vals)))
    for sample_ind, sample in enumerate(n_gram_sequence_dataset):
        count_sample = Counter(sample)
        for inds, val in enumerate(unique_vals):
            feature_set[sample_ind, inds] = count_sample[val]
    return feature_set


def generate_embeddings():
    seq_data, labels = get_dataset()
    embedding_mat, dictionary = gensim_word2vec(seq_data, window=30, size=100, epoch_train=200)
    # embedding_mat = normalize(embedding_mat)
    np.save("C:\\Users\\Ben\\Google Drive\\bigproject\\codes\\saveddata\\Embedding_matrix_networks\\gensim_v5",
            embedding_mat)
    np.save("C:\\Users\\Ben\\Google Drive\\bigproject\\codes\\saveddata\\Embedding_matrix_networks\\gensim_v5_dict",
            dictionary)

def whole_opcode_preprocessing_ops(dataset):
    np.place(dataset, dataset == 270, 262)
    convert_opcode_for_rep_series_prefix(dataset)
    convert_opcode_for_lock_series_prefix(dataset)
    replace_rare_fixed_opcodes(dataset)


def opcode_uniqueness_analysis():
    wholeseq_real_data, wholeseq_real_labels, seqnewbenigndata, seqnewbenignlabels = get_dataset_v2()
    whole_new_benign = get_dataset_new_benign()

    new_malware, label_new_malware = get_external_malware_all('D:\\bigproject_repository\\final_benign\\external_malware.csv')

    new_malware_v2, label_new_malware = get_external_malware_all(
        'D:\\bigproject_repository\\final_benign\\external_malware_v2.csv')

    new_malware_v3, label_new_malware = get_external_malware_all('D:\\bigproject_repository\\final_benign'
                                                      '\\external_malware_v3.csv')

    np.place(whole_new_benign, whole_new_benign == 270, 262)
    np.place(wholeseq_real_data, wholeseq_real_data == 270, 262)

    malware_part_of_data = wholeseq_real_data[np.where(wholeseq_real_labels == 1)[0], :]
    benign_part_of_data = wholeseq_real_data[np.where(wholeseq_real_labels == 0)[0], :]
    print('new_benign_unique_set ' + str(np.unique(whole_new_benign)))
    print('old_benign_unique_set ' + str(np.unique(benign_part_of_data)))
    print('malware_unique_set ' + str(np.unique(malware_part_of_data)))
    nb = set(np.unique(whole_new_benign))
    ob = set(np.unique(benign_part_of_data))
    m = set(np.unique(malware_part_of_data))
    nm = set(np.unique(new_malware))
    nm2 = set(np.unique(new_malware_v2))
    nm3 = set(np.unique(new_malware_v3))

    m.intersection(nb)
    nb.difference(ob)
    ob.difference(nb)
    ab = nb.union(ob)
    all_dataset_set = ab.union(m)
    print(*nb.difference(m))
    print(*nm.difference(m))
    print(*nm.difference(ab))
    print(*nb.intersection(m))
    print(*nb.difference(ob))
    print(*all_dataset_set)
    print(*nm.difference(all_dataset_set))
    print(*nm2.difference(all_dataset_set))
    print(*nm3.difference(all_dataset_set))

    nm_whole_set = [set(samples) for samples in new_malware]
    num = 0
    a = 0
    for ind in nm_whole_set:
        a = a + 1
        if len({538, 531, 557}.intersection(ind)) > 0:
            num = num + 1
    d = 0
    for ind in new_malware:
        for a in [538, 531, 557]:
            if a in ind:
                d = d + 1

    with open('C:\\Users\\Ben\\Google Drive\\bigproject\\dataset\\fullRawOpCodeList.txt', 'r') as file:
        opcode_list = {ind + 1: line[:-1] for ind, line in enumerate(file)}
    print('opcodes = ' + str(opcode_list))

    all_dataset_dict = {opcode_list[key]: key for key in list(all_dataset_set)}
    print(str(all_dataset_dict))

    ab_minus_m = ab.difference(m)
    ab_whole_set = [set(samples) for samples in np.concatenate((whole_new_benign, benign_part_of_data))]
    ab_minus_m_whole_set = [ab_minus_m.intersection(ab_set) for ab_set in ab_whole_set]
    num = 0
    for elem in ab_minus_m_whole_set:
        if not (270 in list(elem)) and len(elem) > 0:
            print(elem)
            num = num + 1

    listwithout270 = [list(elem.difference({270})) for elem in ab_minus_m_whole_set if len(elem) > 0]

    geulllist = list(filter(lambda x: len(x) > 0, listwithout270))
    print(geulllist)
    len(geulllist)
    for elem in ab_minus_m_whole_set:
        if len(elem) > 1 and 270 in list(elem):
            leml = list(elem)
            leml.remove(270)
            listwithout270.append(leml)
        elif len(elem) > 0 and not (270 in list(elem)):
            leml = list(elem)
            listwithout270.append(leml)

    print(geulllist)
    print(np.unique(np.array(geulllist)))
    alono_set = [i for l in geulllist for i in l]
    print(set(alono_set))
    len(set(alono_set))
    count_dict = Counter(alono_set)
    print(count_dict)

    nameddict = {opcode_list[key]: count_dict[key] for key in count_dict.keys()}
    print(nameddict)

    print(count_dict)
    num = 0
    for elem in ab_minus_m_whole_set:
        if len(elem) == 1:
            print(elem)
            num = num + 1

    nb_minus_m = nb.difference(m)
    nb_whole_set = [set(samples) for samples in whole_new_benign]
    nb_minus_m_whole_set = [nb_minus_m.intersection(nb_set) for nb_set in nb_whole_set]
    num = 0
    for elem in nb_minus_m_whole_set:
        if elem == {270}:
            print(elem)
            num = num + 1

    ob_minus_m = ob.difference(m)
    ob_whole_set = [set(samples) for samples in benign_part_of_data]
    ob_minus_m_whole_set = [ob_minus_m.intersection(ob_set) for ob_set in ob_whole_set]

    for ind, ind in enumerate(ob_minus_m_whole_set):
        print(len(list(ind)))
        if len(list(ind)) == 0:
            del ob_minus_m_whole_set[ind]

    print(len(ob_minus_m_whole_set))
    num = 0
    reduced_set = []
    for elem in ob_minus_m_whole_set:
        if elem == {270}:
            num = num + 1
        else:
            reduced_set.append(elem)

    alone_set = [list(i.difference({270}))[0] for i in reduced_set]
    print(len(alone_set))
    alone_unique_set = set(alone_set)
    m_minus_ab = m.difference(ab)
    m_whole_set = [set(samples) for samples in malware_part_of_data]
    m_minus_ab_whole_set = [m_minus_ab.intersection(m_set) for m_set in m_whole_set]
    num = 0
    for elem in m_minus_ab_whole_set:
        if len(elem) > 0:
            print(elem)
            num = num + 1

    num = 0
    for elem in m_minus_ab_whole_set:
        if elem == {297}:
            num = num + 1

    m_minus_nb = m.difference(nb)
    m_whole_set = [set(samples) for samples in malware_part_of_data]
    m_minus_nb_whole_set = [m_minus_nb.intersection(m_set) for m_set in m_whole_set]
    m_minus_nb_whole_set = list(filter(lambda x: len(x) > 0, m_minus_nb_whole_set))

    num = 0
    for elem in m_minus_nb_whole_set:
        if len(elem) > 0:
            num = num + 1

    print(alone_unique_set)

    nm_minus_ab = nm.difference(ab)
    nm_whole_set = [set(samples) for samples in new_malware]
    nm_minus_ab_whole_set = [nm_minus_ab.intersection(nm_set) for nm_set in nm_whole_set]
    num = 0
    for elem in nm_minus_ab_whole_set:
        if len(elem) > 0:
            num = num + 1

    nm_minus_m = nm.difference(m)
    nm_minus_m_whole_set = [nm_minus_m.intersection(nm_set) for nm_set in nm_whole_set]
    num = 0
    for elem in nm_minus_m_whole_set:
        if len(elem) > 0:
            num = num + 1

    ob_minus_nb = ob.difference(nb)
    ob_whole_set = [set(samples) for samples in benign_part_of_data]
    ob_minus_nb_whole_set = [ob_minus_nb.intersection(nm_set) for nm_set in ob_whole_set]
    ob_minus_nb_whole_set = list(filter(lambda x: len(x) > 0, ob_minus_nb_whole_set))

    num = 0
    for elem in ob_minus_nb_whole_set:
        if len(elem) > 0:
            num = num + 1

    nm2_minus_alldataset = nm2.difference(all_dataset_set)
    nm2_whole_set = [set(samples) for samples in new_malware_v2]
    nm2_minus_alldataset_whole_set = [nm2_minus_alldataset.intersection(nm2_set) for nm2_set in nm2_whole_set]
    nm2_minus_alldataset_whole_set = list(filter(lambda x: len(x) > 0, nm2_minus_alldataset_whole_set))


def prepare_skip_gram_from_data_set(data_set, window_size):
    whole_list = []
    token = [whole_list.append(i) for sample in data_set for i in prepare_skip_gram_from_sample(sample, window_size)]

    # whole_list = [prepare_skip_gram_from_sample(sample,window_size) for sample in data_set]
    skip_gram_data = [data_point[0] for data_point in whole_list]
    skip_gram_labels = [data_point[1] for data_point in whole_list]
    return skip_gram_data, skip_gram_labels


def get_dataset(augment_new_benign=False):
    sequence_data_malware = "C:\\Users\\Ben\\Google Drive\\bigproject\\dataset\\markov_1k_malware_release.csv"
    sequence_data_benign = "C:\\Users\\Ben\\Google Drive\\bigproject\\dataset\\markov_1k_benign_release.csv"
    sequence_data_benign_new = "C:\\Users\\Ben\\Google Drive\\bigproject\\dataset\\new_benign_data.csv"

    seqmalwaredata, seqmalwarelabels = preprocessing.extract_data(sequence_data_malware, mode=1)
    seqbenigndata, seqbenignlabels = preprocessing.extract_data(sequence_data_benign, mode=0)

    if augment_new_benign:
        seqnewbenigndata, seqnewbenignlabels = preprocessing.extract_data(sequence_data_benign_new, mode=0)
        seqbenigndata = np.concatenate((seqbenigndata, seqnewbenigndata))
        seqbenignlabels = np.concatenate((seqbenignlabels, seqnewbenignlabels))

    wholeseq_data = np.concatenate((seqmalwaredata, seqbenigndata))
    wholeseq_labels = np.concatenate((seqmalwarelabels, seqbenignlabels))

    inds = np.arange(len(wholeseq_data))
    np.random.shuffle(inds)
    return wholeseq_data[inds, :], wholeseq_labels[inds]


def convert_opcode_for_rep_series_prefix(benign_part_of_data):
    prefix_opcodes = [470, 471, 472, 474, 475]
    existing_opcodes_with_order = [506, 560, 341, 67]
    final_freqs_norm = np.load(embedding_mat_root_dir + '\\prob_dist.npy')
    for x in np.nditer(benign_part_of_data, op_flags=['readwrite']):
        if x in prefix_opcodes:
            trial = multinomial.rvs(n=1, p=final_freqs_norm)
            pos = np.where(trial == 1)[0][0]
            x[...] = existing_opcodes_with_order[pos]


def convert_opcode_for_lock_series_prefix(benign_part_of_data):
    prefix_opcodes = [291]
    existing_opcodes_with_order = [228, 601, 73]
    final_freqs_norm = np.load(embedding_mat_root_dir + '\\prob_dist_2.npy')
    for x in np.nditer(benign_part_of_data, op_flags=['readwrite']):
        if x in prefix_opcodes:
            trial = multinomial.rvs(n=1, p=final_freqs_norm)
            pos = np.where(trial == 1)[0][0]
            x[...] = existing_opcodes_with_order[pos]


def convert_dictionary_values(seq_data, dictionary):
    data_set = np.array(seq_data)
    for ind_row, sample in enumerate(data_set):
        for ind_col, val in enumerate(sample):
            data_set[ind_row][ind_col] = np.where(dictionary == val)[0][0]
    return data_set


def calc_freq_prob_dist_for_conversion_lock():
    opcodes = ["INC", 'XCHG', 'CMPXCHG']
    prefix_opcodes = [291]
    existing_opcodes_with_order = [228, 601, 73]
    whole_new_benign = get_dataset_new_benign()
    codes, counts = np.unique(whole_new_benign, return_counts=True)
    prefic_coversion_freqs = [np.where(codes == i)[0] for i in existing_opcodes_with_order]
    prefic_coversion_counts = [counts[i[0]] for i in prefic_coversion_freqs]
    final_freqs_norm = np.array(prefic_coversion_counts) / sum(prefic_coversion_counts)
    np.save(embedding_mat_root_dir + '\\prob_dist_2.npy', arr=final_freqs_norm)


def calc_freq_prob_dist_for_conversion_rep():
    whole_new_benign = get_dataset_new_benign()
    codes, counts = np.unique(whole_new_benign, return_counts=True)
    prefix_conversion_opcodes = {"SCAS": 506, "STOS": 560, "MOVS": 341, "LODS": 293, "INS": 229, "CMPS": 67}
    prefic_coversion_freqs = [np.where(codes == i[1]) for i in prefix_conversion_opcodes.items()]
    final_freqs = []
    for p in prefic_coversion_freqs:
        if len(p[0]) > 0:
            final_freqs.append(counts[p[0]][0])
    final_freqs_norm = np.array(final_freqs) / sum(final_freqs)
    np.save('D:\\thesis_code_base\\saveddata\\Embedding_matrix_networks\\prob_dist.npy', arr=final_freqs_norm)


def get_external_malware_all(external_malware_path):
    new_malware_data = pd.read_csv(external_malware_path, header=None)
    new_malware_data = np.array(new_malware_data) + 1
    new_malware_labels = np.ones(len(new_malware_data))
    np.place(new_malware_data, new_malware_data == 187, 317)
    np.place(new_malware_data, new_malware_data == 218, 458)
    np.place(new_malware_data, new_malware_data == 270, 262)
    return new_malware_data, new_malware_labels


def get_rare_opcodes(dataset):
    whole_data_flatten = dataset.flatten()
    count_stats = dict(Counter(list(whole_data_flatten)))
    rare_keys = list(filter(lambda x: count_stats[x] < 3, count_stats.keys()))
    ordered_count_stats = sorted(count_stats.items(), key=lambda kv: kv[1])
    print(ordered_count_stats)
    return rare_keys


def replace_rare_opcodes(dataset, rare_codes, replacement_token):
    for code in rare_codes:
        np.place(dataset, dataset == code, vals=replacement_token)


def replace_rare_fixed_opcodes(dataset):
    opcodes = (585, 453, 144, 436, 435, 440, 551, 550, 202, 41, 531, 538, 412, 450, 25, 122, 37, 584, 43)
    for code in opcodes:
        np.place(dataset, dataset == code, vals=-1)


def prapare_new_dictionary_embeddings():
    wholeseq_real_data, wholeseq_real_labels, seqnewbenigndata, seqnewbenignlabels = get_dataset_v2()
    new_benign_dataset = get_dataset_new_benign()
    np.place(wholeseq_real_data, wholeseq_real_data == 270, 262)
    np.place(new_benign_dataset, new_benign_dataset == 270, 262)

    convert_opcode_for_rep_series_prefix(wholeseq_real_data)
    convert_opcode_for_rep_series_prefix(new_benign_dataset)
    convert_opcode_for_lock_series_prefix(wholeseq_real_data)
    convert_opcode_for_lock_series_prefix(new_benign_dataset)

    external_malware_path1 = 'D:\\bigproject_repository\\final_benign\\external_malware.csv'
    external_malware_path2 = 'D:\\bigproject_repository\\final_benign\\external_malware_v2.csv'
    external_malware_path3 = 'D:\\bigproject_repository\\final_benign\\external_malware_v3.csv'

    new_external_malware, labels = get_external_malware_all(external_malware_path1)
    new_external_malware_v2, labels_v2 = get_external_malware_all(external_malware_path2)
    new_external_malware_v3, labels_v3 = get_external_malware_all(external_malware_path3)

    wholeseq_real_data_with_new_benign = np.concatenate(
        (
            wholeseq_real_data, new_benign_dataset, new_external_malware, new_external_malware_v2,
            new_external_malware_v3))
    rare_keys = get_rare_opcodes(wholeseq_real_data_with_new_benign)
    replace_rare_opcodes(wholeseq_real_data_with_new_benign, rare_keys, replacement_token=-1)

    embeddings, dictionary = gensim_word2vec(wholeseq_real_data_with_new_benign, size=64, window=50,
                                             epoch_train=350)

    np.save("D:\\thesis_code_base\\saveddata\\Embedding_matrix_networks\\gensim_v7_new_benign_dict_unk_token.npy",
            dictionary)
    np.save("D:\\thesis_code_base\\saveddata\\Embedding_matrix_networks\\gensim_v7_new_benign_unk_token.npy",
            embeddings)


def get_dataset_v2():
    sequence_data_malware = raw_data_root_dir + "\\markov_1k_malware_release.csv"
    sequence_data_benign = raw_data_root_dir + "\\markov_1k_benign_release.csv"
    sequence_data_benign_new = raw_data_root_dir + "\\new_benign_data.csv"

    seqmalwaredata, seqmalwarelabels = preprocessing.extract_data(sequence_data_malware, mode=1)
    seqbenigndata, seqbenignlabels = preprocessing.extract_data(sequence_data_benign, mode=0)
    print(len(seqmalwaredata))
    print(len(seqmalwarelabels))

    print(len(seqbenigndata))
    print(len(seqbenignlabels))

    seqnewbenigndata, seqnewbenignlabels = preprocessing.extract_data(sequence_data_benign_new, mode=3)

    wholeseq_real_data = np.concatenate((seqmalwaredata, seqbenigndata))
    wholeseq_real_labels = np.concatenate((seqmalwarelabels, seqbenignlabels))
    print(len(wholeseq_real_data))
    print(len(wholeseq_real_labels))

    inds = np.arange(len(wholeseq_real_data))
    np.random.shuffle(inds)
    return wholeseq_real_data[inds, :], wholeseq_real_labels[inds], seqnewbenigndata, seqnewbenignlabels


def get_dataset_new_benign():
    benign_two_dir = raw_data_root_dir + "\\final_benign_v2.csv"
    benign_one_dir = raw_data_root_dir + '\\final_benign_v1.csv'
    data1 = pd.read_csv(benign_two_dir, header=None)
    data2 = pd.read_csv(benign_one_dir, header=None)
    dataall = np.concatenate((np.array(data1), np.array(data2)))
    return dataall + 1


def get_skip_gram_dataset(window_size, subset):
    seq_data, seq_labels = get_dataset()
    seq_data_subset = seq_data[0:subset]
    skip_gram_data, skip_gram_labels = prepare_skip_gram_from_data_set(seq_data_subset, window_size)
    return skip_gram_data, skip_gram_labels


def get_one_hot_batch(data, labels, batch_size_for_embeddings, batch_index):
    skip_gram_data = one_hot_coder_for_embedding_calculation(np.array(data).reshape(-1, 1), batch_size_for_embeddings,
                                                             batch_index)
    skip_gram_labels = one_hot_coder_for_embedding_calculation(np.array(labels).reshape(-1, 1),
                                                               batch_size_for_embeddings, batch_index)
    return skip_gram_data, skip_gram_labels


def one_hot_coder_for_embedding_calculation(dataset, batch_size, batch_index):
    words, dictionary_size = get_dictionary(dataset)

    dataset_batch = dataset[batch_index * batch_size: (batch_index + 1) * batch_size]
    data_set_size = dataset_batch.shape[0]

    dataset_converted = convert_dictionary_values(dataset_batch, words)

    one_hot_repr = np.zeros((data_set_size, dictionary_size), dtype=np.int_)
    for ind, vec in enumerate(one_hot_repr):
        vec[int(dataset_converted[ind])] = 1
    return one_hot_repr


def get_dictionary_from_all():
    data, labels = get_dataset()
    all_words = data.flatten()
    words = np.unique(all_words)
    return words, len(words)


def get_dictionary(seq_data):
    all_words = seq_data.flatten()
    words = np.unique(all_words)
    return words, len(words)


def get_sequence_batch_with_embedding(embedding_mat, seq_data_batch, dictionary):
    embedding_data = np.zeros((seq_data_batch.shape[0], seq_data_batch.shape[1], embedding_mat.shape[1]))
    batch_dataset = convert_dictionary_values(seq_data_batch, dictionary)
    for indices, points in enumerate(batch_dataset):
        embedding_data[indices, :, :] = np.array([embedding_mat[ind][:] for ind in points])
    return embedding_data


def test_train_split(seq_data, seq_label, shuffle=False, split_ratio=0.7):
    length = seq_data.shape[0]

    if shuffle:
        indices = np.arange(length)
        np.random.shuffle(indices)
        seq_data = seq_data[indices, :]
        seq_label = seq_label[indices]

    train_data = seq_data[0:int(split_ratio * length), :]
    train_label = seq_label[0:int(split_ratio * length)]

    test_data = seq_data[int(split_ratio * length):, :]
    test_label = seq_label[int(split_ratio * length):]

    return train_data, train_label, test_data, test_label


def save_result_embeddings(final_loss, window_size, subset, embedding_size, max_epoch, matrix, loss_function_type):
    embedding_parameters = 'final_loss = ' + str(final_loss)[0:6] + '_' + 'window_size = ' + str(
        window_size) + '_' + 'subset = ' \
                           + str(subset) + '_' + 'embedding_size = ' + str(embedding_size) \
                           + '_max_epoch = ' + str(max_epoch) + "__" + 'loss_function_type = ' + loss_function_type

    np.save(
        'C:\\Users\\Ben\\Google Drive\\bigproject\\codes\\saveddata\\Embedding_matrix_folder\\' + embedding_parameters,
        matrix)

    dir = embedding_parameters
    return dir


def load_embedding_matrix(index):
    root_directory = 'C:\\Users\\Ben\\Google Drive\\bigproject\\codes\\saveddata\\Embedding_matrix_folder\\'
    files_dirs = listdir(root_directory)
    requested_file_dir = files_dirs[index]
    full_directory = root_directory + "\\" + requested_file_dir
    loaded_mat = np.load(full_directory)
    return loaded_mat, requested_file_dir


def load_embeddings_network(root_dir, index=0):
    files_dirs = listdir(root_dir)
    meta_files = [file for file in files_dirs if file[-4:None] == 'meta']
    requested_file_dir = meta_files[index]
    return root_dir + requested_file_dir


def obtain_keras_embeddings(embedding_size):
    seq_data, seq_labels = get_dataset()
    train_data, train_label, test_data, test_label = test_train_split(seq_data, seq_labels)
    dictionary = get_dictionary(seq_data)

    model = keras.Model.Sequential()
    model.add(keras.layers.Embedding(len(dictionary), embedding_size, input_length=len(seq_data[0])))

    model.compile('rmsprop', 'mse')
    output_array = model.predict(train_data)
    return output_array


def visualize_embeddings(mode):
    dictionary = np.load(
        "D:\\thesis_code_base\\saveddata\\Embedding_matrix_networks\\gensim_v7_new_benign_dict_unk_token.npy")
    loaded_mat = np.load(
        "D:\\thesis_code_base\\saveddata\\Embedding_matrix_networks\\gensim_v7_new_benign_unk_token.npy")
    if mode == 'tsne':
        X_embedded = TSNE(n_components=2, init='pca', n_iter=10000, perplexity=30).fit_transform(loaded_mat)
    else:
        X_embedded = PCA(n_components=2).fit_transform(loaded_mat)

    file = open('C:\\Users\\Ben\\Google Drive\\bigproject\\dataset\\fullRawOpCodeList.txt', 'r')
    opcode_list = [line[:-1] for line in file]
    reduced_opcode_list = [opcode_list[i - 1] for i in dictionary]
    file.close()

    corr = -0.01
    for ind, opcode_name in enumerate(reduced_opcode_list):
        i = X_embedded[ind][0]
        j = X_embedded[ind][1]
        plt.scatter(i, j, s=2)
        plt.annotate(opcode_name, xy=(i + corr, j + corr), fontsize=5)

    plt.show()

    root_dir_for_plots = "C:\\Users\\Ben\\Google Drive\\bigproject\\images\\plots\\embedding_tsne_graph5"
    plt.savefig(root_dir_for_plots)


def skip_gram_data_reduce(skip_gram_data, skip_gram_labels, dictionary):
    frequencies = np.histogram(skip_gram_data, bins=np.append(dictionary, np.max(skip_gram_data) + 1))[0]
    skip_gram_data_token = np.array(skip_gram_data)
    skip_gram_label_token = np.array(skip_gram_labels)
    for inds, vals in enumerate(frequencies):
        if vals > 10000:
            whole_pos = np.where(dictionary[inds] == skip_gram_data_token)[0]
            selected_pos = whole_pos[0: int(len(whole_pos) / 1.04)]
            # np.delete(whole_inds, [np.where(whole_inds == i)[0] for i in list(selected_pos)])
            skip_gram_data_token = np.delete(skip_gram_data_token, selected_pos)
            skip_gram_label_token = np.delete(skip_gram_label_token, selected_pos)

    return skip_gram_data_token, skip_gram_label_token


def gensim_word2vec(dataset, window, size, epoch_train):
    sentences = []
    for samples in dataset:
        single = []
        for words in samples:
            single.append(str(words))
        sentences.append(single)

    model = gn.models.Word2Vec(
        sentences,
        size=size,
        window=window,
        sg=1,
        min_count=0,
        workers=10)
    model.train(sentences, total_examples=len(sentences), epochs=epoch_train)
    word_vectors = model.wv
    dictionary_list = list(word_vectors.vocab.keys())
    dictionary = [int(x) for x in dictionary_list]
    return word_vectors.vectors, np.array(dictionary)


def word2index(model):
    word2index = OrderedDict((v, k) for k, v in sorted(model.index2word, key=itemgetter(1)))
    return word2index


#
#
# def skip_gram_data_reduce(skip_gram_data, skip_gram_labels,dictionary):
#     frequencies = np.histogram(skip_gram_data, bins = np.append(dictionary, np.max(skip_gram_data) + 1 ))[0]
#     whole_inds = np.arange(len(skip_gram_labels))
#     num_cores = multiprocessing.cpu_count()
#     #
#     # for inds, vals in enumerate(frequencies):
#     #     print(inds)
#     #     if vals > 10000:
#     #         whole_inds = reduce_operation(dictionary, skip_gram_data, whole_inds)
#     skip_gram_data, skip_gram_labels = Parallel(n_jobs=num_cores,verbose=100, backend="threading") (delayed(reduce_operation(dictionary,frequencies, skip_gram_data,skip_gram_labels,inds))(inds) for inds in range(len(frequencies)))
#
#     print(whole_inds)
#     whole_inds.reshape((1,-1))
#     reduced_skip_data = np.array(skip_gram_data)[whole_inds]
#     reduced_skip_label = np.array(skip_gram_labels)[whole_inds]
#     return reduced_skip_data , reduced_skip_label
#
#
# def reduce_operation(dictionary,frequencies,skip_gram_data,skip_gram_labels , inds):
#     if frequencies[inds] > 10000:
#         whole_pos = np.where(dictionary[inds] == skip_gram_data)[0]
#         selected_pos = whole_pos[0: int(len(whole_pos) / 2)]
#         np.delete(skip_gram_data, selected_pos)
#         np.delete(skip_gram_labels, selected_pos)
#     return skip_gram_data,skip_gram_labels


if __name__ == "__main__":

    window_size = 5
    subset = None
    batch_size = 4096
    max_epoch = 20
    embedding_size = 40
    new_matrix_computation = False
    loss_type = 'softmax'

    dictionary, dictionary_size = get_dictionary_from_all()
    skip_gram_data, skip_gram_labels = get_skip_gram_dataset(window_size, subset)
    skip_gram_data_reduced, skip_gram_labels_reduced = skip_gram_data_reduce(skip_gram_data, skip_gram_labels,
                                                                             dictionary)
    dictionary, dictionary_size = get_dictionary(skip_gram_data_reduced)

    total_dataset_size = len(skip_gram_data_reduced)

    tf.reset_default_graph()
    embedding_network_graph = tf.Graph()

    with embedding_network_graph.as_default():

        inputs = tf.placeholder("float", [None, dictionary_size], name='input_placeholder')
        labels = tf.placeholder("float", [None, dictionary_size], name='label_placeholder')

        weight_matrix = tf.Variable(
            tf.random_normal([dictionary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))
        bias_vector = tf.Variable(tf.random_normal([embedding_size]))

        hidden_vector = tf.matmul(inputs, weight_matrix) + bias_vector

        fully_connected_matrix = tf.Variable(
            tf.random_normal([embedding_size, dictionary_size], stddev=1.0 / math.sqrt(embedding_size)))
        fully_connected_bias = tf.Variable(tf.random_normal([dictionary_size]))

        prediction_vector = tf.matmul(hidden_vector, fully_connected_matrix) + fully_connected_bias

        global_step = tf.Variable(0, trainable=False)

        start_learning_rate = 0.0001
        variable_learning_rate = tf.train.exponential_decay(
            start_learning_rate,
            global_step,
            decay_steps=100000,
            decay_rate=0.96,
            staircase=True,
            name=None
        )
        # embed = tf.nn.embedding_lookup()

        if loss_type == 'nce':

            loss = tf.reduce_mean(
                tf.nn.nce_loss(weights=weight_matrix,
                               biases=bias_vector,
                               labels=labels,
                               inputs=weight_matrix,
                               num_sampled=100,
                               num_classes=dictionary_size))
        else:
            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction_vector,
                                                                             labels=labels))

        optimizer = tf.train.AdamOptimizer(variable_learning_rate).minimize(loss, global_step=global_step)

        print('total dataset size = ' + str(total_dataset_size) + '_' + 'window_size = ' + str(
            window_size) + '_' + 'subset = ' \
              + str(subset) + '_' + 'embedding_size = ' + str(embedding_size) \
              + '_max_epoch = ' + str(max_epoch) + '_dictionary_size = ' + str(dictionary_size))

        root_dir = 'C:\\Users\\Ben\\Google Drive\\bigproject\\codes\\saveddata\\Embedding_matrix_networks\\'
        saver = tf.train.Saver()

    with tf.Session(graph=embedding_network_graph) as sess:
        tf.global_variables_initializer().run()

        if (not new_matrix_computation):
            restore_dir = load_embeddings_network(root_dir, index=9)
            print(restore_dir)
            new_saver = tf.train.import_meta_graph(restore_dir)
            new_saver.restore(sess, restore_dir[:-5])

        for epoch_step in range(max_epoch):

            epoch_loss = 0
            total_batch = int(total_dataset_size / batch_size)

            for i in range(total_batch):
                batch_x, batch_y = get_one_hot_batch(skip_gram_data_reduced, skip_gram_labels_reduced, batch_size, i)

                # print("Batch taken  " + str(i))

                train_data_feed = {inputs: batch_x, labels: batch_y}

                _, train_loss = sess.run([optimizer, loss], train_data_feed)

                epoch_loss = epoch_loss + train_loss

            print("Epoch train loss %d :" % (epoch_step), epoch_loss / total_batch)

        final_loss = epoch_loss / total_batch

        embedding_matrix = weight_matrix.eval()

        dir = save_result_embeddings(final_loss, window_size, subset, embedding_size, max_epoch, embedding_matrix,
                                     loss_type)
        print(dir)
        saver.save(sess, root_dir + dir)

#
#
