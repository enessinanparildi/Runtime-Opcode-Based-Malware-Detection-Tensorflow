import numpy as np
import tensorflow as tf
import logging

import helper_functions
import cnnmodeldefinitions
import data_load_functions
import ml_algorithms


class CnnAutoencoder:
    logging.basicConfig(filename='cnn_model.log', format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S',
                        level=logging.DEBUG)

    def __init__(self):
        self.save_results = True
        self.change_graph = True
        self.new_model = True

        self.shuffle_dataset = True

        self.keep_prob_fc = 1.0
        self.keep_prob_conv = 1.0

        self.batch_size = 16
        self.learning_rate = 0.0005
        self.total_seq_length = 1000
        self.epochnum = 50

        self.model_name = 'cnn_model_v4_autoencoder'
        self.save_name = 'cnn_model_v4_autoencoder'

        self.root_dir = 'D:\\thesis_code_base\\saveddata\\Networks\\convolutional_networks'

        self.converted_train_data = None
        self.train_label = None
        self.converted_test_data_balanced = None
        self.test_label_balanced = None
        self.converted_test_data = None
        self.test_label = None
        self.embedding_mat = None
        self.feature_size = None
        self.total_data_length = None
        self.total_test_length = None
        self.class_weight = None
        self.model_type_name = None

    def yield_dataset(self):

        self.converted_train_data, self.train_label, self.converted_test_data, self.test_label, self.embedding_mat, \
        diction, class_weight = \
            data_load_functions.load_data_v3_all_mixed(load_larger_test=True, auto_encoder_data=True,
                                                       modify_opcode=True)

        print(len(np.where(self.train_label == 1)[0]))
        print(len(np.where(self.train_label == 0)[0]))

        print(len(np.where(self.test_label == 1)[0]))
        print(len(np.where(self.test_label == 0)[0]))

        self.feature_size = self.embedding_mat.shape[1]
        self.total_data_length = self.converted_train_data.shape[0]
        self.total_test_length = self.converted_test_data.shape[0]

    def tensorflow_normalize(self, tensor, epsilon=1e-8):
        mean, variance = tf.nn.moments(tensor, axes=[0])
        meantensor = tensor - tf.expand_dims(mean, 0)
        variancetensor = meantensor / (tf.sqrt(tf.expand_dims(variance, 0) + epsilon))
        return variancetensor

    def tensorflow_min_max_normalize(self, tensor, epsilon=1e-8):
        tensor = tf.to_float(tensor)
        max_val = tf.reduce_max(tensor)
        min_val = tf.reduce_min(tensor)
        numerator = tf.subtract(tf.add(tensor, epsilon), min_val)
        denom_val = tf.subtract(max_val, min_val)
        denominator = tf.cond(denom_val < 2 * epsilon, lambda: 2 * epsilon, lambda: denom_val)
        return tf.div(numerator, denominator)

    def get_input_data_pipeline(self, graph):
        with graph.as_default():
            dataset_train = tf.data.Dataset.from_tensor_slices(
                self.converted_train_data)

            dataset_test = tf.data.Dataset.from_tensor_slices(
                self.converted_test_data)

            dataset_all = dataset_train.concatenate(dataset_test)

            if self.shuffle_dataset:
                dataset_all = dataset_all.shuffle(len(self.converted_train_data) + len(self.converted_test_data),
                                                  reshuffle_each_iteration=True)

            batch_op_train = dataset_all.batch(self.batch_size)
            batch_op_train = batch_op_train.prefetch(1)

            iterator_train = tf.data.Iterator.from_structure(batch_op_train.output_types,
                                                             batch_op_train.output_shapes)
            dataset_init_op_train = iterator_train.make_initializer(batch_op_train, name='train_init')

            data_piece = iterator_train.get_next()

        return data_piece

    def autoencoder_cnn_graph(self):

        tf.reset_default_graph()
        cnn_graph = tf.Graph()

        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True

        with cnn_graph.as_default():
            data_piece = self.get_input_data_pipeline(cnn_graph)

            embedded_mat_piece = tf.nn.embedding_lookup(tf.convert_to_tensor(self.embedding_mat), data_piece)
            embedded_mat_piece = tf.expand_dims(embedded_mat_piece, 3)
            embedded_mat_piece_norm = self.tensorflow_min_max_normalize(embedded_mat_piece)

            batch_norm_switch = tf.placeholder(tf.bool, shape=())
            keep_prob_tensor = tf.placeholder_with_default(1.0, shape=())

            model_definition = cnnmodeldefinitions

            out_image, encoded_batch, name = model_definition.keras_autoencoder(embedded_mat_piece_norm)

            self.model_type_name = name

            loss = tf.reduce_mean(
                tf.keras.backend.binary_crossentropy(tf.squeeze(embedded_mat_piece_norm), out_image))

            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
            #
            # decayed_lr = tf.train.exponential_decay(learning_rate,
            #                                         global_step, 10000,
            #                                         0.95, staircase=True)
            #
            # optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)
            optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)
            # # # reset_optimizer_op = tf.variables_initializer(optimizer.variables())
            logging.info(self.learning_rate)

            with tf.control_dependencies(update_ops):
                optimizer = optimizer.minimize(loss, global_step=tf.train.get_global_step())

            saver = tf.train.Saver()

            cnn_graph.add_to_collection('batch_norm_switch', batch_norm_switch)
            cnn_graph.add_to_collection('loss', loss)
            cnn_graph.add_to_collection('config', config)
            cnn_graph.add_to_collection('saver', saver)
            cnn_graph.add_to_collection('optim_op', optimizer)
            cnn_graph.add_to_collection('dropout', keep_prob_tensor)
            cnn_graph.add_to_collection('data', embedded_mat_piece)
            cnn_graph.add_to_collection('encoded_batch', encoded_batch)

            # all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
            return cnn_graph

    def train_graph(self, graph):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()
        else:
            restore_op = None
            main_graph = graph

        if self.change_graph:
            restore_op = graph.get_collection_ref('saver')[0]
            main_graph = graph

        with main_graph.as_default():
            query_list = [main_graph.get_collection_ref(str)[0] for str in
                          ['optim_op', 'loss']]
            print(main_graph.get_collection_ref('optim_op')[0])
            print(main_graph.get_all_collection_keys())
            print(main_graph.get_collection_ref('iterators'))
            with tf.Session() as session:

                if not self.new_model:
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                for epoch_step in range(self.epochnum):
                    session.run(main_graph.get_operation_by_name('train_init'))
                    epoch_loss = 0
                    step = 0

                    while True:
                        step = step + 1

                        try:
                            data_feed = {main_graph.get_collection_ref('batch_norm_switch')[0]: True,
                                         main_graph.get_collection_ref('dropout')[0]: self.keep_prob_fc}

                            _, batch_loss = session.run(query_list, data_feed)

                            epoch_loss = epoch_loss + batch_loss

                        except tf.errors.OutOfRangeError:
                            print('epoch_train_end')
                            epoch_loss = epoch_loss / (step - 1)
                            break

                    print("Epoch: %.4d cost=  %.9f ", (epoch_step + 1), epoch_loss)
                    logging.info("Epoch: %.4d cost=  %.9f", (epoch_step + 1), epoch_loss)

                if self.save_results:
                    self.save_graph(main_graph, session, restore_op)

    def get_encoded_vector(self, graph):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()
        else:
            restore_op = None
            main_graph = graph

        if self.change_graph:
            restore_op = graph.get_collection_ref('saver')[0]
            main_graph = graph

        with main_graph.as_default():

            query_list = [main_graph.get_collection_ref(str)[0] for str in ['encoded_batch']]
            print(main_graph.get_all_collection_keys())

            with tf.Session() as session:

                if not self.new_model:
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                session.run(main_graph.get_operation_by_name('train_init'))
                whole_encoded = []
                while True:
                    try:
                        data_feed = {main_graph.get_collection_ref('batch_norm_switch')[0]: False,
                                     main_graph.get_collection_ref('dropout')[0]: self.keep_prob_fc}

                        batch_encoded = session.run(query_list, data_feed)
                        batch_encoded = batch_encoded[0]
                        shapedata = batch_encoded.shape
                        whole_encoded.append(
                            batch_encoded.reshape((shapedata[0], shapedata[1] * shapedata[2] * shapedata[3])))
                        print(shapedata)

                    except tf.errors.OutOfRangeError:
                        print('epoch_train_end')
                        break
                encoded_all = np.concatenate(whole_encoded)
        return encoded_all

    def save_graph(self, graph, session, new_saver=None):
        with graph.as_default():
            save_dir = helper_functions.cnn_model_save(self.root_dir, 0, self.batch_size, self.model_type_name,
                                                       self.model_name,
                                                       save_name=self.save_name)

            print("save_dir :" + save_dir)
            if not self.new_model:
                new_saver.save(session, save_dir)
            else:
                graph.get_collection_ref('saver')[0].save(session, save_dir)

    def cluster_k_means(self, encoded_vec):
        all_labels = np.concatenate((self.train_label, self.test_label))
        cluster_labels = ml_algorithms.k_means_clustering(encoded_vec)
        matching_rate = len(np.where(cluster_labels == all_labels)[0]) / len(cluster_labels)
        print('matching_rate = ' + str(matching_rate))
        helper_functions.plot_clustering_labels(cluster_labels, encoded_vec, all_labels)

    def run_train(self):
        self.yield_dataset()
        graph = self.autoencoder_cnn_graph()
        self.train_graph(graph=graph)

    def run_eval(self):
        self.yield_dataset()
        graph = self.autoencoder_cnn_graph()
        encoded_vecs = self.get_encoded_vector(graph=graph)
        self.cluster_k_means(encoded_vecs)


if __name__ == "__main__":
    np.set_printoptions(precision=30)
    network = CnnAutoencoder()
    network.run_train()
