import numpy as np
import tensorflow as tf
import helper_functions
import logging
from sklearn.metrics import confusion_matrix
from graph_training_functions import GraphHelperFunctions


class BidirectionalPhasedLstm(GraphHelperFunctions):
    logging.basicConfig(filename='lstm_phased.log', format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S',
                        level=logging.DEBUG)

    def __init__(self):
        super(BidirectionalPhasedLstm, self).__init__()
        self.use_attention = True
        self.use_peepholes = True
        self.use_feature_selection = False
        self.use_label_smooth = True
        self.use_cross_validation = False
        self.cudnn_lstm = True

        self.model_name = 'model_v5_predict_new_malware_test_size=all'
        self.save_name = 'model_v5_predict_new_malware_test_size=all'
        self.root_dir = 'D:\\thesis_code_base\\saveddata\\Networks\\lstm_bidirectional'

        self.save_results = True
        self.new_model = True
        self.change_graph = True

        self.new_malware_test_ratio = 0.999
        self.noise_level = 0.00001
        self.keep_prob = 0.7
        self.lstmfw_size = 64
        self.lstmbw_size = 64
        self.attention_size = 70
        self.rate = 0.001
        self.epochnum = 10

    def get_bidirectional_phased_lstm_graph(self):

        n_classes = 2
        tf.reset_default_graph()
        lstm_graph = tf.Graph()

        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        print('start graph')

        with lstm_graph.as_default():
            iterator_train, iterator_test = self.get_input_data_pipeline(lstm_graph)

            is_training_tensor = tf.placeholder(tf.bool, shape=())
            data_piece, label_piece = tf.cond(is_training_tensor,
                                              lambda: self.fetch_data_from_pipeline(lstm_graph, True, iterator_train,
                                                                                    iterator_test),
                                              lambda: self.fetch_data_from_pipeline(lstm_graph, False, iterator_train,
                                                                                    iterator_test))
            time = tf.placeholder(tf.float32, (None, self.total_seq_length, 1))
            keep_prob_tensor = tf.placeholder_with_default(self.keep_prob, shape=())

            embedded_mat_piece = tf.nn.embedding_lookup(tf.convert_to_tensor(self.embedding_mat), data_piece)
            embedded_mat_piece_norm = tf.cond(is_training_tensor,
                                              lambda: self.gaussian_noise_layer(input_layer=embedded_mat_piece,
                                                                                stddev=self.noise_level),
                                              lambda: embedded_mat_piece)

            def create_one_phased_fw_cell(self: BidirectionalPhasedLstm):
                if self.cudnn_lstm:
                    lstm_fw_cell= tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(self.lstmfw_size)
                else:
                    lstm_fw_cell = tf.contrib.rnn.PhasedLSTMCell(self.lstmfw_size, use_peepholes=True)
                if self.keep_prob < 1.0:
                    lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell, output_keep_prob=keep_prob_tensor)
                return lstm_fw_cell

            def create_one_phased_bw_cell(self):
                if self.cudnn_lstm:
                    lstmbw_cell  = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(self.lstmbw_size)
                else:
                    lstmbw_cell = tf.contrib.rnn.PhasedLSTMCell(self.lstmbw_size, use_peepholes=True)
                if self.keep_prob < 1.0:
                    lstmbw_cell = tf.contrib.rnn.DropoutWrapper(lstmbw_cell, output_keep_prob=keep_prob_tensor)
                return lstmbw_cell

            if self.cudnn_lstm:
                output, out_state_list_fw = tf.nn.bidirectional_dynamic_rnn(create_one_phased_fw_cell(self),
                                                                            create_one_phased_bw_cell(self),
                                                                            (embedded_mat_piece_norm),
                                                                            dtype=tf.float32)
            else:
                output, out_state_list_fw = tf.nn.bidirectional_dynamic_rnn(create_one_phased_fw_cell(self),
                                                                            create_one_phased_bw_cell(self),
                                                                            (time, embedded_mat_piece_norm),
                                                                            dtype=tf.float32)

            self.model_type_name = 'bidirectional_dynamic_rnn'
            all_time_output = tf.concat(list(output), axis=2)
            # for time vector averaging
            if self.use_attention:
                attention_output = self.apply_attention(lstm_graph, all_time_output, self.attention_size)
                time_average_output = attention_output
            else:
                time_average_output = tf.reduce_mean(all_time_output, 1)

            # Softmax regression multi- class logistic regression
            initializer = tf.contrib.layers.xavier_initializer()
            weight = tf.Variable(initializer(shape=[self.lstmfw_size + self.lstmbw_size, n_classes]))
            bias = tf.Variable(tf.constant(0.1, shape=[n_classes]))

            prediction_score = tf.matmul(time_average_output, weight) + bias

            loss = tf.reduce_mean(
                tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction_score, labels=label_piece))
            optimizer = tf.train.AdamOptimizer(learning_rate=self.rate).minimize(loss, name='optim_op')

            self.add_metrics(lstm_graph, prediction_score, label_piece)

            saver = tf.train.Saver()
            config = tf.ConfigProto()

            lstm_graph.add_to_collection('train_iter', iterator_train)
            lstm_graph.add_to_collection('test_iter', iterator_test)
            lstm_graph.add_to_collection('is_training_tensor', is_training_tensor)

            lstm_graph.add_to_collection('time', time)
            lstm_graph.add_to_collection('scores', prediction_score)
            lstm_graph.add_to_collection('features', time_average_output)
            lstm_graph.add_to_collection('loss', loss)
            lstm_graph.add_to_collection('config', config)
            lstm_graph.add_to_collection('saver', saver)
            lstm_graph.add_to_collection('optim_op', optimizer)
            lstm_graph.add_to_collection('dropout', keep_prob_tensor)

            return lstm_graph

    def train_graph(self, graph):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()
        else:
            restore_op = None
            main_graph = graph

        if self.change_graph:
            restore_op = graph.get_collection_ref('saver')[0]
            main_graph = graph

        total_seq_length = self.total_seq_length
        with main_graph.as_default():
            query_list = [main_graph.get_collection_ref(str)[0] for str in ['optim_op', 'loss', 'acc', 'preds']]

            with tf.Session(config=main_graph.get_collection_ref('config')[0]) as session:

                if not self.new_model:
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                total_train_batch, remain_train = divmod(self.total_data_length, self.batch_size)

                for epoch_step in range(self.epochnum):

                    epoch_loss = 0
                    epoch_accuracy = 0
                    step = 0
                    final_predictions = []
                    session.run(main_graph.get_operation_by_name('train_init'))
                    time_data = np.reshape(np.tile(np.array(range(self.total_seq_length)), (self.batch_size, 1)),
                                           (self.batch_size, self.total_seq_length, 1))

                    while True:
                        step = step + 1

                        if step == total_train_batch + 1:
                            time_data = np.reshape(np.tile(np.array(range(total_seq_length)), (remain_train, 1)),
                                                   (remain_train, total_seq_length, 1))
                        else:
                            pass

                        try:
                            data_feed = {main_graph.get_collection_ref('time')[0]: time_data,
                                         main_graph.get_collection_ref('is_training_tensor')[0]: True}

                            _, batch_loss, batch_acc, batch_preds = session.run(query_list, data_feed)

                            epoch_loss = epoch_loss + batch_loss
                            epoch_accuracy = epoch_accuracy + batch_acc
                            final_predictions.append(batch_preds)

                        except tf.errors.OutOfRangeError:
                            print('epoch_train_end')
                            epoch_loss = epoch_loss / (step - 1)
                            epoch_accuracy = epoch_accuracy / (step - 1)
                            predict_set = np.concatenate(final_predictions)
                            break

                    print(confusion_matrix(self.train_label, predict_set))
                    print("Epoch: %.4d cost=  %.9f ", (epoch_step + 1), epoch_loss)
                    logging.info("Epoch: %.4d cost=  %.9f", (epoch_step + 1), epoch_loss)
                    print("Epoch:", '%.4d' % (epoch_step + 1), "acc=", "%9f" % (epoch_accuracy))
                    logging.info("Epoch: %.4d acc =  %.9f ", (epoch_step + 1), epoch_accuracy)

                if (self.save_results):
                    self.save_graph(main_graph, session, restore_op)
        return main_graph

    def run_train_forward_pass(self, graph):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()
        else:
            restore_op = None
            main_graph = graph

        if self.change_graph:
            restore_op = graph.get_collection_ref('saver')[0]
            main_graph = graph

        with main_graph.as_default():
            query_list = [main_graph.get_collection_ref(str)[0] for str in ['acc', 'preds', 'features', 'scores']]

            with tf.Session(config=main_graph.get_collection_ref('config')[0]) as session:

                if (not self.new_model):
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                total_train_batch, remain_train = divmod(self.total_data_length, self.batch_size)
                session.run(main_graph.get_operation_by_name('train_init'))
                step = 0
                final_predictions = []
                train_features = []  # type: list
                train_scores = []
                epoch_accuracy = 0
                time_data = np.reshape(np.tile(np.array(range(self.total_seq_length)), (self.batch_size, 1)),
                                       (self.batch_size, self.total_seq_length, 1))
                while True:

                    step = step + 1

                    if step == total_train_batch + 1:
                        time_data = np.reshape(np.tile(np.array(range(self.total_seq_length)), (remain_train, 1)),
                                               (remain_train, self.total_seq_length, 1))
                    else:
                        pass

                    try:
                        data_feed = {main_graph.get_collection_ref('dropout')[0]: 1.0,
                                     main_graph.get_collection_ref('time')[0]: time_data,
                                     main_graph.get_collection_ref('is_training_tensor')[0]: True}

                        batch_accuracy, batch_predictions, batch_features, batch_scores = session.run(query_list,
                                                                                                      data_feed)

                        final_predictions.append(batch_predictions)
                        train_features.append(batch_features)
                        train_scores.append(batch_scores)
                        epoch_accuracy = epoch_accuracy + batch_accuracy

                    except tf.errors.OutOfRangeError:
                        print('epoch_train_end')
                        break
                predict_set = np.concatenate(final_predictions)
                feature_set = np.concatenate(train_features)
                score_set = np.concatenate(train_scores)
                epoch_accuracy = epoch_accuracy / (step - 1)
                print(confusion_matrix(self.train_label, predict_set))
                print("Epoch_forward_pass:", '%.4d' % (0), "acc=", "%9f" % (epoch_accuracy))

        opt_thres = helper_functions.choose_optimal_threshold(score_set, self.train_label)

        return predict_set, feature_set, score_set, opt_thres

    def run_test_inference(self, graph):

        if not self.new_model:
            restore_op = tf.train.import_meta_graph(self.root_dir + '\\' + self.model_name + '.meta')
            main_graph = tf.get_default_graph()
        else:
            restore_op = None
            main_graph = graph

        if self.change_graph:
            restore_op = graph.get_collection_ref('saver')[0]
            main_graph = graph

        with main_graph.as_default():
            query_list = [main_graph.get_collection_ref(str)[0]
                          for str in ['loss', 'acc', 'preds', 'scores', 'features']]

            with tf.Session(config=main_graph.get_collection_ref('config')[0]) as session:

                if (not self.new_model):
                    restore_op.restore(session, self.root_dir + '\\' + self.model_name)
                else:
                    session.run(tf.global_variables_initializer())

                total_test_batch, remain_test = divmod(self.total_test_length, self.test_batch_size)
                session.run(main_graph.get_operation_by_name('test_init'))

                step = 0
                loss = 0
                accuracy = 0

                final_predictions = []
                train_features = []  # type: list
                train_scores = []
                time_data = np.reshape(np.tile(np.array(range(self.total_seq_length)), (self.test_batch_size, 1)),
                                       (self.test_batch_size, self.total_seq_length, 1))
                while True:

                    step = step + 1

                    if step == total_test_batch + 1:
                        time_data = np.reshape(np.tile(np.array(range(self.total_seq_length)), (remain_test, 1)),
                                               (remain_test, self.total_seq_length, 1))
                    else:
                        pass
                    try:
                        data_feed = {main_graph.get_collection_ref('dropout')[0]: 1.0,
                                     main_graph.get_collection_ref('time')[0]: time_data,
                                     main_graph.get_collection_ref('is_training_tensor')[0]: False}

                        batch_loss, batch_accuracy, batch_predictions, batch_scores, batch_features = session.run(
                            query_list, data_feed)

                        final_predictions.append(batch_predictions)
                        train_features.append(batch_features)
                        train_scores.append(batch_scores)

                        loss = loss + batch_loss
                        accuracy = accuracy + batch_accuracy

                    except tf.errors.OutOfRangeError:
                        print('epoch_test_end')
                        loss = loss / (step - 1)
                        accuracy = accuracy / (step - 1)
                        break

                print("Test:", '%04d' % (0), "cost=", "{:.9f}".format(loss))
                print("Test:", '%04d' % 0, "acc=", "{:.9f}".format(accuracy))

                predict_set = np.concatenate(final_predictions)
                feature_set = np.concatenate(train_features)
                score_set = np.concatenate(train_scores)
                print(confusion_matrix(self.test_label, predict_set))

        return predict_set, feature_set, score_set

    def run_whole_inference(self):
        self.yield_dataset()
        graph = self.get_bidirectional_phased_lstm_graph()
        train_predict_set, train_feature_set, score_set, opt_thres = self.run_train_forward_pass(graph=graph)
        test_predict_set, test_feature_set, test_score_set = self.run_test_inference(graph=graph)
        self.run_ml_classification(train_feature_set, test_feature_set, test_score_set, opt_thres)

    def run_train(self):
        self.yield_dataset()
        graph = self.get_bidirectional_phased_lstm_graph()
        self.train_graph(graph=graph)

    def run_train_with_inference(self):
        self.yield_dataset()
        graph = self.get_bidirectional_phased_lstm_graph()
        graph = self.train_graph(graph=graph)
        predict_set, train_feature_set, score_set, opt_thres = self.run_train_forward_pass(graph=graph)
        test_predict_set, test_feature_set, test_score_set = self.run_test_inference(graph=graph)
        self.run_ml_classification(train_feature_set, test_feature_set, test_score_set, opt_thres)


if __name__ == "__main__":
    network = BidirectionalPhasedLstm()
    network.run_train()
