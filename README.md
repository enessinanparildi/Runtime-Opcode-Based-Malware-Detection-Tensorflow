# Opcode Based Malware Detection Tensorflow
Runtime opcode based deep learning aided malware detection. CNN, LSTM- RNN, Transformer, autoencoder and Word2Vec embeddings implemented on assembly opcode sequence data.
Thousands of new malware codes are developed every day. Signature-based methods, which are employed by common malware detectors, are susceptible to code obfuscation and novel malware. In this paper, we present an alternative method for malware detection which makes use of assembly opcode sequences or frequencies obtained during runtime. Firstly, for sequential opcode data, we utilize natural language processing and deep learning techniques to facilitate the extraction of deeper behavioral features. Due to these features, this method can be impervious to code obfuscation and effective against novel malware. Finally, these features are fed to various machine learning algorithms for classification. The experiments on 26027 samples that were conducted demonstrated high MCC (Matthew Correlation Coefficient) scores are achievable with this approach. Secondly, for frequency dataset, we conduct another experiment that indicates the runtime opcode approach can achieve \%70 accuracy for 16 malware labels on 59424 samples.

# Bidirectional Phased LSTM Network

A TensorFlow implementation of a Bidirectional Phased LSTM network for sequence classification tasks. This implementation includes attention mechanisms, peepholes, and supports both CUDA-optimized and standard LSTM cells.

## Features

- Bidirectional LSTM architecture with phased LSTM cells
- Optional attention mechanism
- Support for peepholes
- Feature selection capabilities
- Label smoothing
- Cross-validation support
- CUDA-optimized LSTM cells option
- Dropout regularization
- Gaussian noise injection
- Embedding layer support

## Requirements

- TensorFlow
- NumPy
- scikit-learn
- Python logging module

## Configuration

The network can be configured with the following parameters:

```python
self.use_attention = True           # Enable/disable attention mechanism
self.use_peepholes = True          # Enable/disable peepholes in LSTM cells
self.use_feature_selection = False  # Enable/disable feature selection
self.use_label_smooth = True       # Enable/disable label smoothing
self.use_cross_validation = False  # Enable/disable cross-validation
self.cudnn_lstm = True             # Use CUDA-optimized LSTM cells
```

### Hyperparameters

```python
self.new_malware_test_ratio = 0.999
self.noise_level = 0.00001
self.keep_prob = 0.7              # Dropout keep probability
self.lstmfw_size = 64            # Forward LSTM cell size
self.lstmbw_size = 64            # Backward LSTM cell size
self.attention_size = 70         # Attention layer size
self.rate = 0.001               # Learning rate
self.epochnum = 10              # Number of training epochs
```

## Usage

### Basic Training

```python
network = BidirectionalPhasedLstm()
network.run_train()
```

### Training with Inference

```python
network = BidirectionalPhasedLstm()
network.run_train_with_inference()
```

### Inference Only

```python
network = BidirectionalPhasedLstm()
network.run_whole_inference()
```

## Model Architecture

The network implements a bidirectional LSTM architecture with the following key components:

1. Embedding Layer
2. Bidirectional Phased LSTM Layer
3. Optional Attention Mechanism
4. Softmax Classification Layer

### Attention Mechanism

When enabled, the attention mechanism helps the network focus on relevant parts of the input sequence. The attention output is computed using:

```python
attention_output = self.apply_attention(lstm_graph, all_time_output, self.attention_size)
```

### LSTM Cell Configuration

The network supports both CUDA-optimized and standard LSTM cells:

```python
if self.cudnn_lstm:
    lstm_fw_cell = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(self.lstmfw_size)
else:
    lstm_fw_cell = tf.contrib.rnn.PhasedLSTMCell(self.lstmfw_size, use_peepholes=True)
```

## Logging

The network logs training progress and metrics to 'lstm_phased.log':

```python
logging.basicConfig(filename='lstm_phased.log', 
                   format='%(asctime)s - %(message)s',
                   datefmt='%d-%b-%y %H:%M:%S',
                   level=logging.DEBUG)
```

## Model Saving and Loading

Models can be saved and loaded using TensorFlow's Saver functionality. Set the following parameters:

```python
self.model_name = 'model_v5_predict_new_malware_test_size=all'
self.save_name = 'model_v5_predict_new_malware_test_size=all'
self.root_dir = 'D:\\thesis_code_base\\saveddata\\Networks\\lstm_bidirectional'
```

## Performance Metrics

The network tracks and reports:
- Loss
- Accuracy
- Confusion Matrix
- Feature Representations
- Classification Scores

## Contributing

Feel free to submit issues and enhancement requests!

## License

[Add your license information here]
