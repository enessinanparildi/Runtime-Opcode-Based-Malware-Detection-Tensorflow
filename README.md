# Opcode Based Malware Detection Tensorflow
Runtime opcode based deep learning aided malware detection. CNN, LSTM- RNN, Transformer, autoencoder and Word2Vec embeddings implemented on assembly opcode sequence data.
Thousands of new malware codes are developed every day. Signature-based methods, which are employed by common malware detectors, are susceptible to code obfuscation and novel malware. In this paper, we present an alternative method for malware detection which makes use of assembly opcode sequences or frequencies obtained during runtime. Firstly, for sequential opcode data, we utilize natural language processing and deep learning techniques to facilitate the extraction of deeper behavioral features. Due to these features, this method can be impervious to code obfuscation and effective against novel malware. Finally, these features are fed to various machine learning algorithms for classification. The experiments on 26027 samples that were conducted demonstrated high MCC (Matthew Correlation Coefficient) scores are achievable with this approach. Secondly, for frequency dataset, we conduct another experiment that indicates the runtime opcode approach can achieve \%70 accuracy for 16 malware labels on 59424 samples.

# Bidirectional Phased LSTM Network

A TensorFlow implementation of a Bidirectional Phased LSTM network for sequence classification tasks. This implementation includes attention mechanisms, peepholes, and supports both CUDA-optimized and standard LSTM cells.

## Features

- Bidirectional LSTM architecture with phased LSTM cells
- Optional attention mechanism
- Support for peepholes
- Feature selection capabilities
- Label smoothing
- Cross-validation support
- CUDA-optimized LSTM cells option
- Dropout regularization
- Gaussian noise injection
- Embedding layer support

## Requirements

- TensorFlow
- NumPy
- scikit-learn
- Python logging module

## Configuration

The network can be configured with the following parameters:

```python
self.use_attention = True           # Enable/disable attention mechanism
self.use_peepholes = True          # Enable/disable peepholes in LSTM cells
self.use_feature_selection = False  # Enable/disable feature selection
self.use_label_smooth = True       # Enable/disable label smoothing
self.use_cross_validation = False  # Enable/disable cross-validation
self.cudnn_lstm = True             # Use CUDA-optimized LSTM cells
```

### Hyperparameters

```python
self.new_malware_test_ratio = 0.999
self.noise_level = 0.00001
self.keep_prob = 0.7              # Dropout keep probability
self.lstmfw_size = 64            # Forward LSTM cell size
self.lstmbw_size = 64            # Backward LSTM cell size
self.attention_size = 70         # Attention layer size
self.rate = 0.001               # Learning rate
self.epochnum = 10              # Number of training epochs
```

## Usage

### Basic Training

```python
network = BidirectionalPhasedLstm()
network.run_train()
```

### Training with Inference

```python
network = BidirectionalPhasedLstm()
network.run_train_with_inference()
```

### Inference Only

```python
network = BidirectionalPhasedLstm()
network.run_whole_inference()
```

## Model Architecture

The network implements a bidirectional LSTM architecture with the following key components:

1. Embedding Layer
2. Bidirectional Phased LSTM Layer
3. Optional Attention Mechanism
4. Softmax Classification Layer

### Attention Mechanism

When enabled, the attention mechanism helps the network focus on relevant parts of the input sequence. The attention output is computed using:

```python
attention_output = self.apply_attention(lstm_graph, all_time_output, self.attention_size)
```

### LSTM Cell Configuration

The network supports both CUDA-optimized and standard LSTM cells:

```python
if self.cudnn_lstm:
    lstm_fw_cell = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(self.lstmfw_size)
else:
    lstm_fw_cell = tf.contrib.rnn.PhasedLSTMCell(self.lstmfw_size, use_peepholes=True)
```

## Logging

The network logs training progress and metrics to 'lstm_phased.log':

```python
logging.basicConfig(filename='lstm_phased.log', 
                   format='%(asctime)s - %(message)s',
                   datefmt='%d-%b-%y %H:%M:%S',
                   level=logging.DEBUG)
```

## Model Saving and Loading

Models can be saved and loaded using TensorFlow's Saver functionality. Set the following parameters:

```python
self.model_name = 'model_v5_predict_new_malware_test_size=all'
self.save_name = 'model_v5_predict_new_malware_test_size=all'
self.root_dir = 'D:\\thesis_code_base\\saveddata\\Networks\\lstm_bidirectional'
```

## Performance Metrics

The network tracks and reports:
- Loss
- Accuracy
- Confusion Matrix
- Feature Representations
- Classification Scores

# Opcode Embeddings

A Python library for generating and analyzing embeddings of assembly language opcodes, particularly useful for malware detection and analysis.

## Overview

This project implements word2vec-style embeddings for assembly language opcodes, allowing for numerical representation of assembly code sequences. The embeddings can be used for downstream tasks like malware detection, code similarity analysis, and program behavior modeling.

## Features

- Generation of opcode embeddings using skip-gram architecture
- Support for various embedding approaches:
  - Word2vec using Gensim
  - Custom TensorFlow implementation
  - Keras Embedding layer
- Visualization tools for embeddings using t-SNE and PCA
- Preprocessing utilities for opcode sequences
- Support for handling rare opcodes and special prefix cases

## Installation

### Prerequisites
- Python 3.x
- TensorFlow
- NumPy
- Pandas
- Gensim
- NLTK
- Scikit-learn
- Matplotlib

### Dependencies
```bash
pip install tensorflow numpy pandas gensim nltk scikit-learn matplotlib
```

## Usage

### Basic Embedding Generation

```python
from opcode_embeddings import gensim_word2vec

# Load your opcode sequences
sequences = load_opcode_sequences()

# Generate embeddings
embeddings, dictionary = gensim_word2vec(
    dataset=sequences,
    window=30,
    size=100,
    epoch_train=200
)
```

### Preprocessing Data

```python
# Load and preprocess dataset
seq_data, labels = get_dataset()

# Handle special cases
convert_opcode_for_rep_series_prefix(seq_data)
convert_opcode_for_lock_series_prefix(seq_data)

# Handle rare opcodes
rare_keys = get_rare_opcodes(seq_data)
replace_rare_opcodes(seq_data, rare_keys, replacement_token=-1)
```

### Visualization

```python
# Visualize embeddings using t-SNE or PCA
visualize_embeddings(mode='tsne')  # or mode='pca'
```

## Main Components

### Data Processing
- `get_dataset()`: Loads and prepares opcode sequence datasets
- `prepare_skip_gram_from_data_set()`: Creates skip-gram training pairs
- `get_rare_opcodes()`: Identifies rare opcodes for special handling

### Embedding Generation
- `gensim_word2vec()`: Generates embeddings using Gensim's implementation
- Custom TensorFlow implementation using skip-gram architecture
- Support for different loss functions (NCE loss, softmax)

### Visualization and Analysis
- t-SNE visualization of embeddings
- PCA dimensionality reduction
- Opcode frequency analysis

## File Structure

```
opcode_embeddings/
├── opcode_embeddings.py     # Main implementation
├── preprocessing.py         # Data preprocessing utilities
├── saveddata/              # Directory for saved models
│   ├── Embedding_matrix/   # Saved embedding matrices
│   └── Fixed_numpy_dataset/# Preprocessed datasets
└── dataset/                # Raw datasets
```

## Configuration

Key parameters for embedding generation:

- `window_size`: Context window size for skip-gram (default: 5)
- `embedding_size`: Dimension of embedding vectors (default: 40)
- `batch_size`: Training batch size (default: 4096)
- `max_epoch`: Number of training epochs (default: 20)

## Data Formats

### Input Data
- Raw opcode sequences in CSV format
- Each sequence should be a list of opcode indices
- Opcodes should be indexed starting from 1

### Output Embeddings
- Numpy arrays of shape (vocabulary_size, embedding_dimension)
- Corresponding dictionary mapping opcodes to indices

## Error Handling

The code includes handling for:
- Rare opcodes
- Special prefix cases (REP, LOCK)
- Invalid input sequences
- Missing or corrupted data files

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Intel x86 instruction set documentation
- Word2vec paper and implementation
- Malware analysis community

## Contact

For questions and feedback, please open an issue in the GitHub repository.
